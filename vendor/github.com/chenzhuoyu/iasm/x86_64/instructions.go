// Code generated by "mkasm_amd64.py", DO NOT EDIT.

package x86_64

// ADCB performs "Add with Carry".
//
// Mnemonic        : ADC
// Supported forms : (6 forms)
//
//    * ADCB imm8, al
//    * ADCB imm8, r8
//    * ADCB r8, r8
//    * ADCB m8, r8
//    * ADCB imm8, m8
//    * ADCB r8, m8
//
func (self *Program) ADCB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ADCB imm8, al
    if isImm8(v0) && v1 == AL {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x14)
            m.imm1(toImmAny(v[0]))
        })
    }
    // ADCB imm8, r8
    if isImm8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0x80)
            m.emit(0xd0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // ADCB r8, r8
    if isReg8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], isReg8REX(v[0]) || isReg8REX(v[1]))
            m.emit(0x10)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], isReg8REX(v[0]) || isReg8REX(v[1]))
            m.emit(0x12)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // ADCB m8, r8
    if isM8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), isReg8REX(v[1]))
            m.emit(0x12)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // ADCB imm8, m8
    if isImm8(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0x80)
            m.mrsd(2, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // ADCB r8, m8
    if isReg8(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), isReg8REX(v[0]))
            m.emit(0x10)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ADCB")
    }
    return p
}

// ADCL performs "Add with Carry".
//
// Mnemonic        : ADC
// Supported forms : (8 forms)
//
//    * ADCL imm32, eax
//    * ADCL imm8, r32
//    * ADCL imm32, r32
//    * ADCL r32, r32
//    * ADCL m32, r32
//    * ADCL imm8, m32
//    * ADCL imm32, m32
//    * ADCL r32, m32
//
func (self *Program) ADCL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ADCL imm32, eax
    if isImm32(v0) && v1 == EAX {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x15)
            m.imm4(toImmAny(v[0]))
        })
    }
    // ADCL imm8, r32
    if isImm8Ext(v0, 4) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0x83)
            m.emit(0xd0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // ADCL imm32, r32
    if isImm32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0x81)
            m.emit(0xd0 | lcode(v[1]))
            m.imm4(toImmAny(v[0]))
        })
    }
    // ADCL r32, r32
    if isReg32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x11)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x13)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // ADCL m32, r32
    if isM32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x13)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // ADCL imm8, m32
    if isImm8Ext(v0, 4) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0x83)
            m.mrsd(2, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // ADCL imm32, m32
    if isImm32(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0x81)
            m.mrsd(2, addr(v[1]), 1)
            m.imm4(toImmAny(v[0]))
        })
    }
    // ADCL r32, m32
    if isReg32(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x11)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ADCL")
    }
    return p
}

// ADCQ performs "Add with Carry".
//
// Mnemonic        : ADC
// Supported forms : (8 forms)
//
//    * ADCQ imm32, rax
//    * ADCQ imm8, r64
//    * ADCQ imm32, r64
//    * ADCQ r64, r64
//    * ADCQ m64, r64
//    * ADCQ imm8, m64
//    * ADCQ imm32, m64
//    * ADCQ r64, m64
//
func (self *Program) ADCQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ADCQ imm32, rax
    if isImm32(v0) && v1 == RAX {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48)
            m.emit(0x15)
            m.imm4(toImmAny(v[0]))
        })
    }
    // ADCQ imm8, r64
    if isImm8Ext(v0, 8) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0x83)
            m.emit(0xd0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // ADCQ imm32, r64
    if isImm32Ext(v0, 8) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0x81)
            m.emit(0xd0 | lcode(v[1]))
            m.imm4(toImmAny(v[0]))
        })
    }
    // ADCQ r64, r64
    if isReg64(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[0]) << 2 | hcode(v[1]))
            m.emit(0x11)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x13)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // ADCQ m64, r64
    if isM64(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x13)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // ADCQ imm8, m64
    if isImm8Ext(v0, 8) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0x83)
            m.mrsd(2, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // ADCQ imm32, m64
    if isImm32Ext(v0, 8) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0x81)
            m.mrsd(2, addr(v[1]), 1)
            m.imm4(toImmAny(v[0]))
        })
    }
    // ADCQ r64, m64
    if isReg64(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[0]), addr(v[1]))
            m.emit(0x11)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ADCQ")
    }
    return p
}

// ADCW performs "Add with Carry".
//
// Mnemonic        : ADC
// Supported forms : (8 forms)
//
//    * ADCW imm16, ax
//    * ADCW imm8, r16
//    * ADCW imm16, r16
//    * ADCW r16, r16
//    * ADCW m16, r16
//    * ADCW imm8, m16
//    * ADCW imm16, m16
//    * ADCW r16, m16
//
func (self *Program) ADCW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ADCW imm16, ax
    if isImm16(v0) && v1 == AX {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.emit(0x15)
            m.imm2(toImmAny(v[0]))
        })
    }
    // ADCW imm8, r16
    if isImm8Ext(v0, 2) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0x83)
            m.emit(0xd0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // ADCW imm16, r16
    if isImm16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0x81)
            m.emit(0xd0 | lcode(v[1]))
            m.imm2(toImmAny(v[0]))
        })
    }
    // ADCW r16, r16
    if isReg16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x11)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x13)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // ADCW m16, r16
    if isM16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x13)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // ADCW imm8, m16
    if isImm8Ext(v0, 2) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0x83)
            m.mrsd(2, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // ADCW imm16, m16
    if isImm16(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0x81)
            m.mrsd(2, addr(v[1]), 1)
            m.imm2(toImmAny(v[0]))
        })
    }
    // ADCW r16, m16
    if isReg16(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x11)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ADCW")
    }
    return p
}

// ADCXL performs "Unsigned Integer Addition of Two Operands with Carry Flag".
//
// Mnemonic        : ADCX
// ISA extensions  : ADX
// Supported forms : (2 forms)
//
//    * ADCXL r32, r32
//    * ADCXL m32, r32
//
func (self *Program) ADCXL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ADCXL r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_ADX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xf6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // ADCXL m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_ADX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xf6)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ADCXL")
    }
    return p
}

// ADCXQ performs "Unsigned Integer Addition of Two Operands with Carry Flag".
//
// Mnemonic        : ADCX
// ISA extensions  : ADX
// Supported forms : (2 forms)
//
//    * ADCXQ r64, r64
//    * ADCXQ m64, r64
//
func (self *Program) ADCXQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ADCXQ r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_ADX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xf6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // ADCXQ m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_ADX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xf6)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ADCXQ")
    }
    return p
}

// ADDB performs "Add".
//
// Mnemonic        : ADD
// Supported forms : (6 forms)
//
//    * ADDB imm8, al
//    * ADDB imm8, r8
//    * ADDB r8, r8
//    * ADDB m8, r8
//    * ADDB imm8, m8
//    * ADDB r8, m8
//
func (self *Program) ADDB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ADDB imm8, al
    if isImm8(v0) && v1 == AL {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x04)
            m.imm1(toImmAny(v[0]))
        })
    }
    // ADDB imm8, r8
    if isImm8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0x80)
            m.emit(0xc0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // ADDB r8, r8
    if isReg8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], isReg8REX(v[0]) || isReg8REX(v[1]))
            m.emit(0x00)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], isReg8REX(v[0]) || isReg8REX(v[1]))
            m.emit(0x02)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // ADDB m8, r8
    if isM8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), isReg8REX(v[1]))
            m.emit(0x02)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // ADDB imm8, m8
    if isImm8(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0x80)
            m.mrsd(0, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // ADDB r8, m8
    if isReg8(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), isReg8REX(v[0]))
            m.emit(0x00)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ADDB")
    }
    return p
}

// ADDL performs "Add".
//
// Mnemonic        : ADD
// Supported forms : (8 forms)
//
//    * ADDL imm32, eax
//    * ADDL imm8, r32
//    * ADDL imm32, r32
//    * ADDL r32, r32
//    * ADDL m32, r32
//    * ADDL imm8, m32
//    * ADDL imm32, m32
//    * ADDL r32, m32
//
func (self *Program) ADDL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ADDL imm32, eax
    if isImm32(v0) && v1 == EAX {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x05)
            m.imm4(toImmAny(v[0]))
        })
    }
    // ADDL imm8, r32
    if isImm8Ext(v0, 4) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0x83)
            m.emit(0xc0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // ADDL imm32, r32
    if isImm32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0x81)
            m.emit(0xc0 | lcode(v[1]))
            m.imm4(toImmAny(v[0]))
        })
    }
    // ADDL r32, r32
    if isReg32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x01)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x03)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // ADDL m32, r32
    if isM32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x03)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // ADDL imm8, m32
    if isImm8Ext(v0, 4) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0x83)
            m.mrsd(0, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // ADDL imm32, m32
    if isImm32(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0x81)
            m.mrsd(0, addr(v[1]), 1)
            m.imm4(toImmAny(v[0]))
        })
    }
    // ADDL r32, m32
    if isReg32(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x01)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ADDL")
    }
    return p
}

// ADDPD performs "Add Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : ADDPD
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * ADDPD xmm, xmm
//    * ADDPD m128, xmm
//
func (self *Program) ADDPD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ADDPD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x58)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // ADDPD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x58)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ADDPD")
    }
    return p
}

// ADDPS performs "Add Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : ADDPS
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * ADDPS xmm, xmm
//    * ADDPS m128, xmm
//
func (self *Program) ADDPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ADDPS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x58)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // ADDPS m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x58)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ADDPS")
    }
    return p
}

// ADDQ performs "Add".
//
// Mnemonic        : ADD
// Supported forms : (8 forms)
//
//    * ADDQ imm32, rax
//    * ADDQ imm8, r64
//    * ADDQ imm32, r64
//    * ADDQ r64, r64
//    * ADDQ m64, r64
//    * ADDQ imm8, m64
//    * ADDQ imm32, m64
//    * ADDQ r64, m64
//
func (self *Program) ADDQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ADDQ imm32, rax
    if isImm32(v0) && v1 == RAX {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48)
            m.emit(0x05)
            m.imm4(toImmAny(v[0]))
        })
    }
    // ADDQ imm8, r64
    if isImm8Ext(v0, 8) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0x83)
            m.emit(0xc0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // ADDQ imm32, r64
    if isImm32Ext(v0, 8) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0x81)
            m.emit(0xc0 | lcode(v[1]))
            m.imm4(toImmAny(v[0]))
        })
    }
    // ADDQ r64, r64
    if isReg64(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[0]) << 2 | hcode(v[1]))
            m.emit(0x01)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x03)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // ADDQ m64, r64
    if isM64(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x03)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // ADDQ imm8, m64
    if isImm8Ext(v0, 8) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0x83)
            m.mrsd(0, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // ADDQ imm32, m64
    if isImm32Ext(v0, 8) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0x81)
            m.mrsd(0, addr(v[1]), 1)
            m.imm4(toImmAny(v[0]))
        })
    }
    // ADDQ r64, m64
    if isReg64(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[0]), addr(v[1]))
            m.emit(0x01)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ADDQ")
    }
    return p
}

// ADDSD performs "Add Scalar Double-Precision Floating-Point Values".
//
// Mnemonic        : ADDSD
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * ADDSD xmm, xmm
//    * ADDSD m64, xmm
//
func (self *Program) ADDSD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ADDSD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x58)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // ADDSD m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x58)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ADDSD")
    }
    return p
}

// ADDSS performs "Add Scalar Single-Precision Floating-Point Values".
//
// Mnemonic        : ADDSS
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * ADDSS xmm, xmm
//    * ADDSS m32, xmm
//
func (self *Program) ADDSS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ADDSS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x58)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // ADDSS m32, xmm
    if isM32(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x58)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ADDSS")
    }
    return p
}

// ADDSUBPD performs "Packed Double-FP Add/Subtract".
//
// Mnemonic        : ADDSUBPD
// ISA extensions  : SSE3
// Supported forms : (2 forms)
//
//    * ADDSUBPD xmm, xmm
//    * ADDSUBPD m128, xmm
//
func (self *Program) ADDSUBPD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ADDSUBPD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xd0)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // ADDSUBPD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xd0)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ADDSUBPD")
    }
    return p
}

// ADDSUBPS performs "Packed Single-FP Add/Subtract".
//
// Mnemonic        : ADDSUBPS
// ISA extensions  : SSE3
// Supported forms : (2 forms)
//
//    * ADDSUBPS xmm, xmm
//    * ADDSUBPS m128, xmm
//
func (self *Program) ADDSUBPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ADDSUBPS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xd0)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // ADDSUBPS m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xd0)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ADDSUBPS")
    }
    return p
}

// ADDW performs "Add".
//
// Mnemonic        : ADD
// Supported forms : (8 forms)
//
//    * ADDW imm16, ax
//    * ADDW imm8, r16
//    * ADDW imm16, r16
//    * ADDW r16, r16
//    * ADDW m16, r16
//    * ADDW imm8, m16
//    * ADDW imm16, m16
//    * ADDW r16, m16
//
func (self *Program) ADDW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ADDW imm16, ax
    if isImm16(v0) && v1 == AX {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.emit(0x05)
            m.imm2(toImmAny(v[0]))
        })
    }
    // ADDW imm8, r16
    if isImm8Ext(v0, 2) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0x83)
            m.emit(0xc0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // ADDW imm16, r16
    if isImm16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0x81)
            m.emit(0xc0 | lcode(v[1]))
            m.imm2(toImmAny(v[0]))
        })
    }
    // ADDW r16, r16
    if isReg16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x01)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x03)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // ADDW m16, r16
    if isM16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x03)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // ADDW imm8, m16
    if isImm8Ext(v0, 2) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0x83)
            m.mrsd(0, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // ADDW imm16, m16
    if isImm16(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0x81)
            m.mrsd(0, addr(v[1]), 1)
            m.imm2(toImmAny(v[0]))
        })
    }
    // ADDW r16, m16
    if isReg16(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x01)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ADDW")
    }
    return p
}

// ADOXL performs "Unsigned Integer Addition of Two Operands with Overflow Flag".
//
// Mnemonic        : ADOX
// ISA extensions  : ADX
// Supported forms : (2 forms)
//
//    * ADOXL r32, r32
//    * ADOXL m32, r32
//
func (self *Program) ADOXL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ADOXL r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_ADX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xf6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // ADOXL m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_ADX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xf6)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ADOXL")
    }
    return p
}

// ADOXQ performs "Unsigned Integer Addition of Two Operands with Overflow Flag".
//
// Mnemonic        : ADOX
// ISA extensions  : ADX
// Supported forms : (2 forms)
//
//    * ADOXQ r64, r64
//    * ADOXQ m64, r64
//
func (self *Program) ADOXQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ADOXQ r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_ADX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xf6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // ADOXQ m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_ADX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xf6)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ADOXQ")
    }
    return p
}

// AESDEC performs "Perform One Round of an AES Decryption Flow".
//
// Mnemonic        : AESDEC
// ISA extensions  : AES
// Supported forms : (2 forms)
//
//    * AESDEC xmm, xmm
//    * AESDEC m128, xmm
//
func (self *Program) AESDEC(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // AESDEC xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AES)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xde)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // AESDEC m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_AES)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xde)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for AESDEC")
    }
    return p
}

// AESDECLAST performs "Perform Last Round of an AES Decryption Flow".
//
// Mnemonic        : AESDECLAST
// ISA extensions  : AES
// Supported forms : (2 forms)
//
//    * AESDECLAST xmm, xmm
//    * AESDECLAST m128, xmm
//
func (self *Program) AESDECLAST(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // AESDECLAST xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AES)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xdf)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // AESDECLAST m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_AES)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xdf)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for AESDECLAST")
    }
    return p
}

// AESENC performs "Perform One Round of an AES Encryption Flow".
//
// Mnemonic        : AESENC
// ISA extensions  : AES
// Supported forms : (2 forms)
//
//    * AESENC xmm, xmm
//    * AESENC m128, xmm
//
func (self *Program) AESENC(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // AESENC xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AES)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xdc)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // AESENC m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_AES)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xdc)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for AESENC")
    }
    return p
}

// AESENCLAST performs "Perform Last Round of an AES Encryption Flow".
//
// Mnemonic        : AESENCLAST
// ISA extensions  : AES
// Supported forms : (2 forms)
//
//    * AESENCLAST xmm, xmm
//    * AESENCLAST m128, xmm
//
func (self *Program) AESENCLAST(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // AESENCLAST xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AES)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xdd)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // AESENCLAST m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_AES)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xdd)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for AESENCLAST")
    }
    return p
}

// AESIMC performs "Perform the AES InvMixColumn Transformation".
//
// Mnemonic        : AESIMC
// ISA extensions  : AES
// Supported forms : (2 forms)
//
//    * AESIMC xmm, xmm
//    * AESIMC m128, xmm
//
func (self *Program) AESIMC(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // AESIMC xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AES)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xdb)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // AESIMC m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_AES)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xdb)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for AESIMC")
    }
    return p
}

// AESKEYGENASSIST performs "AES Round Key Generation Assist".
//
// Mnemonic        : AESKEYGENASSIST
// ISA extensions  : AES
// Supported forms : (2 forms)
//
//    * AESKEYGENASSIST imm8, xmm, xmm
//    * AESKEYGENASSIST imm8, m128, xmm
//
func (self *Program) AESKEYGENASSIST(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // AESKEYGENASSIST imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AES)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0xdf)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // AESKEYGENASSIST imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_AES)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0xdf)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for AESKEYGENASSIST")
    }
    return p
}

// ANDB performs "Logical AND".
//
// Mnemonic        : AND
// Supported forms : (6 forms)
//
//    * ANDB imm8, al
//    * ANDB imm8, r8
//    * ANDB r8, r8
//    * ANDB m8, r8
//    * ANDB imm8, m8
//    * ANDB r8, m8
//
func (self *Program) ANDB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ANDB imm8, al
    if isImm8(v0) && v1 == AL {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x24)
            m.imm1(toImmAny(v[0]))
        })
    }
    // ANDB imm8, r8
    if isImm8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0x80)
            m.emit(0xe0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // ANDB r8, r8
    if isReg8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], isReg8REX(v[0]) || isReg8REX(v[1]))
            m.emit(0x20)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], isReg8REX(v[0]) || isReg8REX(v[1]))
            m.emit(0x22)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // ANDB m8, r8
    if isM8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), isReg8REX(v[1]))
            m.emit(0x22)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // ANDB imm8, m8
    if isImm8(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0x80)
            m.mrsd(4, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // ANDB r8, m8
    if isReg8(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), isReg8REX(v[0]))
            m.emit(0x20)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ANDB")
    }
    return p
}

// ANDL performs "Logical AND".
//
// Mnemonic        : AND
// Supported forms : (8 forms)
//
//    * ANDL imm32, eax
//    * ANDL imm8, r32
//    * ANDL imm32, r32
//    * ANDL r32, r32
//    * ANDL m32, r32
//    * ANDL imm8, m32
//    * ANDL imm32, m32
//    * ANDL r32, m32
//
func (self *Program) ANDL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ANDL imm32, eax
    if isImm32(v0) && v1 == EAX {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x25)
            m.imm4(toImmAny(v[0]))
        })
    }
    // ANDL imm8, r32
    if isImm8Ext(v0, 4) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0x83)
            m.emit(0xe0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // ANDL imm32, r32
    if isImm32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0x81)
            m.emit(0xe0 | lcode(v[1]))
            m.imm4(toImmAny(v[0]))
        })
    }
    // ANDL r32, r32
    if isReg32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x21)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x23)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // ANDL m32, r32
    if isM32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x23)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // ANDL imm8, m32
    if isImm8Ext(v0, 4) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0x83)
            m.mrsd(4, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // ANDL imm32, m32
    if isImm32(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0x81)
            m.mrsd(4, addr(v[1]), 1)
            m.imm4(toImmAny(v[0]))
        })
    }
    // ANDL r32, m32
    if isReg32(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x21)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ANDL")
    }
    return p
}

// ANDNL performs "Logical AND NOT".
//
// Mnemonic        : ANDN
// ISA extensions  : BMI
// Supported forms : (2 forms)
//
//    * ANDNL r32, r32, r32
//    * ANDNL m32, r32, r32
//
func (self *Program) ANDNL(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // ANDNL r32, r32, r32
    if isReg32(v0) && isReg32(v1) && isReg32(v2) {
        self.require(ISA_BMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x78 ^ (hlcode(v[1]) << 3))
            m.emit(0xf2)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // ANDNL m32, r32, r32
    if isM32(v0) && isReg32(v1) && isReg32(v2) {
        self.require(ISA_BMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x00, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xf2)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ANDNL")
    }
    return p
}

// ANDNPD performs "Bitwise Logical AND NOT of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : ANDNPD
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * ANDNPD xmm, xmm
//    * ANDNPD m128, xmm
//
func (self *Program) ANDNPD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ANDNPD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x55)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // ANDNPD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x55)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ANDNPD")
    }
    return p
}

// ANDNPS performs "Bitwise Logical AND NOT of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : ANDNPS
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * ANDNPS xmm, xmm
//    * ANDNPS m128, xmm
//
func (self *Program) ANDNPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ANDNPS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x55)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // ANDNPS m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x55)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ANDNPS")
    }
    return p
}

// ANDNQ performs "Logical AND NOT".
//
// Mnemonic        : ANDN
// ISA extensions  : BMI
// Supported forms : (2 forms)
//
//    * ANDNQ r64, r64, r64
//    * ANDNQ m64, r64, r64
//
func (self *Program) ANDNQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // ANDNQ r64, r64, r64
    if isReg64(v0) && isReg64(v1) && isReg64(v2) {
        self.require(ISA_BMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf8 ^ (hlcode(v[1]) << 3))
            m.emit(0xf2)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // ANDNQ m64, r64, r64
    if isM64(v0) && isReg64(v1) && isReg64(v2) {
        self.require(ISA_BMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x80, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xf2)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ANDNQ")
    }
    return p
}

// ANDPD performs "Bitwise Logical AND of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : ANDPD
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * ANDPD xmm, xmm
//    * ANDPD m128, xmm
//
func (self *Program) ANDPD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ANDPD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x54)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // ANDPD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x54)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ANDPD")
    }
    return p
}

// ANDPS performs "Bitwise Logical AND of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : ANDPS
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * ANDPS xmm, xmm
//    * ANDPS m128, xmm
//
func (self *Program) ANDPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ANDPS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x54)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // ANDPS m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x54)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ANDPS")
    }
    return p
}

// ANDQ performs "Logical AND".
//
// Mnemonic        : AND
// Supported forms : (8 forms)
//
//    * ANDQ imm32, rax
//    * ANDQ imm8, r64
//    * ANDQ imm32, r64
//    * ANDQ r64, r64
//    * ANDQ m64, r64
//    * ANDQ imm8, m64
//    * ANDQ imm32, m64
//    * ANDQ r64, m64
//
func (self *Program) ANDQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ANDQ imm32, rax
    if isImm32(v0) && v1 == RAX {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48)
            m.emit(0x25)
            m.imm4(toImmAny(v[0]))
        })
    }
    // ANDQ imm8, r64
    if isImm8Ext(v0, 8) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0x83)
            m.emit(0xe0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // ANDQ imm32, r64
    if isImm32Ext(v0, 8) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0x81)
            m.emit(0xe0 | lcode(v[1]))
            m.imm4(toImmAny(v[0]))
        })
    }
    // ANDQ r64, r64
    if isReg64(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[0]) << 2 | hcode(v[1]))
            m.emit(0x21)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x23)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // ANDQ m64, r64
    if isM64(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x23)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // ANDQ imm8, m64
    if isImm8Ext(v0, 8) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0x83)
            m.mrsd(4, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // ANDQ imm32, m64
    if isImm32Ext(v0, 8) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0x81)
            m.mrsd(4, addr(v[1]), 1)
            m.imm4(toImmAny(v[0]))
        })
    }
    // ANDQ r64, m64
    if isReg64(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[0]), addr(v[1]))
            m.emit(0x21)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ANDQ")
    }
    return p
}

// ANDW performs "Logical AND".
//
// Mnemonic        : AND
// Supported forms : (8 forms)
//
//    * ANDW imm16, ax
//    * ANDW imm8, r16
//    * ANDW imm16, r16
//    * ANDW r16, r16
//    * ANDW m16, r16
//    * ANDW imm8, m16
//    * ANDW imm16, m16
//    * ANDW r16, m16
//
func (self *Program) ANDW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ANDW imm16, ax
    if isImm16(v0) && v1 == AX {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.emit(0x25)
            m.imm2(toImmAny(v[0]))
        })
    }
    // ANDW imm8, r16
    if isImm8Ext(v0, 2) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0x83)
            m.emit(0xe0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // ANDW imm16, r16
    if isImm16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0x81)
            m.emit(0xe0 | lcode(v[1]))
            m.imm2(toImmAny(v[0]))
        })
    }
    // ANDW r16, r16
    if isReg16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x21)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x23)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // ANDW m16, r16
    if isM16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x23)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // ANDW imm8, m16
    if isImm8Ext(v0, 2) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0x83)
            m.mrsd(4, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // ANDW imm16, m16
    if isImm16(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0x81)
            m.mrsd(4, addr(v[1]), 1)
            m.imm2(toImmAny(v[0]))
        })
    }
    // ANDW r16, m16
    if isReg16(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x21)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ANDW")
    }
    return p
}

// BEXTR performs "Bit Field Extract".
//
// Mnemonic        : BEXTR
// ISA extensions  : BMI, TBM
// Supported forms : (8 forms)
//
//    * BEXTR imm32, r32, r32
//    * BEXTR r32, r32, r32
//    * BEXTR imm32, m32, r32
//    * BEXTR r32, m32, r32
//    * BEXTR imm32, r64, r64
//    * BEXTR r64, r64, r64
//    * BEXTR imm32, m64, r64
//    * BEXTR r64, m64, r64
//
func (self *Program) BEXTR(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // BEXTR imm32, r32, r32
    if isImm32(v0) && isReg32(v1) && isReg32(v2) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xea ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78)
            m.emit(0x10)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm4(toImmAny(v[0]))
        })
    }
    // BEXTR r32, r32, r32
    if isReg32(v0) && isReg32(v1) && isReg32(v2) {
        self.require(ISA_BMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[0]) << 3))
            m.emit(0xf7)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // BEXTR imm32, m32, r32
    if isImm32(v0) && isM32(v1) && isReg32(v2) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1010, 0x00, hcode(v[2]), addr(v[1]), 0)
            m.emit(0x10)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm4(toImmAny(v[0]))
        })
    }
    // BEXTR r32, m32, r32
    if isReg32(v0) && isM32(v1) && isReg32(v2) {
        self.require(ISA_BMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x00, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0xf7)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    // BEXTR imm32, r64, r64
    if isImm32(v0) && isReg64(v1) && isReg64(v2) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xea ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0xf8)
            m.emit(0x10)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm4(toImmAny(v[0]))
        })
    }
    // BEXTR r64, r64, r64
    if isReg64(v0) && isReg64(v1) && isReg64(v2) {
        self.require(ISA_BMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0xf8 ^ (hlcode(v[0]) << 3))
            m.emit(0xf7)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // BEXTR imm32, m64, r64
    if isImm32(v0) && isM64(v1) && isReg64(v2) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1010, 0x80, hcode(v[2]), addr(v[1]), 0)
            m.emit(0x10)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm4(toImmAny(v[0]))
        })
    }
    // BEXTR r64, m64, r64
    if isReg64(v0) && isM64(v1) && isReg64(v2) {
        self.require(ISA_BMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x80, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0xf7)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for BEXTR")
    }
    return p
}

// BLCFILL performs "Fill From Lowest Clear Bit".
//
// Mnemonic        : BLCFILL
// ISA extensions  : TBM
// Supported forms : (4 forms)
//
//    * BLCFILL r32, r32
//    * BLCFILL m32, r32
//    * BLCFILL r64, r64
//    * BLCFILL m64, r64
//
func (self *Program) BLCFILL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // BLCFILL r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[0]) << 5))
            m.emit(0x78 ^ (hlcode(v[1]) << 3))
            m.emit(0x01)
            m.emit(0xc8 | lcode(v[0]))
        })
    }
    // BLCFILL m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, 0, addr(v[0]), hlcode(v[1]))
            m.emit(0x01)
            m.mrsd(1, addr(v[0]), 1)
        })
    }
    // BLCFILL r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[0]) << 5))
            m.emit(0xf8 ^ (hlcode(v[1]) << 3))
            m.emit(0x01)
            m.emit(0xc8 | lcode(v[0]))
        })
    }
    // BLCFILL m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x80, 0, addr(v[0]), hlcode(v[1]))
            m.emit(0x01)
            m.mrsd(1, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for BLCFILL")
    }
    return p
}

// BLCI performs "Isolate Lowest Clear Bit".
//
// Mnemonic        : BLCI
// ISA extensions  : TBM
// Supported forms : (4 forms)
//
//    * BLCI r32, r32
//    * BLCI m32, r32
//    * BLCI r64, r64
//    * BLCI m64, r64
//
func (self *Program) BLCI(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // BLCI r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[0]) << 5))
            m.emit(0x78 ^ (hlcode(v[1]) << 3))
            m.emit(0x02)
            m.emit(0xf0 | lcode(v[0]))
        })
    }
    // BLCI m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, 0, addr(v[0]), hlcode(v[1]))
            m.emit(0x02)
            m.mrsd(6, addr(v[0]), 1)
        })
    }
    // BLCI r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[0]) << 5))
            m.emit(0xf8 ^ (hlcode(v[1]) << 3))
            m.emit(0x02)
            m.emit(0xf0 | lcode(v[0]))
        })
    }
    // BLCI m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x80, 0, addr(v[0]), hlcode(v[1]))
            m.emit(0x02)
            m.mrsd(6, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for BLCI")
    }
    return p
}

// BLCIC performs "Isolate Lowest Set Bit and Complement".
//
// Mnemonic        : BLCIC
// ISA extensions  : TBM
// Supported forms : (4 forms)
//
//    * BLCIC r32, r32
//    * BLCIC m32, r32
//    * BLCIC r64, r64
//    * BLCIC m64, r64
//
func (self *Program) BLCIC(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // BLCIC r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[0]) << 5))
            m.emit(0x78 ^ (hlcode(v[1]) << 3))
            m.emit(0x01)
            m.emit(0xe8 | lcode(v[0]))
        })
    }
    // BLCIC m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, 0, addr(v[0]), hlcode(v[1]))
            m.emit(0x01)
            m.mrsd(5, addr(v[0]), 1)
        })
    }
    // BLCIC r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[0]) << 5))
            m.emit(0xf8 ^ (hlcode(v[1]) << 3))
            m.emit(0x01)
            m.emit(0xe8 | lcode(v[0]))
        })
    }
    // BLCIC m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x80, 0, addr(v[0]), hlcode(v[1]))
            m.emit(0x01)
            m.mrsd(5, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for BLCIC")
    }
    return p
}

// BLCMSK performs "Mask From Lowest Clear Bit".
//
// Mnemonic        : BLCMSK
// ISA extensions  : TBM
// Supported forms : (4 forms)
//
//    * BLCMSK r32, r32
//    * BLCMSK m32, r32
//    * BLCMSK r64, r64
//    * BLCMSK m64, r64
//
func (self *Program) BLCMSK(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // BLCMSK r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[0]) << 5))
            m.emit(0x78 ^ (hlcode(v[1]) << 3))
            m.emit(0x02)
            m.emit(0xc8 | lcode(v[0]))
        })
    }
    // BLCMSK m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, 0, addr(v[0]), hlcode(v[1]))
            m.emit(0x02)
            m.mrsd(1, addr(v[0]), 1)
        })
    }
    // BLCMSK r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[0]) << 5))
            m.emit(0xf8 ^ (hlcode(v[1]) << 3))
            m.emit(0x02)
            m.emit(0xc8 | lcode(v[0]))
        })
    }
    // BLCMSK m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x80, 0, addr(v[0]), hlcode(v[1]))
            m.emit(0x02)
            m.mrsd(1, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for BLCMSK")
    }
    return p
}

// BLCS performs "Set Lowest Clear Bit".
//
// Mnemonic        : BLCS
// ISA extensions  : TBM
// Supported forms : (4 forms)
//
//    * BLCS r32, r32
//    * BLCS m32, r32
//    * BLCS r64, r64
//    * BLCS m64, r64
//
func (self *Program) BLCS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // BLCS r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[0]) << 5))
            m.emit(0x78 ^ (hlcode(v[1]) << 3))
            m.emit(0x01)
            m.emit(0xd8 | lcode(v[0]))
        })
    }
    // BLCS m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, 0, addr(v[0]), hlcode(v[1]))
            m.emit(0x01)
            m.mrsd(3, addr(v[0]), 1)
        })
    }
    // BLCS r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[0]) << 5))
            m.emit(0xf8 ^ (hlcode(v[1]) << 3))
            m.emit(0x01)
            m.emit(0xd8 | lcode(v[0]))
        })
    }
    // BLCS m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x80, 0, addr(v[0]), hlcode(v[1]))
            m.emit(0x01)
            m.mrsd(3, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for BLCS")
    }
    return p
}

// BLENDPD performs "Blend Packed Double Precision Floating-Point Values".
//
// Mnemonic        : BLENDPD
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * BLENDPD imm8, xmm, xmm
//    * BLENDPD imm8, m128, xmm
//
func (self *Program) BLENDPD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // BLENDPD imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x0d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // BLENDPD imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x0d)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for BLENDPD")
    }
    return p
}

// BLENDPS performs " Blend Packed Single Precision Floating-Point Values".
//
// Mnemonic        : BLENDPS
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * BLENDPS imm8, xmm, xmm
//    * BLENDPS imm8, m128, xmm
//
func (self *Program) BLENDPS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // BLENDPS imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x0c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // BLENDPS imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x0c)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for BLENDPS")
    }
    return p
}

// BLENDVPD performs " Variable Blend Packed Double Precision Floating-Point Values".
//
// Mnemonic        : BLENDVPD
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * BLENDVPD xmm0, xmm, xmm
//    * BLENDVPD xmm0, m128, xmm
//
func (self *Program) BLENDVPD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // BLENDVPD xmm0, xmm, xmm
    if v0 == XMM0 && isXMM(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x15)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // BLENDVPD xmm0, m128, xmm
    if v0 == XMM0 && isM128(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x15)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for BLENDVPD")
    }
    return p
}

// BLENDVPS performs " Variable Blend Packed Single Precision Floating-Point Values".
//
// Mnemonic        : BLENDVPS
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * BLENDVPS xmm0, xmm, xmm
//    * BLENDVPS xmm0, m128, xmm
//
func (self *Program) BLENDVPS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // BLENDVPS xmm0, xmm, xmm
    if v0 == XMM0 && isXMM(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x14)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // BLENDVPS xmm0, m128, xmm
    if v0 == XMM0 && isM128(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x14)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for BLENDVPS")
    }
    return p
}

// BLSFILL performs "Fill From Lowest Set Bit".
//
// Mnemonic        : BLSFILL
// ISA extensions  : TBM
// Supported forms : (4 forms)
//
//    * BLSFILL r32, r32
//    * BLSFILL m32, r32
//    * BLSFILL r64, r64
//    * BLSFILL m64, r64
//
func (self *Program) BLSFILL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // BLSFILL r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[0]) << 5))
            m.emit(0x78 ^ (hlcode(v[1]) << 3))
            m.emit(0x01)
            m.emit(0xd0 | lcode(v[0]))
        })
    }
    // BLSFILL m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, 0, addr(v[0]), hlcode(v[1]))
            m.emit(0x01)
            m.mrsd(2, addr(v[0]), 1)
        })
    }
    // BLSFILL r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[0]) << 5))
            m.emit(0xf8 ^ (hlcode(v[1]) << 3))
            m.emit(0x01)
            m.emit(0xd0 | lcode(v[0]))
        })
    }
    // BLSFILL m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x80, 0, addr(v[0]), hlcode(v[1]))
            m.emit(0x01)
            m.mrsd(2, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for BLSFILL")
    }
    return p
}

// BLSI performs "Isolate Lowest Set Bit".
//
// Mnemonic        : BLSI
// ISA extensions  : BMI
// Supported forms : (4 forms)
//
//    * BLSI r32, r32
//    * BLSI m32, r32
//    * BLSI r64, r64
//    * BLSI m64, r64
//
func (self *Program) BLSI(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // BLSI r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_BMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[0]) << 5))
            m.emit(0x78 ^ (hlcode(v[1]) << 3))
            m.emit(0xf3)
            m.emit(0xd8 | lcode(v[0]))
        })
    }
    // BLSI m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_BMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x00, 0, addr(v[0]), hlcode(v[1]))
            m.emit(0xf3)
            m.mrsd(3, addr(v[0]), 1)
        })
    }
    // BLSI r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_BMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[0]) << 5))
            m.emit(0xf8 ^ (hlcode(v[1]) << 3))
            m.emit(0xf3)
            m.emit(0xd8 | lcode(v[0]))
        })
    }
    // BLSI m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_BMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x80, 0, addr(v[0]), hlcode(v[1]))
            m.emit(0xf3)
            m.mrsd(3, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for BLSI")
    }
    return p
}

// BLSIC performs "Isolate Lowest Set Bit and Complement".
//
// Mnemonic        : BLSIC
// ISA extensions  : TBM
// Supported forms : (4 forms)
//
//    * BLSIC r32, r32
//    * BLSIC m32, r32
//    * BLSIC r64, r64
//    * BLSIC m64, r64
//
func (self *Program) BLSIC(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // BLSIC r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[0]) << 5))
            m.emit(0x78 ^ (hlcode(v[1]) << 3))
            m.emit(0x01)
            m.emit(0xf0 | lcode(v[0]))
        })
    }
    // BLSIC m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, 0, addr(v[0]), hlcode(v[1]))
            m.emit(0x01)
            m.mrsd(6, addr(v[0]), 1)
        })
    }
    // BLSIC r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[0]) << 5))
            m.emit(0xf8 ^ (hlcode(v[1]) << 3))
            m.emit(0x01)
            m.emit(0xf0 | lcode(v[0]))
        })
    }
    // BLSIC m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x80, 0, addr(v[0]), hlcode(v[1]))
            m.emit(0x01)
            m.mrsd(6, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for BLSIC")
    }
    return p
}

// BLSMSK performs "Mask From Lowest Set Bit".
//
// Mnemonic        : BLSMSK
// ISA extensions  : BMI
// Supported forms : (4 forms)
//
//    * BLSMSK r32, r32
//    * BLSMSK m32, r32
//    * BLSMSK r64, r64
//    * BLSMSK m64, r64
//
func (self *Program) BLSMSK(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // BLSMSK r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_BMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[0]) << 5))
            m.emit(0x78 ^ (hlcode(v[1]) << 3))
            m.emit(0xf3)
            m.emit(0xd0 | lcode(v[0]))
        })
    }
    // BLSMSK m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_BMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x00, 0, addr(v[0]), hlcode(v[1]))
            m.emit(0xf3)
            m.mrsd(2, addr(v[0]), 1)
        })
    }
    // BLSMSK r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_BMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[0]) << 5))
            m.emit(0xf8 ^ (hlcode(v[1]) << 3))
            m.emit(0xf3)
            m.emit(0xd0 | lcode(v[0]))
        })
    }
    // BLSMSK m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_BMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x80, 0, addr(v[0]), hlcode(v[1]))
            m.emit(0xf3)
            m.mrsd(2, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for BLSMSK")
    }
    return p
}

// BLSR performs "Reset Lowest Set Bit".
//
// Mnemonic        : BLSR
// ISA extensions  : BMI
// Supported forms : (4 forms)
//
//    * BLSR r32, r32
//    * BLSR m32, r32
//    * BLSR r64, r64
//    * BLSR m64, r64
//
func (self *Program) BLSR(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // BLSR r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_BMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[0]) << 5))
            m.emit(0x78 ^ (hlcode(v[1]) << 3))
            m.emit(0xf3)
            m.emit(0xc8 | lcode(v[0]))
        })
    }
    // BLSR m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_BMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x00, 0, addr(v[0]), hlcode(v[1]))
            m.emit(0xf3)
            m.mrsd(1, addr(v[0]), 1)
        })
    }
    // BLSR r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_BMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[0]) << 5))
            m.emit(0xf8 ^ (hlcode(v[1]) << 3))
            m.emit(0xf3)
            m.emit(0xc8 | lcode(v[0]))
        })
    }
    // BLSR m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_BMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x80, 0, addr(v[0]), hlcode(v[1]))
            m.emit(0xf3)
            m.mrsd(1, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for BLSR")
    }
    return p
}

// BSFL performs "Bit Scan Forward".
//
// Mnemonic        : BSF
// Supported forms : (2 forms)
//
//    * BSFL r32, r32
//    * BSFL m32, r32
//
func (self *Program) BSFL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // BSFL r32, r32
    if isReg32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xbc)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // BSFL m32, r32
    if isM32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xbc)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for BSFL")
    }
    return p
}

// BSFQ performs "Bit Scan Forward".
//
// Mnemonic        : BSF
// Supported forms : (2 forms)
//
//    * BSFQ r64, r64
//    * BSFQ m64, r64
//
func (self *Program) BSFQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // BSFQ r64, r64
    if isReg64(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0xbc)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // BSFQ m64, r64
    if isM64(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0xbc)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for BSFQ")
    }
    return p
}

// BSFW performs "Bit Scan Forward".
//
// Mnemonic        : BSF
// Supported forms : (2 forms)
//
//    * BSFW r16, r16
//    * BSFW m16, r16
//
func (self *Program) BSFW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // BSFW r16, r16
    if isReg16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xbc)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // BSFW m16, r16
    if isM16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xbc)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for BSFW")
    }
    return p
}

// BSRL performs "Bit Scan Reverse".
//
// Mnemonic        : BSR
// Supported forms : (2 forms)
//
//    * BSRL r32, r32
//    * BSRL m32, r32
//
func (self *Program) BSRL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // BSRL r32, r32
    if isReg32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xbd)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // BSRL m32, r32
    if isM32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xbd)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for BSRL")
    }
    return p
}

// BSRQ performs "Bit Scan Reverse".
//
// Mnemonic        : BSR
// Supported forms : (2 forms)
//
//    * BSRQ r64, r64
//    * BSRQ m64, r64
//
func (self *Program) BSRQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // BSRQ r64, r64
    if isReg64(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0xbd)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // BSRQ m64, r64
    if isM64(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0xbd)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for BSRQ")
    }
    return p
}

// BSRW performs "Bit Scan Reverse".
//
// Mnemonic        : BSR
// Supported forms : (2 forms)
//
//    * BSRW r16, r16
//    * BSRW m16, r16
//
func (self *Program) BSRW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // BSRW r16, r16
    if isReg16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xbd)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // BSRW m16, r16
    if isM16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xbd)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for BSRW")
    }
    return p
}

// BSWAPL performs "Byte Swap".
//
// Mnemonic        : BSWAP
// Supported forms : (1 form)
//
//    * BSWAPL r32
//
func (self *Program) BSWAPL(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // BSWAPL r32
    if isReg32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], false)
            m.emit(0x0f)
            m.emit(0xc8 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for BSWAPL")
    }
    return p
}

// BSWAPQ performs "Byte Swap".
//
// Mnemonic        : BSWAP
// Supported forms : (1 form)
//
//    * BSWAPQ r64
//
func (self *Program) BSWAPQ(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // BSWAPQ r64
    if isReg64(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0xc8 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for BSWAPQ")
    }
    return p
}

// BTCL performs "Bit Test and Complement".
//
// Mnemonic        : BTC
// Supported forms : (4 forms)
//
//    * BTCL imm8, r32
//    * BTCL r32, r32
//    * BTCL imm8, m32
//    * BTCL r32, m32
//
func (self *Program) BTCL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // BTCL imm8, r32
    if isImm8(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0x0f)
            m.emit(0xba)
            m.emit(0xf8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // BTCL r32, r32
    if isReg32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x0f)
            m.emit(0xbb)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // BTCL imm8, m32
    if isImm8(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0xba)
            m.mrsd(7, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // BTCL r32, m32
    if isReg32(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0xbb)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for BTCL")
    }
    return p
}

// BTCQ performs "Bit Test and Complement".
//
// Mnemonic        : BTC
// Supported forms : (4 forms)
//
//    * BTCQ imm8, r64
//    * BTCQ r64, r64
//    * BTCQ imm8, m64
//    * BTCQ r64, m64
//
func (self *Program) BTCQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // BTCQ imm8, r64
    if isImm8(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0x0f)
            m.emit(0xba)
            m.emit(0xf8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // BTCQ r64, r64
    if isReg64(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[0]) << 2 | hcode(v[1]))
            m.emit(0x0f)
            m.emit(0xbb)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // BTCQ imm8, m64
    if isImm8(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0x0f)
            m.emit(0xba)
            m.mrsd(7, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // BTCQ r64, m64
    if isReg64(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[0]), addr(v[1]))
            m.emit(0x0f)
            m.emit(0xbb)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for BTCQ")
    }
    return p
}

// BTCW performs "Bit Test and Complement".
//
// Mnemonic        : BTC
// Supported forms : (4 forms)
//
//    * BTCW imm8, r16
//    * BTCW r16, r16
//    * BTCW imm8, m16
//    * BTCW r16, m16
//
func (self *Program) BTCW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // BTCW imm8, r16
    if isImm8(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0x0f)
            m.emit(0xba)
            m.emit(0xf8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // BTCW r16, r16
    if isReg16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x0f)
            m.emit(0xbb)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // BTCW imm8, m16
    if isImm8(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0xba)
            m.mrsd(7, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // BTCW r16, m16
    if isReg16(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0xbb)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for BTCW")
    }
    return p
}

// BTL performs "Bit Test".
//
// Mnemonic        : BT
// Supported forms : (4 forms)
//
//    * BTL imm8, r32
//    * BTL r32, r32
//    * BTL imm8, m32
//    * BTL r32, m32
//
func (self *Program) BTL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // BTL imm8, r32
    if isImm8(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0x0f)
            m.emit(0xba)
            m.emit(0xe0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // BTL r32, r32
    if isReg32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x0f)
            m.emit(0xa3)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // BTL imm8, m32
    if isImm8(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0xba)
            m.mrsd(4, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // BTL r32, m32
    if isReg32(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0xa3)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for BTL")
    }
    return p
}

// BTQ performs "Bit Test".
//
// Mnemonic        : BT
// Supported forms : (4 forms)
//
//    * BTQ imm8, r64
//    * BTQ r64, r64
//    * BTQ imm8, m64
//    * BTQ r64, m64
//
func (self *Program) BTQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // BTQ imm8, r64
    if isImm8(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0x0f)
            m.emit(0xba)
            m.emit(0xe0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // BTQ r64, r64
    if isReg64(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[0]) << 2 | hcode(v[1]))
            m.emit(0x0f)
            m.emit(0xa3)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // BTQ imm8, m64
    if isImm8(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0x0f)
            m.emit(0xba)
            m.mrsd(4, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // BTQ r64, m64
    if isReg64(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[0]), addr(v[1]))
            m.emit(0x0f)
            m.emit(0xa3)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for BTQ")
    }
    return p
}

// BTRL performs "Bit Test and Reset".
//
// Mnemonic        : BTR
// Supported forms : (4 forms)
//
//    * BTRL imm8, r32
//    * BTRL r32, r32
//    * BTRL imm8, m32
//    * BTRL r32, m32
//
func (self *Program) BTRL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // BTRL imm8, r32
    if isImm8(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0x0f)
            m.emit(0xba)
            m.emit(0xf0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // BTRL r32, r32
    if isReg32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x0f)
            m.emit(0xb3)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // BTRL imm8, m32
    if isImm8(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0xba)
            m.mrsd(6, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // BTRL r32, m32
    if isReg32(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0xb3)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for BTRL")
    }
    return p
}

// BTRQ performs "Bit Test and Reset".
//
// Mnemonic        : BTR
// Supported forms : (4 forms)
//
//    * BTRQ imm8, r64
//    * BTRQ r64, r64
//    * BTRQ imm8, m64
//    * BTRQ r64, m64
//
func (self *Program) BTRQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // BTRQ imm8, r64
    if isImm8(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0x0f)
            m.emit(0xba)
            m.emit(0xf0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // BTRQ r64, r64
    if isReg64(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[0]) << 2 | hcode(v[1]))
            m.emit(0x0f)
            m.emit(0xb3)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // BTRQ imm8, m64
    if isImm8(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0x0f)
            m.emit(0xba)
            m.mrsd(6, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // BTRQ r64, m64
    if isReg64(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[0]), addr(v[1]))
            m.emit(0x0f)
            m.emit(0xb3)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for BTRQ")
    }
    return p
}

// BTRW performs "Bit Test and Reset".
//
// Mnemonic        : BTR
// Supported forms : (4 forms)
//
//    * BTRW imm8, r16
//    * BTRW r16, r16
//    * BTRW imm8, m16
//    * BTRW r16, m16
//
func (self *Program) BTRW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // BTRW imm8, r16
    if isImm8(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0x0f)
            m.emit(0xba)
            m.emit(0xf0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // BTRW r16, r16
    if isReg16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x0f)
            m.emit(0xb3)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // BTRW imm8, m16
    if isImm8(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0xba)
            m.mrsd(6, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // BTRW r16, m16
    if isReg16(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0xb3)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for BTRW")
    }
    return p
}

// BTSL performs "Bit Test and Set".
//
// Mnemonic        : BTS
// Supported forms : (4 forms)
//
//    * BTSL imm8, r32
//    * BTSL r32, r32
//    * BTSL imm8, m32
//    * BTSL r32, m32
//
func (self *Program) BTSL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // BTSL imm8, r32
    if isImm8(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0x0f)
            m.emit(0xba)
            m.emit(0xe8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // BTSL r32, r32
    if isReg32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x0f)
            m.emit(0xab)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // BTSL imm8, m32
    if isImm8(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0xba)
            m.mrsd(5, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // BTSL r32, m32
    if isReg32(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0xab)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for BTSL")
    }
    return p
}

// BTSQ performs "Bit Test and Set".
//
// Mnemonic        : BTS
// Supported forms : (4 forms)
//
//    * BTSQ imm8, r64
//    * BTSQ r64, r64
//    * BTSQ imm8, m64
//    * BTSQ r64, m64
//
func (self *Program) BTSQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // BTSQ imm8, r64
    if isImm8(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0x0f)
            m.emit(0xba)
            m.emit(0xe8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // BTSQ r64, r64
    if isReg64(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[0]) << 2 | hcode(v[1]))
            m.emit(0x0f)
            m.emit(0xab)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // BTSQ imm8, m64
    if isImm8(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0x0f)
            m.emit(0xba)
            m.mrsd(5, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // BTSQ r64, m64
    if isReg64(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[0]), addr(v[1]))
            m.emit(0x0f)
            m.emit(0xab)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for BTSQ")
    }
    return p
}

// BTSW performs "Bit Test and Set".
//
// Mnemonic        : BTS
// Supported forms : (4 forms)
//
//    * BTSW imm8, r16
//    * BTSW r16, r16
//    * BTSW imm8, m16
//    * BTSW r16, m16
//
func (self *Program) BTSW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // BTSW imm8, r16
    if isImm8(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0x0f)
            m.emit(0xba)
            m.emit(0xe8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // BTSW r16, r16
    if isReg16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x0f)
            m.emit(0xab)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // BTSW imm8, m16
    if isImm8(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0xba)
            m.mrsd(5, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // BTSW r16, m16
    if isReg16(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0xab)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for BTSW")
    }
    return p
}

// BTW performs "Bit Test".
//
// Mnemonic        : BT
// Supported forms : (4 forms)
//
//    * BTW imm8, r16
//    * BTW r16, r16
//    * BTW imm8, m16
//    * BTW r16, m16
//
func (self *Program) BTW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // BTW imm8, r16
    if isImm8(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0x0f)
            m.emit(0xba)
            m.emit(0xe0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // BTW r16, r16
    if isReg16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x0f)
            m.emit(0xa3)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // BTW imm8, m16
    if isImm8(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0xba)
            m.mrsd(4, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // BTW r16, m16
    if isReg16(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0xa3)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for BTW")
    }
    return p
}

// BZHI performs "Zero High Bits Starting with Specified Bit Position".
//
// Mnemonic        : BZHI
// ISA extensions  : BMI2
// Supported forms : (4 forms)
//
//    * BZHI r32, r32, r32
//    * BZHI r32, m32, r32
//    * BZHI r64, r64, r64
//    * BZHI r64, m64, r64
//
func (self *Program) BZHI(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // BZHI r32, r32, r32
    if isReg32(v0) && isReg32(v1) && isReg32(v2) {
        self.require(ISA_BMI2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[0]) << 3))
            m.emit(0xf5)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // BZHI r32, m32, r32
    if isReg32(v0) && isM32(v1) && isReg32(v2) {
        self.require(ISA_BMI2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x00, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0xf5)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    // BZHI r64, r64, r64
    if isReg64(v0) && isReg64(v1) && isReg64(v2) {
        self.require(ISA_BMI2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0xf8 ^ (hlcode(v[0]) << 3))
            m.emit(0xf5)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // BZHI r64, m64, r64
    if isReg64(v0) && isM64(v1) && isReg64(v2) {
        self.require(ISA_BMI2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x80, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0xf5)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for BZHI")
    }
    return p
}

// CALL performs "Call Procedure".
//
// Mnemonic        : CALL
// Supported forms : (1 form)
//
//    * CALL rel32
//
func (self *Program) CALL(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // CALL rel32
    if isRel32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xe8)
            m.imm4(relv(v[0]))
        })
    }
    // CALL label
    if isLabel(v0) {
        p.add(_REL4, func(m *_Encoding, v []interface{}) {
            m.emit(0xe8)
            m.imm4(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for CALL")
    }
    return p
}

// CALLQ performs "Call Procedure".
//
// Mnemonic        : CALL
// Supported forms : (2 forms)
//
//    * CALLQ r64
//    * CALLQ m64
//
func (self *Program) CALLQ(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // CALLQ r64
    if isReg64(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], false)
            m.emit(0xff)
            m.emit(0xd0 | lcode(v[0]))
        })
    }
    // CALLQ m64
    if isM64(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0xff)
            m.mrsd(2, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CALLQ")
    }
    return p
}

// CBTW performs "Convert Byte to Word".
//
// Mnemonic        : CBW
// Supported forms : (1 form)
//
//    * CBTW
//
func (self *Program) CBTW() *Instruction {
    p := self.alloc(0, Operands{})
    // CBTW
    p.add(0, func(m *_Encoding, v []interface{}) {
        m.emit(0x66)
        m.emit(0x98)
    })
    return p
}

// CLC performs "Clear Carry Flag".
//
// Mnemonic        : CLC
// Supported forms : (1 form)
//
//    * CLC
//
func (self *Program) CLC() *Instruction {
    p := self.alloc(0, Operands{})
    // CLC
    p.add(0, func(m *_Encoding, v []interface{}) {
        m.emit(0xf8)
    })
    return p
}

// CLD performs "Clear Direction Flag".
//
// Mnemonic        : CLD
// Supported forms : (1 form)
//
//    * CLD
//
func (self *Program) CLD() *Instruction {
    p := self.alloc(0, Operands{})
    // CLD
    p.add(0, func(m *_Encoding, v []interface{}) {
        m.emit(0xfc)
    })
    return p
}

// CLFLUSH performs "Flush Cache Line".
//
// Mnemonic        : CLFLUSH
// ISA extensions  : CLFLUSH
// Supported forms : (1 form)
//
//    * CLFLUSH m8
//
func (self *Program) CLFLUSH(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // CLFLUSH m8
    if isM8(v0) {
        self.require(ISA_CLFLUSH)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xae)
            m.mrsd(7, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CLFLUSH")
    }
    return p
}

// CLFLUSHOPT performs "Flush Cache Line Optimized".
//
// Mnemonic        : CLFLUSHOPT
// ISA extensions  : CLFLUSHOPT
// Supported forms : (1 form)
//
//    * CLFLUSHOPT m8
//
func (self *Program) CLFLUSHOPT(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // CLFLUSHOPT m8
    if isM8(v0) {
        self.require(ISA_CLFLUSHOPT)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xae)
            m.mrsd(7, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CLFLUSHOPT")
    }
    return p
}

// CLTD performs "Convert Doubleword to Quadword".
//
// Mnemonic        : CDQ
// Supported forms : (1 form)
//
//    * CLTD
//
func (self *Program) CLTD() *Instruction {
    p := self.alloc(0, Operands{})
    // CLTD
    p.add(0, func(m *_Encoding, v []interface{}) {
        m.emit(0x99)
    })
    return p
}

// CLTQ performs "Convert Doubleword to Quadword".
//
// Mnemonic        : CDQE
// Supported forms : (1 form)
//
//    * CLTQ
//
func (self *Program) CLTQ() *Instruction {
    p := self.alloc(0, Operands{})
    // CLTQ
    p.add(0, func(m *_Encoding, v []interface{}) {
        m.emit(0x48)
        m.emit(0x98)
    })
    return p
}

// CLWB performs "Cache Line Write Back".
//
// Mnemonic        : CLWB
// ISA extensions  : CLWB
// Supported forms : (1 form)
//
//    * CLWB m8
//
func (self *Program) CLWB(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // CLWB m8
    if isM8(v0) {
        self.require(ISA_CLWB)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xae)
            m.mrsd(6, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CLWB")
    }
    return p
}

// CLZERO performs "Zero-out 64-bit Cache Line".
//
// Mnemonic        : CLZERO
// ISA extensions  : CLZERO
// Supported forms : (1 form)
//
//    * CLZERO
//
func (self *Program) CLZERO() *Instruction {
    p := self.alloc(0, Operands{})
    // CLZERO
    self.require(ISA_CLZERO)
    p.add(0, func(m *_Encoding, v []interface{}) {
        m.emit(0x0f)
        m.emit(0x01)
        m.emit(0xfc)
    })
    return p
}

// CMC performs "Complement Carry Flag".
//
// Mnemonic        : CMC
// Supported forms : (1 form)
//
//    * CMC
//
func (self *Program) CMC() *Instruction {
    p := self.alloc(0, Operands{})
    // CMC
    p.add(0, func(m *_Encoding, v []interface{}) {
        m.emit(0xf5)
    })
    return p
}

// CMOVA performs "Move if above (CF == 0 and ZF == 0)".
//
// Mnemonic        : CMOVA
// ISA extensions  : CMOV
// Supported forms : (6 forms)
//
//    * CMOVA r16, r16
//    * CMOVA m16, r16
//    * CMOVA r32, r32
//    * CMOVA m32, r32
//    * CMOVA r64, r64
//    * CMOVA m64, r64
//
func (self *Program) CMOVA(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMOVA r16, r16
    if isReg16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x47)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVA m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x47)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVA r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x47)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVA m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x47)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVA r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x47)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVA m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x47)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMOVA")
    }
    return p
}

// CMOVAE performs "Move if above or equal (CF == 0)".
//
// Mnemonic        : CMOVAE
// ISA extensions  : CMOV
// Supported forms : (6 forms)
//
//    * CMOVAE r16, r16
//    * CMOVAE m16, r16
//    * CMOVAE r32, r32
//    * CMOVAE m32, r32
//    * CMOVAE r64, r64
//    * CMOVAE m64, r64
//
func (self *Program) CMOVAE(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMOVAE r16, r16
    if isReg16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x43)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVAE m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x43)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVAE r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x43)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVAE m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x43)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVAE r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x43)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVAE m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x43)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMOVAE")
    }
    return p
}

// CMOVB performs "Move if below (CF == 1)".
//
// Mnemonic        : CMOVB
// ISA extensions  : CMOV
// Supported forms : (6 forms)
//
//    * CMOVB r16, r16
//    * CMOVB m16, r16
//    * CMOVB r32, r32
//    * CMOVB m32, r32
//    * CMOVB r64, r64
//    * CMOVB m64, r64
//
func (self *Program) CMOVB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMOVB r16, r16
    if isReg16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x42)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVB m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x42)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVB r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x42)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVB m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x42)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVB r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x42)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVB m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x42)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMOVB")
    }
    return p
}

// CMOVBE performs "Move if below or equal (CF == 1 or ZF == 1)".
//
// Mnemonic        : CMOVBE
// ISA extensions  : CMOV
// Supported forms : (6 forms)
//
//    * CMOVBE r16, r16
//    * CMOVBE m16, r16
//    * CMOVBE r32, r32
//    * CMOVBE m32, r32
//    * CMOVBE r64, r64
//    * CMOVBE m64, r64
//
func (self *Program) CMOVBE(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMOVBE r16, r16
    if isReg16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x46)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVBE m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x46)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVBE r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x46)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVBE m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x46)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVBE r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x46)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVBE m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x46)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMOVBE")
    }
    return p
}

// CMOVC performs "Move if carry (CF == 1)".
//
// Mnemonic        : CMOVC
// ISA extensions  : CMOV
// Supported forms : (6 forms)
//
//    * CMOVC r16, r16
//    * CMOVC m16, r16
//    * CMOVC r32, r32
//    * CMOVC m32, r32
//    * CMOVC r64, r64
//    * CMOVC m64, r64
//
func (self *Program) CMOVC(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMOVC r16, r16
    if isReg16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x42)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVC m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x42)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVC r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x42)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVC m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x42)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVC r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x42)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVC m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x42)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMOVC")
    }
    return p
}

// CMOVE performs "Move if equal (ZF == 1)".
//
// Mnemonic        : CMOVE
// ISA extensions  : CMOV
// Supported forms : (6 forms)
//
//    * CMOVE r16, r16
//    * CMOVE m16, r16
//    * CMOVE r32, r32
//    * CMOVE m32, r32
//    * CMOVE r64, r64
//    * CMOVE m64, r64
//
func (self *Program) CMOVE(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMOVE r16, r16
    if isReg16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x44)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVE m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x44)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVE r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x44)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVE m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x44)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVE r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x44)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVE m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x44)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMOVE")
    }
    return p
}

// CMOVG performs "Move if greater (ZF == 0 and SF == OF)".
//
// Mnemonic        : CMOVG
// ISA extensions  : CMOV
// Supported forms : (6 forms)
//
//    * CMOVG r16, r16
//    * CMOVG m16, r16
//    * CMOVG r32, r32
//    * CMOVG m32, r32
//    * CMOVG r64, r64
//    * CMOVG m64, r64
//
func (self *Program) CMOVG(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMOVG r16, r16
    if isReg16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x4f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVG m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x4f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVG r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x4f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVG m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x4f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVG r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x4f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVG m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x4f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMOVG")
    }
    return p
}

// CMOVGE performs "Move if greater or equal (SF == OF)".
//
// Mnemonic        : CMOVGE
// ISA extensions  : CMOV
// Supported forms : (6 forms)
//
//    * CMOVGE r16, r16
//    * CMOVGE m16, r16
//    * CMOVGE r32, r32
//    * CMOVGE m32, r32
//    * CMOVGE r64, r64
//    * CMOVGE m64, r64
//
func (self *Program) CMOVGE(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMOVGE r16, r16
    if isReg16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x4d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVGE m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x4d)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVGE r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x4d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVGE m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x4d)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVGE r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x4d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVGE m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x4d)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMOVGE")
    }
    return p
}

// CMOVL performs "Move if less (SF != OF)".
//
// Mnemonic        : CMOVL
// ISA extensions  : CMOV
// Supported forms : (6 forms)
//
//    * CMOVL r16, r16
//    * CMOVL m16, r16
//    * CMOVL r32, r32
//    * CMOVL m32, r32
//    * CMOVL r64, r64
//    * CMOVL m64, r64
//
func (self *Program) CMOVL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMOVL r16, r16
    if isReg16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x4c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVL m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x4c)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVL r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x4c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVL m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x4c)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVL r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x4c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVL m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x4c)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMOVL")
    }
    return p
}

// CMOVLE performs "Move if less or equal (ZF == 1 or SF != OF)".
//
// Mnemonic        : CMOVLE
// ISA extensions  : CMOV
// Supported forms : (6 forms)
//
//    * CMOVLE r16, r16
//    * CMOVLE m16, r16
//    * CMOVLE r32, r32
//    * CMOVLE m32, r32
//    * CMOVLE r64, r64
//    * CMOVLE m64, r64
//
func (self *Program) CMOVLE(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMOVLE r16, r16
    if isReg16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x4e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVLE m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x4e)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVLE r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x4e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVLE m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x4e)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVLE r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x4e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVLE m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x4e)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMOVLE")
    }
    return p
}

// CMOVNA performs "Move if not above (CF == 1 or ZF == 1)".
//
// Mnemonic        : CMOVNA
// ISA extensions  : CMOV
// Supported forms : (6 forms)
//
//    * CMOVNA r16, r16
//    * CMOVNA m16, r16
//    * CMOVNA r32, r32
//    * CMOVNA m32, r32
//    * CMOVNA r64, r64
//    * CMOVNA m64, r64
//
func (self *Program) CMOVNA(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMOVNA r16, r16
    if isReg16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x46)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNA m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x46)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVNA r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x46)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNA m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x46)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVNA r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x46)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNA m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x46)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMOVNA")
    }
    return p
}

// CMOVNAE performs "Move if not above or equal (CF == 1)".
//
// Mnemonic        : CMOVNAE
// ISA extensions  : CMOV
// Supported forms : (6 forms)
//
//    * CMOVNAE r16, r16
//    * CMOVNAE m16, r16
//    * CMOVNAE r32, r32
//    * CMOVNAE m32, r32
//    * CMOVNAE r64, r64
//    * CMOVNAE m64, r64
//
func (self *Program) CMOVNAE(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMOVNAE r16, r16
    if isReg16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x42)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNAE m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x42)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVNAE r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x42)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNAE m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x42)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVNAE r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x42)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNAE m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x42)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMOVNAE")
    }
    return p
}

// CMOVNB performs "Move if not below (CF == 0)".
//
// Mnemonic        : CMOVNB
// ISA extensions  : CMOV
// Supported forms : (6 forms)
//
//    * CMOVNB r16, r16
//    * CMOVNB m16, r16
//    * CMOVNB r32, r32
//    * CMOVNB m32, r32
//    * CMOVNB r64, r64
//    * CMOVNB m64, r64
//
func (self *Program) CMOVNB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMOVNB r16, r16
    if isReg16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x43)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNB m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x43)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVNB r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x43)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNB m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x43)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVNB r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x43)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNB m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x43)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMOVNB")
    }
    return p
}

// CMOVNBE performs "Move if not below or equal (CF == 0 and ZF == 0)".
//
// Mnemonic        : CMOVNBE
// ISA extensions  : CMOV
// Supported forms : (6 forms)
//
//    * CMOVNBE r16, r16
//    * CMOVNBE m16, r16
//    * CMOVNBE r32, r32
//    * CMOVNBE m32, r32
//    * CMOVNBE r64, r64
//    * CMOVNBE m64, r64
//
func (self *Program) CMOVNBE(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMOVNBE r16, r16
    if isReg16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x47)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNBE m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x47)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVNBE r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x47)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNBE m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x47)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVNBE r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x47)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNBE m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x47)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMOVNBE")
    }
    return p
}

// CMOVNC performs "Move if not carry (CF == 0)".
//
// Mnemonic        : CMOVNC
// ISA extensions  : CMOV
// Supported forms : (6 forms)
//
//    * CMOVNC r16, r16
//    * CMOVNC m16, r16
//    * CMOVNC r32, r32
//    * CMOVNC m32, r32
//    * CMOVNC r64, r64
//    * CMOVNC m64, r64
//
func (self *Program) CMOVNC(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMOVNC r16, r16
    if isReg16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x43)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNC m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x43)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVNC r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x43)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNC m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x43)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVNC r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x43)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNC m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x43)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMOVNC")
    }
    return p
}

// CMOVNE performs "Move if not equal (ZF == 0)".
//
// Mnemonic        : CMOVNE
// ISA extensions  : CMOV
// Supported forms : (6 forms)
//
//    * CMOVNE r16, r16
//    * CMOVNE m16, r16
//    * CMOVNE r32, r32
//    * CMOVNE m32, r32
//    * CMOVNE r64, r64
//    * CMOVNE m64, r64
//
func (self *Program) CMOVNE(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMOVNE r16, r16
    if isReg16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x45)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNE m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x45)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVNE r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x45)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNE m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x45)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVNE r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x45)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNE m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x45)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMOVNE")
    }
    return p
}

// CMOVNG performs "Move if not greater (ZF == 1 or SF != OF)".
//
// Mnemonic        : CMOVNG
// ISA extensions  : CMOV
// Supported forms : (6 forms)
//
//    * CMOVNG r16, r16
//    * CMOVNG m16, r16
//    * CMOVNG r32, r32
//    * CMOVNG m32, r32
//    * CMOVNG r64, r64
//    * CMOVNG m64, r64
//
func (self *Program) CMOVNG(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMOVNG r16, r16
    if isReg16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x4e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNG m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x4e)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVNG r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x4e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNG m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x4e)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVNG r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x4e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNG m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x4e)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMOVNG")
    }
    return p
}

// CMOVNGE performs "Move if not greater or equal (SF != OF)".
//
// Mnemonic        : CMOVNGE
// ISA extensions  : CMOV
// Supported forms : (6 forms)
//
//    * CMOVNGE r16, r16
//    * CMOVNGE m16, r16
//    * CMOVNGE r32, r32
//    * CMOVNGE m32, r32
//    * CMOVNGE r64, r64
//    * CMOVNGE m64, r64
//
func (self *Program) CMOVNGE(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMOVNGE r16, r16
    if isReg16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x4c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNGE m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x4c)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVNGE r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x4c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNGE m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x4c)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVNGE r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x4c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNGE m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x4c)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMOVNGE")
    }
    return p
}

// CMOVNL performs "Move if not less (SF == OF)".
//
// Mnemonic        : CMOVNL
// ISA extensions  : CMOV
// Supported forms : (6 forms)
//
//    * CMOVNL r16, r16
//    * CMOVNL m16, r16
//    * CMOVNL r32, r32
//    * CMOVNL m32, r32
//    * CMOVNL r64, r64
//    * CMOVNL m64, r64
//
func (self *Program) CMOVNL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMOVNL r16, r16
    if isReg16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x4d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNL m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x4d)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVNL r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x4d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNL m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x4d)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVNL r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x4d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNL m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x4d)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMOVNL")
    }
    return p
}

// CMOVNLE performs "Move if not less or equal (ZF == 0 and SF == OF)".
//
// Mnemonic        : CMOVNLE
// ISA extensions  : CMOV
// Supported forms : (6 forms)
//
//    * CMOVNLE r16, r16
//    * CMOVNLE m16, r16
//    * CMOVNLE r32, r32
//    * CMOVNLE m32, r32
//    * CMOVNLE r64, r64
//    * CMOVNLE m64, r64
//
func (self *Program) CMOVNLE(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMOVNLE r16, r16
    if isReg16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x4f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNLE m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x4f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVNLE r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x4f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNLE m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x4f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVNLE r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x4f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNLE m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x4f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMOVNLE")
    }
    return p
}

// CMOVNO performs "Move if not overflow (OF == 0)".
//
// Mnemonic        : CMOVNO
// ISA extensions  : CMOV
// Supported forms : (6 forms)
//
//    * CMOVNO r16, r16
//    * CMOVNO m16, r16
//    * CMOVNO r32, r32
//    * CMOVNO m32, r32
//    * CMOVNO r64, r64
//    * CMOVNO m64, r64
//
func (self *Program) CMOVNO(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMOVNO r16, r16
    if isReg16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x41)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNO m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x41)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVNO r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x41)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNO m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x41)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVNO r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x41)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNO m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x41)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMOVNO")
    }
    return p
}

// CMOVNP performs "Move if not parity (PF == 0)".
//
// Mnemonic        : CMOVNP
// ISA extensions  : CMOV
// Supported forms : (6 forms)
//
//    * CMOVNP r16, r16
//    * CMOVNP m16, r16
//    * CMOVNP r32, r32
//    * CMOVNP m32, r32
//    * CMOVNP r64, r64
//    * CMOVNP m64, r64
//
func (self *Program) CMOVNP(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMOVNP r16, r16
    if isReg16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x4b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNP m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x4b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVNP r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x4b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNP m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x4b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVNP r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x4b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNP m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x4b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMOVNP")
    }
    return p
}

// CMOVNS performs "Move if not sign (SF == 0)".
//
// Mnemonic        : CMOVNS
// ISA extensions  : CMOV
// Supported forms : (6 forms)
//
//    * CMOVNS r16, r16
//    * CMOVNS m16, r16
//    * CMOVNS r32, r32
//    * CMOVNS m32, r32
//    * CMOVNS r64, r64
//    * CMOVNS m64, r64
//
func (self *Program) CMOVNS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMOVNS r16, r16
    if isReg16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x49)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNS m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x49)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVNS r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x49)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNS m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x49)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVNS r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x49)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNS m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x49)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMOVNS")
    }
    return p
}

// CMOVNZ performs "Move if not zero (ZF == 0)".
//
// Mnemonic        : CMOVNZ
// ISA extensions  : CMOV
// Supported forms : (6 forms)
//
//    * CMOVNZ r16, r16
//    * CMOVNZ m16, r16
//    * CMOVNZ r32, r32
//    * CMOVNZ m32, r32
//    * CMOVNZ r64, r64
//    * CMOVNZ m64, r64
//
func (self *Program) CMOVNZ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMOVNZ r16, r16
    if isReg16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x45)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNZ m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x45)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVNZ r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x45)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNZ m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x45)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVNZ r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x45)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVNZ m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x45)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMOVNZ")
    }
    return p
}

// CMOVO performs "Move if overflow (OF == 1)".
//
// Mnemonic        : CMOVO
// ISA extensions  : CMOV
// Supported forms : (6 forms)
//
//    * CMOVO r16, r16
//    * CMOVO m16, r16
//    * CMOVO r32, r32
//    * CMOVO m32, r32
//    * CMOVO r64, r64
//    * CMOVO m64, r64
//
func (self *Program) CMOVO(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMOVO r16, r16
    if isReg16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x40)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVO m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x40)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVO r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x40)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVO m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x40)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVO r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x40)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVO m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x40)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMOVO")
    }
    return p
}

// CMOVP performs "Move if parity (PF == 1)".
//
// Mnemonic        : CMOVP
// ISA extensions  : CMOV
// Supported forms : (6 forms)
//
//    * CMOVP r16, r16
//    * CMOVP m16, r16
//    * CMOVP r32, r32
//    * CMOVP m32, r32
//    * CMOVP r64, r64
//    * CMOVP m64, r64
//
func (self *Program) CMOVP(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMOVP r16, r16
    if isReg16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x4a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVP m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x4a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVP r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x4a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVP m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x4a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVP r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x4a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVP m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x4a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMOVP")
    }
    return p
}

// CMOVPE performs "Move if parity even (PF == 1)".
//
// Mnemonic        : CMOVPE
// ISA extensions  : CMOV
// Supported forms : (6 forms)
//
//    * CMOVPE r16, r16
//    * CMOVPE m16, r16
//    * CMOVPE r32, r32
//    * CMOVPE m32, r32
//    * CMOVPE r64, r64
//    * CMOVPE m64, r64
//
func (self *Program) CMOVPE(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMOVPE r16, r16
    if isReg16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x4a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVPE m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x4a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVPE r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x4a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVPE m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x4a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVPE r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x4a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVPE m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x4a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMOVPE")
    }
    return p
}

// CMOVPO performs "Move if parity odd (PF == 0)".
//
// Mnemonic        : CMOVPO
// ISA extensions  : CMOV
// Supported forms : (6 forms)
//
//    * CMOVPO r16, r16
//    * CMOVPO m16, r16
//    * CMOVPO r32, r32
//    * CMOVPO m32, r32
//    * CMOVPO r64, r64
//    * CMOVPO m64, r64
//
func (self *Program) CMOVPO(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMOVPO r16, r16
    if isReg16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x4b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVPO m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x4b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVPO r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x4b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVPO m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x4b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVPO r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x4b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVPO m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x4b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMOVPO")
    }
    return p
}

// CMOVS performs "Move if sign (SF == 1)".
//
// Mnemonic        : CMOVS
// ISA extensions  : CMOV
// Supported forms : (6 forms)
//
//    * CMOVS r16, r16
//    * CMOVS m16, r16
//    * CMOVS r32, r32
//    * CMOVS m32, r32
//    * CMOVS r64, r64
//    * CMOVS m64, r64
//
func (self *Program) CMOVS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMOVS r16, r16
    if isReg16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x48)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVS m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x48)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVS r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x48)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVS m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x48)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVS r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x48)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVS m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x48)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMOVS")
    }
    return p
}

// CMOVZ performs "Move if zero (ZF == 1)".
//
// Mnemonic        : CMOVZ
// ISA extensions  : CMOV
// Supported forms : (6 forms)
//
//    * CMOVZ r16, r16
//    * CMOVZ m16, r16
//    * CMOVZ r32, r32
//    * CMOVZ m32, r32
//    * CMOVZ r64, r64
//    * CMOVZ m64, r64
//
func (self *Program) CMOVZ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMOVZ r16, r16
    if isReg16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x44)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVZ m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x44)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVZ r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x44)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVZ m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x44)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMOVZ r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x44)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMOVZ m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_CMOV)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x44)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMOVZ")
    }
    return p
}

// CMPB performs "Compare Two Operands".
//
// Mnemonic        : CMP
// Supported forms : (6 forms)
//
//    * CMPB imm8, al
//    * CMPB imm8, r8
//    * CMPB r8, r8
//    * CMPB m8, r8
//    * CMPB imm8, m8
//    * CMPB r8, m8
//
func (self *Program) CMPB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMPB imm8, al
    if isImm8(v0) && v1 == AL {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x3c)
            m.imm1(toImmAny(v[0]))
        })
    }
    // CMPB imm8, r8
    if isImm8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0x80)
            m.emit(0xf8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // CMPB r8, r8
    if isReg8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], isReg8REX(v[0]) || isReg8REX(v[1]))
            m.emit(0x38)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], isReg8REX(v[0]) || isReg8REX(v[1]))
            m.emit(0x3a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMPB m8, r8
    if isM8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), isReg8REX(v[1]))
            m.emit(0x3a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMPB imm8, m8
    if isImm8(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0x80)
            m.mrsd(7, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // CMPB r8, m8
    if isReg8(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), isReg8REX(v[0]))
            m.emit(0x38)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMPB")
    }
    return p
}

// CMPL performs "Compare Two Operands".
//
// Mnemonic        : CMP
// Supported forms : (8 forms)
//
//    * CMPL imm32, eax
//    * CMPL imm8, r32
//    * CMPL imm32, r32
//    * CMPL r32, r32
//    * CMPL m32, r32
//    * CMPL imm8, m32
//    * CMPL imm32, m32
//    * CMPL r32, m32
//
func (self *Program) CMPL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMPL imm32, eax
    if isImm32(v0) && v1 == EAX {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x3d)
            m.imm4(toImmAny(v[0]))
        })
    }
    // CMPL imm8, r32
    if isImm8Ext(v0, 4) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0x83)
            m.emit(0xf8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // CMPL imm32, r32
    if isImm32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0x81)
            m.emit(0xf8 | lcode(v[1]))
            m.imm4(toImmAny(v[0]))
        })
    }
    // CMPL r32, r32
    if isReg32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x39)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x3b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMPL m32, r32
    if isM32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x3b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMPL imm8, m32
    if isImm8Ext(v0, 4) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0x83)
            m.mrsd(7, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // CMPL imm32, m32
    if isImm32(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0x81)
            m.mrsd(7, addr(v[1]), 1)
            m.imm4(toImmAny(v[0]))
        })
    }
    // CMPL r32, m32
    if isReg32(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x39)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMPL")
    }
    return p
}

// CMPPD performs "Compare Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : CMPPD
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * CMPPD imm8, xmm, xmm
//    * CMPPD imm8, m128, xmm
//
func (self *Program) CMPPD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // CMPPD imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0xc2)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // CMPPD imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0xc2)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMPPD")
    }
    return p
}

// CMPPS performs "Compare Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : CMPPS
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * CMPPS imm8, xmm, xmm
//    * CMPPS imm8, m128, xmm
//
func (self *Program) CMPPS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // CMPPS imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0xc2)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // CMPPS imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0xc2)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMPPS")
    }
    return p
}

// CMPQ performs "Compare Two Operands".
//
// Mnemonic        : CMP
// Supported forms : (8 forms)
//
//    * CMPQ imm32, rax
//    * CMPQ imm8, r64
//    * CMPQ imm32, r64
//    * CMPQ r64, r64
//    * CMPQ m64, r64
//    * CMPQ imm8, m64
//    * CMPQ imm32, m64
//    * CMPQ r64, m64
//
func (self *Program) CMPQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMPQ imm32, rax
    if isImm32(v0) && v1 == RAX {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48)
            m.emit(0x3d)
            m.imm4(toImmAny(v[0]))
        })
    }
    // CMPQ imm8, r64
    if isImm8Ext(v0, 8) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0x83)
            m.emit(0xf8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // CMPQ imm32, r64
    if isImm32Ext(v0, 8) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0x81)
            m.emit(0xf8 | lcode(v[1]))
            m.imm4(toImmAny(v[0]))
        })
    }
    // CMPQ r64, r64
    if isReg64(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[0]) << 2 | hcode(v[1]))
            m.emit(0x39)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x3b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMPQ m64, r64
    if isM64(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x3b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMPQ imm8, m64
    if isImm8Ext(v0, 8) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0x83)
            m.mrsd(7, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // CMPQ imm32, m64
    if isImm32Ext(v0, 8) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0x81)
            m.mrsd(7, addr(v[1]), 1)
            m.imm4(toImmAny(v[0]))
        })
    }
    // CMPQ r64, m64
    if isReg64(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[0]), addr(v[1]))
            m.emit(0x39)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMPQ")
    }
    return p
}

// CMPSD performs "Compare Scalar Double-Precision Floating-Point Values".
//
// Mnemonic        : CMPSD
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * CMPSD imm8, xmm, xmm
//    * CMPSD imm8, m64, xmm
//
func (self *Program) CMPSD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // CMPSD imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0xc2)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // CMPSD imm8, m64, xmm
    if isImm8(v0) && isM64(v1) && isXMM(v2) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0xc2)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMPSD")
    }
    return p
}

// CMPSS performs "Compare Scalar Single-Precision Floating-Point Values".
//
// Mnemonic        : CMPSS
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * CMPSS imm8, xmm, xmm
//    * CMPSS imm8, m32, xmm
//
func (self *Program) CMPSS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // CMPSS imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0xc2)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // CMPSS imm8, m32, xmm
    if isImm8(v0) && isM32(v1) && isXMM(v2) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0xc2)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMPSS")
    }
    return p
}

// CMPW performs "Compare Two Operands".
//
// Mnemonic        : CMP
// Supported forms : (8 forms)
//
//    * CMPW imm16, ax
//    * CMPW imm8, r16
//    * CMPW imm16, r16
//    * CMPW r16, r16
//    * CMPW m16, r16
//    * CMPW imm8, m16
//    * CMPW imm16, m16
//    * CMPW r16, m16
//
func (self *Program) CMPW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMPW imm16, ax
    if isImm16(v0) && v1 == AX {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.emit(0x3d)
            m.imm2(toImmAny(v[0]))
        })
    }
    // CMPW imm8, r16
    if isImm8Ext(v0, 2) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0x83)
            m.emit(0xf8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // CMPW imm16, r16
    if isImm16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0x81)
            m.emit(0xf8 | lcode(v[1]))
            m.imm2(toImmAny(v[0]))
        })
    }
    // CMPW r16, r16
    if isReg16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x39)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x3b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CMPW m16, r16
    if isM16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x3b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CMPW imm8, m16
    if isImm8Ext(v0, 2) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0x83)
            m.mrsd(7, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // CMPW imm16, m16
    if isImm16(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0x81)
            m.mrsd(7, addr(v[1]), 1)
            m.imm2(toImmAny(v[0]))
        })
    }
    // CMPW r16, m16
    if isReg16(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x39)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMPW")
    }
    return p
}

// CMPXCHG16B performs "Compare and Exchange 16 Bytes".
//
// Mnemonic        : CMPXCHG16B
// Supported forms : (1 form)
//
//    * CMPXCHG16B m128
//
func (self *Program) CMPXCHG16B(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // CMPXCHG16B m128
    if isM128(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[0]))
            m.emit(0x0f)
            m.emit(0xc7)
            m.mrsd(1, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMPXCHG16B")
    }
    return p
}

// CMPXCHG8B performs "Compare and Exchange 8 Bytes".
//
// Mnemonic        : CMPXCHG8B
// Supported forms : (1 form)
//
//    * CMPXCHG8B m64
//
func (self *Program) CMPXCHG8B(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // CMPXCHG8B m64
    if isM64(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xc7)
            m.mrsd(1, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMPXCHG8B")
    }
    return p
}

// CMPXCHGB performs "Compare and Exchange".
//
// Mnemonic        : CMPXCHG
// Supported forms : (2 forms)
//
//    * CMPXCHGB r8, r8
//    * CMPXCHGB r8, m8
//
func (self *Program) CMPXCHGB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMPXCHGB r8, r8
    if isReg8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], isReg8REX(v[0]) || isReg8REX(v[1]))
            m.emit(0x0f)
            m.emit(0xb0)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // CMPXCHGB r8, m8
    if isReg8(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0xb0)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMPXCHGB")
    }
    return p
}

// CMPXCHGL performs "Compare and Exchange".
//
// Mnemonic        : CMPXCHG
// Supported forms : (2 forms)
//
//    * CMPXCHGL r32, r32
//    * CMPXCHGL r32, m32
//
func (self *Program) CMPXCHGL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMPXCHGL r32, r32
    if isReg32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x0f)
            m.emit(0xb1)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // CMPXCHGL r32, m32
    if isReg32(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0xb1)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMPXCHGL")
    }
    return p
}

// CMPXCHGQ performs "Compare and Exchange".
//
// Mnemonic        : CMPXCHG
// Supported forms : (2 forms)
//
//    * CMPXCHGQ r64, r64
//    * CMPXCHGQ r64, m64
//
func (self *Program) CMPXCHGQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMPXCHGQ r64, r64
    if isReg64(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[0]) << 2 | hcode(v[1]))
            m.emit(0x0f)
            m.emit(0xb1)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // CMPXCHGQ r64, m64
    if isReg64(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[0]), addr(v[1]))
            m.emit(0x0f)
            m.emit(0xb1)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMPXCHGQ")
    }
    return p
}

// CMPXCHGW performs "Compare and Exchange".
//
// Mnemonic        : CMPXCHG
// Supported forms : (2 forms)
//
//    * CMPXCHGW r16, r16
//    * CMPXCHGW r16, m16
//
func (self *Program) CMPXCHGW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CMPXCHGW r16, r16
    if isReg16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x0f)
            m.emit(0xb1)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // CMPXCHGW r16, m16
    if isReg16(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0xb1)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CMPXCHGW")
    }
    return p
}

// COMISD performs "Compare Scalar Ordered Double-Precision Floating-Point Values and Set EFLAGS".
//
// Mnemonic        : COMISD
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * COMISD xmm, xmm
//    * COMISD m64, xmm
//
func (self *Program) COMISD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // COMISD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x2f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // COMISD m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x2f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for COMISD")
    }
    return p
}

// COMISS performs "Compare Scalar Ordered Single-Precision Floating-Point Values and Set EFLAGS".
//
// Mnemonic        : COMISS
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * COMISS xmm, xmm
//    * COMISS m32, xmm
//
func (self *Program) COMISS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // COMISS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x2f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // COMISS m32, xmm
    if isM32(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x2f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for COMISS")
    }
    return p
}

// CPUID performs "CPU Identification".
//
// Mnemonic        : CPUID
// ISA extensions  : CPUID
// Supported forms : (1 form)
//
//    * CPUID
//
func (self *Program) CPUID() *Instruction {
    p := self.alloc(0, Operands{})
    // CPUID
    self.require(ISA_CPUID)
    p.add(0, func(m *_Encoding, v []interface{}) {
        m.emit(0x0f)
        m.emit(0xa2)
    })
    return p
}

// CQTO performs "Convert Quadword to Octaword".
//
// Mnemonic        : CQO
// Supported forms : (1 form)
//
//    * CQTO
//
func (self *Program) CQTO() *Instruction {
    p := self.alloc(0, Operands{})
    // CQTO
    p.add(0, func(m *_Encoding, v []interface{}) {
        m.emit(0x48)
        m.emit(0x99)
    })
    return p
}

// CRC32B performs "Accumulate CRC32 Value".
//
// Mnemonic        : CRC32
// ISA extensions  : SSE4.2
// Supported forms : (4 forms)
//
//    * CRC32B r8, r32
//    * CRC32B m8, r32
//    * CRC32B r8, r64
//    * CRC32B m8, r64
//
func (self *Program) CRC32B(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CRC32B r8, r32
    if isReg8(v0) && isReg32(v1) {
        self.require(ISA_SSE4_2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xf0)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CRC32B m8, r32
    if isM8(v0) && isReg32(v1) {
        self.require(ISA_SSE4_2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xf0)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CRC32B r8, r64
    if isReg8(v0) && isReg64(v1) {
        self.require(ISA_SSE4_2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xf0)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CRC32B m8, r64
    if isM8(v0) && isReg64(v1) {
        self.require(ISA_SSE4_2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xf0)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CRC32B")
    }
    return p
}

// CRC32L performs "Accumulate CRC32 Value".
//
// Mnemonic        : CRC32
// ISA extensions  : SSE4.2
// Supported forms : (2 forms)
//
//    * CRC32L r32, r32
//    * CRC32L m32, r32
//
func (self *Program) CRC32L(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CRC32L r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_SSE4_2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xf1)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CRC32L m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_SSE4_2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xf1)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CRC32L")
    }
    return p
}

// CRC32Q performs "Accumulate CRC32 Value".
//
// Mnemonic        : CRC32
// ISA extensions  : SSE4.2
// Supported forms : (2 forms)
//
//    * CRC32Q r64, r64
//    * CRC32Q m64, r64
//
func (self *Program) CRC32Q(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CRC32Q r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_SSE4_2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xf1)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CRC32Q m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_SSE4_2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xf1)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CRC32Q")
    }
    return p
}

// CRC32W performs "Accumulate CRC32 Value".
//
// Mnemonic        : CRC32
// ISA extensions  : SSE4.2
// Supported forms : (2 forms)
//
//    * CRC32W r16, r32
//    * CRC32W m16, r32
//
func (self *Program) CRC32W(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CRC32W r16, r32
    if isReg16(v0) && isReg32(v1) {
        self.require(ISA_SSE4_2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.emit(0xf2)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xf1)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CRC32W m16, r32
    if isM16(v0) && isReg32(v1) {
        self.require(ISA_SSE4_2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.emit(0xf2)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xf1)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CRC32W")
    }
    return p
}

// CVTDQ2PD performs "Convert Packed Dword Integers to Packed Double-Precision FP Values".
//
// Mnemonic        : CVTDQ2PD
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * CVTDQ2PD xmm, xmm
//    * CVTDQ2PD m64, xmm
//
func (self *Program) CVTDQ2PD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CVTDQ2PD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xe6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CVTDQ2PD m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xe6)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CVTDQ2PD")
    }
    return p
}

// CVTDQ2PS performs "Convert Packed Dword Integers to Packed Single-Precision FP Values".
//
// Mnemonic        : CVTDQ2PS
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * CVTDQ2PS xmm, xmm
//    * CVTDQ2PS m128, xmm
//
func (self *Program) CVTDQ2PS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CVTDQ2PS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x5b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CVTDQ2PS m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x5b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CVTDQ2PS")
    }
    return p
}

// CVTPD2DQ performs "Convert Packed Double-Precision FP Values to Packed Dword Integers".
//
// Mnemonic        : CVTPD2DQ
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * CVTPD2DQ xmm, xmm
//    * CVTPD2DQ m128, xmm
//
func (self *Program) CVTPD2DQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CVTPD2DQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xe6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CVTPD2DQ m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xe6)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CVTPD2DQ")
    }
    return p
}

// CVTPD2PI performs "Convert Packed Double-Precision FP Values to Packed Dword Integers".
//
// Mnemonic        : CVTPD2PI
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * CVTPD2PI xmm, mm
//    * CVTPD2PI m128, mm
//
func (self *Program) CVTPD2PI(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CVTPD2PI xmm, mm
    if isXMM(v0) && isMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x2d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CVTPD2PI m128, mm
    if isM128(v0) && isMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x2d)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CVTPD2PI")
    }
    return p
}

// CVTPD2PS performs "Convert Packed Double-Precision FP Values to Packed Single-Precision FP Values".
//
// Mnemonic        : CVTPD2PS
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * CVTPD2PS xmm, xmm
//    * CVTPD2PS m128, xmm
//
func (self *Program) CVTPD2PS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CVTPD2PS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x5a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CVTPD2PS m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x5a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CVTPD2PS")
    }
    return p
}

// CVTPI2PD performs "Convert Packed Dword Integers to Packed Double-Precision FP Values".
//
// Mnemonic        : CVTPI2PD
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * CVTPI2PD mm, xmm
//    * CVTPI2PD m64, xmm
//
func (self *Program) CVTPI2PD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CVTPI2PD mm, xmm
    if isMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x2a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CVTPI2PD m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x2a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CVTPI2PD")
    }
    return p
}

// CVTPI2PS performs "Convert Packed Dword Integers to Packed Single-Precision FP Values".
//
// Mnemonic        : CVTPI2PS
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * CVTPI2PS mm, xmm
//    * CVTPI2PS m64, xmm
//
func (self *Program) CVTPI2PS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CVTPI2PS mm, xmm
    if isMM(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x2a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CVTPI2PS m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x2a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CVTPI2PS")
    }
    return p
}

// CVTPS2DQ performs "Convert Packed Single-Precision FP Values to Packed Dword Integers".
//
// Mnemonic        : CVTPS2DQ
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * CVTPS2DQ xmm, xmm
//    * CVTPS2DQ m128, xmm
//
func (self *Program) CVTPS2DQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CVTPS2DQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x5b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CVTPS2DQ m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x5b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CVTPS2DQ")
    }
    return p
}

// CVTPS2PD performs "Convert Packed Single-Precision FP Values to Packed Double-Precision FP Values".
//
// Mnemonic        : CVTPS2PD
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * CVTPS2PD xmm, xmm
//    * CVTPS2PD m64, xmm
//
func (self *Program) CVTPS2PD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CVTPS2PD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x5a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CVTPS2PD m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x5a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CVTPS2PD")
    }
    return p
}

// CVTPS2PI performs "Convert Packed Single-Precision FP Values to Packed Dword Integers".
//
// Mnemonic        : CVTPS2PI
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * CVTPS2PI xmm, mm
//    * CVTPS2PI m64, mm
//
func (self *Program) CVTPS2PI(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CVTPS2PI xmm, mm
    if isXMM(v0) && isMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x2d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CVTPS2PI m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x2d)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CVTPS2PI")
    }
    return p
}

// CVTSD2SI performs "Convert Scalar Double-Precision FP Value to Integer".
//
// Mnemonic        : CVTSD2SI
// ISA extensions  : SSE2
// Supported forms : (4 forms)
//
//    * CVTSD2SI xmm, r32
//    * CVTSD2SI m64, r32
//    * CVTSD2SI xmm, r64
//    * CVTSD2SI m64, r64
//
func (self *Program) CVTSD2SI(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CVTSD2SI xmm, r32
    if isXMM(v0) && isReg32(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x2d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CVTSD2SI m64, r32
    if isM64(v0) && isReg32(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x2d)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CVTSD2SI xmm, r64
    if isXMM(v0) && isReg64(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x2d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CVTSD2SI m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x2d)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CVTSD2SI")
    }
    return p
}

// CVTSD2SS performs "Convert Scalar Double-Precision FP Value to Scalar Single-Precision FP Value".
//
// Mnemonic        : CVTSD2SS
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * CVTSD2SS xmm, xmm
//    * CVTSD2SS m64, xmm
//
func (self *Program) CVTSD2SS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CVTSD2SS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x5a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CVTSD2SS m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x5a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CVTSD2SS")
    }
    return p
}

// CVTSI2SD performs "Convert Dword Integer to Scalar Double-Precision FP Value".
//
// Mnemonic        : CVTSI2SD
// ISA extensions  : SSE2
// Supported forms : (4 forms)
//
//    * CVTSI2SD r32, xmm
//    * CVTSI2SD r64, xmm
//    * CVTSI2SD m32, xmm
//    * CVTSI2SD m64, xmm
//
func (self *Program) CVTSI2SD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CVTSI2SD r32, xmm
    if isReg32(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x2a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CVTSI2SD r64, xmm
    if isReg64(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x2a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CVTSI2SD m32, xmm
    if isM32(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x2a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CVTSI2SD m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x2a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CVTSI2SD")
    }
    return p
}

// CVTSI2SS performs "Convert Dword Integer to Scalar Single-Precision FP Value".
//
// Mnemonic        : CVTSI2SS
// ISA extensions  : SSE
// Supported forms : (4 forms)
//
//    * CVTSI2SS r32, xmm
//    * CVTSI2SS r64, xmm
//    * CVTSI2SS m32, xmm
//    * CVTSI2SS m64, xmm
//
func (self *Program) CVTSI2SS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CVTSI2SS r32, xmm
    if isReg32(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x2a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CVTSI2SS r64, xmm
    if isReg64(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x2a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CVTSI2SS m32, xmm
    if isM32(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x2a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CVTSI2SS m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x2a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CVTSI2SS")
    }
    return p
}

// CVTSS2SD performs "Convert Scalar Single-Precision FP Value to Scalar Double-Precision FP Value".
//
// Mnemonic        : CVTSS2SD
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * CVTSS2SD xmm, xmm
//    * CVTSS2SD m32, xmm
//
func (self *Program) CVTSS2SD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CVTSS2SD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x5a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CVTSS2SD m32, xmm
    if isM32(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x5a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CVTSS2SD")
    }
    return p
}

// CVTSS2SI performs "Convert Scalar Single-Precision FP Value to Dword Integer".
//
// Mnemonic        : CVTSS2SI
// ISA extensions  : SSE
// Supported forms : (4 forms)
//
//    * CVTSS2SI xmm, r32
//    * CVTSS2SI m32, r32
//    * CVTSS2SI xmm, r64
//    * CVTSS2SI m32, r64
//
func (self *Program) CVTSS2SI(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CVTSS2SI xmm, r32
    if isXMM(v0) && isReg32(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x2d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CVTSS2SI m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x2d)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CVTSS2SI xmm, r64
    if isXMM(v0) && isReg64(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x2d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CVTSS2SI m32, r64
    if isM32(v0) && isReg64(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x2d)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CVTSS2SI")
    }
    return p
}

// CVTTPD2DQ performs "Convert with Truncation Packed Double-Precision FP Values to Packed Dword Integers".
//
// Mnemonic        : CVTTPD2DQ
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * CVTTPD2DQ xmm, xmm
//    * CVTTPD2DQ m128, xmm
//
func (self *Program) CVTTPD2DQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CVTTPD2DQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xe6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CVTTPD2DQ m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xe6)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CVTTPD2DQ")
    }
    return p
}

// CVTTPD2PI performs "Convert with Truncation Packed Double-Precision FP Values to Packed Dword Integers".
//
// Mnemonic        : CVTTPD2PI
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * CVTTPD2PI xmm, mm
//    * CVTTPD2PI m128, mm
//
func (self *Program) CVTTPD2PI(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CVTTPD2PI xmm, mm
    if isXMM(v0) && isMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x2c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CVTTPD2PI m128, mm
    if isM128(v0) && isMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x2c)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CVTTPD2PI")
    }
    return p
}

// CVTTPS2DQ performs "Convert with Truncation Packed Single-Precision FP Values to Packed Dword Integers".
//
// Mnemonic        : CVTTPS2DQ
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * CVTTPS2DQ xmm, xmm
//    * CVTTPS2DQ m128, xmm
//
func (self *Program) CVTTPS2DQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CVTTPS2DQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x5b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CVTTPS2DQ m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x5b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CVTTPS2DQ")
    }
    return p
}

// CVTTPS2PI performs "Convert with Truncation Packed Single-Precision FP Values to Packed Dword Integers".
//
// Mnemonic        : CVTTPS2PI
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * CVTTPS2PI xmm, mm
//    * CVTTPS2PI m64, mm
//
func (self *Program) CVTTPS2PI(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CVTTPS2PI xmm, mm
    if isXMM(v0) && isMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x2c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CVTTPS2PI m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x2c)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CVTTPS2PI")
    }
    return p
}

// CVTTSD2SI performs "Convert with Truncation Scalar Double-Precision FP Value to Signed Integer".
//
// Mnemonic        : CVTTSD2SI
// ISA extensions  : SSE2
// Supported forms : (4 forms)
//
//    * CVTTSD2SI xmm, r32
//    * CVTTSD2SI m64, r32
//    * CVTTSD2SI xmm, r64
//    * CVTTSD2SI m64, r64
//
func (self *Program) CVTTSD2SI(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CVTTSD2SI xmm, r32
    if isXMM(v0) && isReg32(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x2c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CVTTSD2SI m64, r32
    if isM64(v0) && isReg32(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x2c)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CVTTSD2SI xmm, r64
    if isXMM(v0) && isReg64(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x2c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CVTTSD2SI m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x2c)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CVTTSD2SI")
    }
    return p
}

// CVTTSS2SI performs "Convert with Truncation Scalar Single-Precision FP Value to Dword Integer".
//
// Mnemonic        : CVTTSS2SI
// ISA extensions  : SSE
// Supported forms : (4 forms)
//
//    * CVTTSS2SI xmm, r32
//    * CVTTSS2SI m32, r32
//    * CVTTSS2SI xmm, r64
//    * CVTTSS2SI m32, r64
//
func (self *Program) CVTTSS2SI(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // CVTTSS2SI xmm, r32
    if isXMM(v0) && isReg32(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x2c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CVTTSS2SI m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x2c)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // CVTTSS2SI xmm, r64
    if isXMM(v0) && isReg64(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x2c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // CVTTSS2SI m32, r64
    if isM32(v0) && isReg64(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x2c)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for CVTTSS2SI")
    }
    return p
}

// CWTD performs "Convert Word to Doubleword".
//
// Mnemonic        : CWD
// Supported forms : (1 form)
//
//    * CWTD
//
func (self *Program) CWTD() *Instruction {
    p := self.alloc(0, Operands{})
    // CWTD
    p.add(0, func(m *_Encoding, v []interface{}) {
        m.emit(0x66)
        m.emit(0x99)
    })
    return p
}

// CWTL performs "Convert Word to Doubleword".
//
// Mnemonic        : CWDE
// Supported forms : (1 form)
//
//    * CWTL
//
func (self *Program) CWTL() *Instruction {
    p := self.alloc(0, Operands{})
    // CWTL
    p.add(0, func(m *_Encoding, v []interface{}) {
        m.emit(0x98)
    })
    return p
}

// DECB performs "Decrement by 1".
//
// Mnemonic        : DEC
// Supported forms : (2 forms)
//
//    * DECB r8
//    * DECB m8
//
func (self *Program) DECB(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // DECB r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0xfe)
            m.emit(0xc8 | lcode(v[0]))
        })
    }
    // DECB m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0xfe)
            m.mrsd(1, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for DECB")
    }
    return p
}

// DECL performs "Decrement by 1".
//
// Mnemonic        : DEC
// Supported forms : (2 forms)
//
//    * DECL r32
//    * DECL m32
//
func (self *Program) DECL(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // DECL r32
    if isReg32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], false)
            m.emit(0xff)
            m.emit(0xc8 | lcode(v[0]))
        })
    }
    // DECL m32
    if isM32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0xff)
            m.mrsd(1, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for DECL")
    }
    return p
}

// DECQ performs "Decrement by 1".
//
// Mnemonic        : DEC
// Supported forms : (2 forms)
//
//    * DECQ r64
//    * DECQ m64
//
func (self *Program) DECQ(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // DECQ r64
    if isReg64(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[0]))
            m.emit(0xff)
            m.emit(0xc8 | lcode(v[0]))
        })
    }
    // DECQ m64
    if isM64(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[0]))
            m.emit(0xff)
            m.mrsd(1, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for DECQ")
    }
    return p
}

// DECW performs "Decrement by 1".
//
// Mnemonic        : DEC
// Supported forms : (2 forms)
//
//    * DECW r16
//    * DECW m16
//
func (self *Program) DECW(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // DECW r16
    if isReg16(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[0], false)
            m.emit(0xff)
            m.emit(0xc8 | lcode(v[0]))
        })
    }
    // DECW m16
    if isM16(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[0]), false)
            m.emit(0xff)
            m.mrsd(1, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for DECW")
    }
    return p
}

// DIVB performs "Unsigned Divide".
//
// Mnemonic        : DIV
// Supported forms : (2 forms)
//
//    * DIVB r8
//    * DIVB m8
//
func (self *Program) DIVB(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // DIVB r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0xf6)
            m.emit(0xf0 | lcode(v[0]))
        })
    }
    // DIVB m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0xf6)
            m.mrsd(6, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for DIVB")
    }
    return p
}

// DIVL performs "Unsigned Divide".
//
// Mnemonic        : DIV
// Supported forms : (2 forms)
//
//    * DIVL r32
//    * DIVL m32
//
func (self *Program) DIVL(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // DIVL r32
    if isReg32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], false)
            m.emit(0xf7)
            m.emit(0xf0 | lcode(v[0]))
        })
    }
    // DIVL m32
    if isM32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0xf7)
            m.mrsd(6, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for DIVL")
    }
    return p
}

// DIVPD performs "Divide Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : DIVPD
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * DIVPD xmm, xmm
//    * DIVPD m128, xmm
//
func (self *Program) DIVPD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // DIVPD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x5e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // DIVPD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x5e)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for DIVPD")
    }
    return p
}

// DIVPS performs "Divide Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : DIVPS
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * DIVPS xmm, xmm
//    * DIVPS m128, xmm
//
func (self *Program) DIVPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // DIVPS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x5e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // DIVPS m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x5e)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for DIVPS")
    }
    return p
}

// DIVQ performs "Unsigned Divide".
//
// Mnemonic        : DIV
// Supported forms : (2 forms)
//
//    * DIVQ r64
//    * DIVQ m64
//
func (self *Program) DIVQ(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // DIVQ r64
    if isReg64(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[0]))
            m.emit(0xf7)
            m.emit(0xf0 | lcode(v[0]))
        })
    }
    // DIVQ m64
    if isM64(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[0]))
            m.emit(0xf7)
            m.mrsd(6, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for DIVQ")
    }
    return p
}

// DIVSD performs "Divide Scalar Double-Precision Floating-Point Values".
//
// Mnemonic        : DIVSD
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * DIVSD xmm, xmm
//    * DIVSD m64, xmm
//
func (self *Program) DIVSD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // DIVSD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x5e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // DIVSD m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x5e)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for DIVSD")
    }
    return p
}

// DIVSS performs "Divide Scalar Single-Precision Floating-Point Values".
//
// Mnemonic        : DIVSS
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * DIVSS xmm, xmm
//    * DIVSS m32, xmm
//
func (self *Program) DIVSS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // DIVSS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x5e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // DIVSS m32, xmm
    if isM32(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x5e)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for DIVSS")
    }
    return p
}

// DIVW performs "Unsigned Divide".
//
// Mnemonic        : DIV
// Supported forms : (2 forms)
//
//    * DIVW r16
//    * DIVW m16
//
func (self *Program) DIVW(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // DIVW r16
    if isReg16(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[0], false)
            m.emit(0xf7)
            m.emit(0xf0 | lcode(v[0]))
        })
    }
    // DIVW m16
    if isM16(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[0]), false)
            m.emit(0xf7)
            m.mrsd(6, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for DIVW")
    }
    return p
}

// DPPD performs "Dot Product of Packed Double Precision Floating-Point Values".
//
// Mnemonic        : DPPD
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * DPPD imm8, xmm, xmm
//    * DPPD imm8, m128, xmm
//
func (self *Program) DPPD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // DPPD imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x41)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // DPPD imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x41)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for DPPD")
    }
    return p
}

// DPPS performs "Dot Product of Packed Single Precision Floating-Point Values".
//
// Mnemonic        : DPPS
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * DPPS imm8, xmm, xmm
//    * DPPS imm8, m128, xmm
//
func (self *Program) DPPS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // DPPS imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x40)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // DPPS imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x40)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for DPPS")
    }
    return p
}

// EMMS performs "Exit MMX State".
//
// Mnemonic        : EMMS
// ISA extensions  : MMX
// Supported forms : (1 form)
//
//    * EMMS
//
func (self *Program) EMMS() *Instruction {
    p := self.alloc(0, Operands{})
    // EMMS
    self.require(ISA_MMX)
    p.add(0, func(m *_Encoding, v []interface{}) {
        m.emit(0x0f)
        m.emit(0x77)
    })
    return p
}

// EXTRACTPS performs "Extract Packed Single Precision Floating-Point Value".
//
// Mnemonic        : EXTRACTPS
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * EXTRACTPS imm8, xmm, r32
//    * EXTRACTPS imm8, xmm, m32
//
func (self *Program) EXTRACTPS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // EXTRACTPS imm8, xmm, r32
    if isImm8(v0) && isXMM(v1) && isReg32(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[2], false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x17)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // EXTRACTPS imm8, xmm, m32
    if isImm8(v0) && isXMM(v1) && isM32(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[2]), false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x17)
            m.mrsd(lcode(v[1]), addr(v[2]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for EXTRACTPS")
    }
    return p
}

// EXTRQ performs "Extract Field".
//
// Mnemonic        : EXTRQ
// ISA extensions  : SSE4A
// Supported forms : (2 forms)
//
//    * EXTRQ xmm, xmm
//    * EXTRQ imm8, imm8, xmm
//
func (self *Program) EXTRQ(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction EXTRQ takes 2 or 3 operands")
    }
    // EXTRQ xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE4A)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // EXTRQ imm8, imm8, xmm
    if len(vv) == 1 && isImm8(v0) && isImm8(v1) && isXMM(vv[0]) {
        self.require(ISA_SSE4A)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[2], false)
            m.emit(0x0f)
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[2]))
            m.imm1(toImmAny(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for EXTRQ")
    }
    return p
}

// FEMMS performs "Fast Exit Multimedia State".
//
// Mnemonic        : FEMMS
// ISA extensions  : FEMMS
// Supported forms : (1 form)
//
//    * FEMMS
//
func (self *Program) FEMMS() *Instruction {
    p := self.alloc(0, Operands{})
    // FEMMS
    self.require(ISA_FEMMS)
    p.add(0, func(m *_Encoding, v []interface{}) {
        m.emit(0x0f)
        m.emit(0x0e)
    })
    return p
}

// HADDPD performs "Packed Double-FP Horizontal Add".
//
// Mnemonic        : HADDPD
// ISA extensions  : SSE3
// Supported forms : (2 forms)
//
//    * HADDPD xmm, xmm
//    * HADDPD m128, xmm
//
func (self *Program) HADDPD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // HADDPD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x7c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // HADDPD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x7c)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for HADDPD")
    }
    return p
}

// HADDPS performs "Packed Single-FP Horizontal Add".
//
// Mnemonic        : HADDPS
// ISA extensions  : SSE3
// Supported forms : (2 forms)
//
//    * HADDPS xmm, xmm
//    * HADDPS m128, xmm
//
func (self *Program) HADDPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // HADDPS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x7c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // HADDPS m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x7c)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for HADDPS")
    }
    return p
}

// HSUBPD performs "Packed Double-FP Horizontal Subtract".
//
// Mnemonic        : HSUBPD
// ISA extensions  : SSE3
// Supported forms : (2 forms)
//
//    * HSUBPD xmm, xmm
//    * HSUBPD m128, xmm
//
func (self *Program) HSUBPD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // HSUBPD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x7d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // HSUBPD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x7d)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for HSUBPD")
    }
    return p
}

// HSUBPS performs "Packed Single-FP Horizontal Subtract".
//
// Mnemonic        : HSUBPS
// ISA extensions  : SSE3
// Supported forms : (2 forms)
//
//    * HSUBPS xmm, xmm
//    * HSUBPS m128, xmm
//
func (self *Program) HSUBPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // HSUBPS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x7d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // HSUBPS m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x7d)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for HSUBPS")
    }
    return p
}

// IDIVB performs "Signed Divide".
//
// Mnemonic        : IDIV
// Supported forms : (2 forms)
//
//    * IDIVB r8
//    * IDIVB m8
//
func (self *Program) IDIVB(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // IDIVB r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0xf6)
            m.emit(0xf8 | lcode(v[0]))
        })
    }
    // IDIVB m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0xf6)
            m.mrsd(7, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for IDIVB")
    }
    return p
}

// IDIVL performs "Signed Divide".
//
// Mnemonic        : IDIV
// Supported forms : (2 forms)
//
//    * IDIVL r32
//    * IDIVL m32
//
func (self *Program) IDIVL(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // IDIVL r32
    if isReg32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], false)
            m.emit(0xf7)
            m.emit(0xf8 | lcode(v[0]))
        })
    }
    // IDIVL m32
    if isM32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0xf7)
            m.mrsd(7, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for IDIVL")
    }
    return p
}

// IDIVQ performs "Signed Divide".
//
// Mnemonic        : IDIV
// Supported forms : (2 forms)
//
//    * IDIVQ r64
//    * IDIVQ m64
//
func (self *Program) IDIVQ(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // IDIVQ r64
    if isReg64(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[0]))
            m.emit(0xf7)
            m.emit(0xf8 | lcode(v[0]))
        })
    }
    // IDIVQ m64
    if isM64(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[0]))
            m.emit(0xf7)
            m.mrsd(7, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for IDIVQ")
    }
    return p
}

// IDIVW performs "Signed Divide".
//
// Mnemonic        : IDIV
// Supported forms : (2 forms)
//
//    * IDIVW r16
//    * IDIVW m16
//
func (self *Program) IDIVW(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // IDIVW r16
    if isReg16(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[0], false)
            m.emit(0xf7)
            m.emit(0xf8 | lcode(v[0]))
        })
    }
    // IDIVW m16
    if isM16(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[0]), false)
            m.emit(0xf7)
            m.mrsd(7, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for IDIVW")
    }
    return p
}

// IMULB performs "Signed Multiply".
//
// Mnemonic        : IMUL
// Supported forms : (2 forms)
//
//    * IMULB r8
//    * IMULB m8
//
func (self *Program) IMULB(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // IMULB r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0xf6)
            m.emit(0xe8 | lcode(v[0]))
        })
    }
    // IMULB m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0xf6)
            m.mrsd(5, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for IMULB")
    }
    return p
}

// IMULL performs "Signed Multiply".
//
// Mnemonic        : IMUL
// Supported forms : (8 forms)
//
//    * IMULL r32
//    * IMULL m32
//    * IMULL r32, r32
//    * IMULL m32, r32
//    * IMULL imm8, r32, r32
//    * IMULL imm32, r32, r32
//    * IMULL imm8, m32, r32
//    * IMULL imm32, m32, r32
//
func (self *Program) IMULL(v0 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(1, Operands{v0})
        case 1  : p = self.alloc(2, Operands{v0, vv[0]})
        case 2  : p = self.alloc(3, Operands{v0, vv[0], vv[1]})
        default : panic("instruction IMULL takes 1 or 2 or 3 operands")
    }
    // IMULL r32
    if len(vv) == 0 && isReg32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], false)
            m.emit(0xf7)
            m.emit(0xe8 | lcode(v[0]))
        })
    }
    // IMULL m32
    if len(vv) == 0 && isM32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0xf7)
            m.mrsd(5, addr(v[0]), 1)
        })
    }
    // IMULL r32, r32
    if len(vv) == 1 && isReg32(v0) && isReg32(vv[0]) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xaf)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // IMULL m32, r32
    if len(vv) == 1 && isM32(v0) && isReg32(vv[0]) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xaf)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // IMULL imm8, r32, r32
    if len(vv) == 2 && isImm8(v0) && isReg32(vv[0]) && isReg32(vv[1]) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x6b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // IMULL imm32, r32, r32
    if len(vv) == 2 && isImm32(v0) && isReg32(vv[0]) && isReg32(vv[1]) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x69)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm4(toImmAny(v[0]))
        })
    }
    // IMULL imm8, m32, r32
    if len(vv) == 2 && isImm8(v0) && isM32(vv[0]) && isReg32(vv[1]) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x6b)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // IMULL imm32, m32, r32
    if len(vv) == 2 && isImm32(v0) && isM32(vv[0]) && isReg32(vv[1]) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x69)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm4(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for IMULL")
    }
    return p
}

// IMULQ performs "Signed Multiply".
//
// Mnemonic        : IMUL
// Supported forms : (8 forms)
//
//    * IMULQ r64
//    * IMULQ m64
//    * IMULQ r64, r64
//    * IMULQ m64, r64
//    * IMULQ imm8, r64, r64
//    * IMULQ imm32, r64, r64
//    * IMULQ imm8, m64, r64
//    * IMULQ imm32, m64, r64
//
func (self *Program) IMULQ(v0 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(1, Operands{v0})
        case 1  : p = self.alloc(2, Operands{v0, vv[0]})
        case 2  : p = self.alloc(3, Operands{v0, vv[0], vv[1]})
        default : panic("instruction IMULQ takes 1 or 2 or 3 operands")
    }
    // IMULQ r64
    if len(vv) == 0 && isReg64(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[0]))
            m.emit(0xf7)
            m.emit(0xe8 | lcode(v[0]))
        })
    }
    // IMULQ m64
    if len(vv) == 0 && isM64(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[0]))
            m.emit(0xf7)
            m.mrsd(5, addr(v[0]), 1)
        })
    }
    // IMULQ r64, r64
    if len(vv) == 1 && isReg64(v0) && isReg64(vv[0]) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0xaf)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // IMULQ m64, r64
    if len(vv) == 1 && isM64(v0) && isReg64(vv[0]) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0xaf)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // IMULQ imm8, r64, r64
    if len(vv) == 2 && isImm8(v0) && isReg64(vv[0]) && isReg64(vv[1]) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[2]) << 2 | hcode(v[1]))
            m.emit(0x6b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // IMULQ imm32, r64, r64
    if len(vv) == 2 && isImm32(v0) && isReg64(vv[0]) && isReg64(vv[1]) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[2]) << 2 | hcode(v[1]))
            m.emit(0x69)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm4(toImmAny(v[0]))
        })
    }
    // IMULQ imm8, m64, r64
    if len(vv) == 2 && isImm8(v0) && isM64(vv[0]) && isReg64(vv[1]) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[2]), addr(v[1]))
            m.emit(0x6b)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // IMULQ imm32, m64, r64
    if len(vv) == 2 && isImm32(v0) && isM64(vv[0]) && isReg64(vv[1]) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[2]), addr(v[1]))
            m.emit(0x69)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm4(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for IMULQ")
    }
    return p
}

// IMULW performs "Signed Multiply".
//
// Mnemonic        : IMUL
// Supported forms : (8 forms)
//
//    * IMULW r16
//    * IMULW m16
//    * IMULW r16, r16
//    * IMULW m16, r16
//    * IMULW imm8, r16, r16
//    * IMULW imm16, r16, r16
//    * IMULW imm8, m16, r16
//    * IMULW imm16, m16, r16
//
func (self *Program) IMULW(v0 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(1, Operands{v0})
        case 1  : p = self.alloc(2, Operands{v0, vv[0]})
        case 2  : p = self.alloc(3, Operands{v0, vv[0], vv[1]})
        default : panic("instruction IMULW takes 1 or 2 or 3 operands")
    }
    // IMULW r16
    if len(vv) == 0 && isReg16(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[0], false)
            m.emit(0xf7)
            m.emit(0xe8 | lcode(v[0]))
        })
    }
    // IMULW m16
    if len(vv) == 0 && isM16(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[0]), false)
            m.emit(0xf7)
            m.mrsd(5, addr(v[0]), 1)
        })
    }
    // IMULW r16, r16
    if len(vv) == 1 && isReg16(v0) && isReg16(vv[0]) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xaf)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // IMULW m16, r16
    if len(vv) == 1 && isM16(v0) && isReg16(vv[0]) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xaf)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // IMULW imm8, r16, r16
    if len(vv) == 2 && isImm8(v0) && isReg16(vv[0]) && isReg16(vv[1]) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x6b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // IMULW imm16, r16, r16
    if len(vv) == 2 && isImm16(v0) && isReg16(vv[0]) && isReg16(vv[1]) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x69)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm2(toImmAny(v[0]))
        })
    }
    // IMULW imm8, m16, r16
    if len(vv) == 2 && isImm8(v0) && isM16(vv[0]) && isReg16(vv[1]) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x6b)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // IMULW imm16, m16, r16
    if len(vv) == 2 && isImm16(v0) && isM16(vv[0]) && isReg16(vv[1]) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x69)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm2(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for IMULW")
    }
    return p
}

// INCB performs "Increment by 1".
//
// Mnemonic        : INC
// Supported forms : (2 forms)
//
//    * INCB r8
//    * INCB m8
//
func (self *Program) INCB(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // INCB r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0xfe)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // INCB m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0xfe)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for INCB")
    }
    return p
}

// INCL performs "Increment by 1".
//
// Mnemonic        : INC
// Supported forms : (2 forms)
//
//    * INCL r32
//    * INCL m32
//
func (self *Program) INCL(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // INCL r32
    if isReg32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], false)
            m.emit(0xff)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // INCL m32
    if isM32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0xff)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for INCL")
    }
    return p
}

// INCQ performs "Increment by 1".
//
// Mnemonic        : INC
// Supported forms : (2 forms)
//
//    * INCQ r64
//    * INCQ m64
//
func (self *Program) INCQ(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // INCQ r64
    if isReg64(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[0]))
            m.emit(0xff)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // INCQ m64
    if isM64(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[0]))
            m.emit(0xff)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for INCQ")
    }
    return p
}

// INCW performs "Increment by 1".
//
// Mnemonic        : INC
// Supported forms : (2 forms)
//
//    * INCW r16
//    * INCW m16
//
func (self *Program) INCW(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // INCW r16
    if isReg16(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[0], false)
            m.emit(0xff)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // INCW m16
    if isM16(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[0]), false)
            m.emit(0xff)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for INCW")
    }
    return p
}

// INSERTPS performs "Insert Packed Single Precision Floating-Point Value".
//
// Mnemonic        : INSERTPS
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * INSERTPS imm8, xmm, xmm
//    * INSERTPS imm8, m32, xmm
//
func (self *Program) INSERTPS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // INSERTPS imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x21)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // INSERTPS imm8, m32, xmm
    if isImm8(v0) && isM32(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x21)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for INSERTPS")
    }
    return p
}

// INSERTQ performs "Insert Field".
//
// Mnemonic        : INSERTQ
// ISA extensions  : SSE4A
// Supported forms : (2 forms)
//
//    * INSERTQ xmm, xmm
//    * INSERTQ imm8, imm8, xmm, xmm
//
func (self *Program) INSERTQ(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 2  : p = self.alloc(4, Operands{v0, v1, vv[0], vv[1]})
        default : panic("instruction INSERTQ takes 2 or 4 operands")
    }
    // INSERTQ xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE4A)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // INSERTQ imm8, imm8, xmm, xmm
    if len(vv) == 2 && isImm8(v0) && isImm8(v1) && isXMM(vv[0]) && isXMM(vv[1]) {
        self.require(ISA_SSE4A)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[3]), v[2], false)
            m.emit(0x0f)
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for INSERTQ")
    }
    return p
}

// INT performs "Call to Interrupt Procedure".
//
// Mnemonic        : INT
// Supported forms : (2 forms)
//
//    * INT 3
//    * INT imm8
//
func (self *Program) INT(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // INT 3
    if isConst3(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xcc)
        })
    }
    // INT imm8
    if isImm8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xcd)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for INT")
    }
    return p
}

// JA performs "Jump if above (CF == 0 and ZF == 0)".
//
// Mnemonic        : JA
// Supported forms : (2 forms)
//
//    * JA rel8
//    * JA rel32
//
func (self *Program) JA(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    p.branch = _BranchConditional
    // JA rel8
    if isRel8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x77)
            m.imm1(relv(v[0]))
        })
    }
    // JA rel32
    if isRel32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x87)
            m.imm4(relv(v[0]))
        })
    }
    // JA label
    if isLabel(v0) {
        p.add(_REL1, func(m *_Encoding, v []interface{}) {
            m.emit(0x77)
            m.imm1(relv(v[0]))
        })
        p.add(_REL4, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x87)
            m.imm4(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for JA")
    }
    return p
}

// JAE performs "Jump if above or equal (CF == 0)".
//
// Mnemonic        : JAE
// Supported forms : (2 forms)
//
//    * JAE rel8
//    * JAE rel32
//
func (self *Program) JAE(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    p.branch = _BranchConditional
    // JAE rel8
    if isRel8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x73)
            m.imm1(relv(v[0]))
        })
    }
    // JAE rel32
    if isRel32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x83)
            m.imm4(relv(v[0]))
        })
    }
    // JAE label
    if isLabel(v0) {
        p.add(_REL1, func(m *_Encoding, v []interface{}) {
            m.emit(0x73)
            m.imm1(relv(v[0]))
        })
        p.add(_REL4, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x83)
            m.imm4(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for JAE")
    }
    return p
}

// JB performs "Jump if below (CF == 1)".
//
// Mnemonic        : JB
// Supported forms : (2 forms)
//
//    * JB rel8
//    * JB rel32
//
func (self *Program) JB(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    p.branch = _BranchConditional
    // JB rel8
    if isRel8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x72)
            m.imm1(relv(v[0]))
        })
    }
    // JB rel32
    if isRel32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x82)
            m.imm4(relv(v[0]))
        })
    }
    // JB label
    if isLabel(v0) {
        p.add(_REL1, func(m *_Encoding, v []interface{}) {
            m.emit(0x72)
            m.imm1(relv(v[0]))
        })
        p.add(_REL4, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x82)
            m.imm4(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for JB")
    }
    return p
}

// JBE performs "Jump if below or equal (CF == 1 or ZF == 1)".
//
// Mnemonic        : JBE
// Supported forms : (2 forms)
//
//    * JBE rel8
//    * JBE rel32
//
func (self *Program) JBE(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    p.branch = _BranchConditional
    // JBE rel8
    if isRel8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x76)
            m.imm1(relv(v[0]))
        })
    }
    // JBE rel32
    if isRel32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x86)
            m.imm4(relv(v[0]))
        })
    }
    // JBE label
    if isLabel(v0) {
        p.add(_REL1, func(m *_Encoding, v []interface{}) {
            m.emit(0x76)
            m.imm1(relv(v[0]))
        })
        p.add(_REL4, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x86)
            m.imm4(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for JBE")
    }
    return p
}

// JC performs "Jump if carry (CF == 1)".
//
// Mnemonic        : JC
// Supported forms : (2 forms)
//
//    * JC rel8
//    * JC rel32
//
func (self *Program) JC(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    p.branch = _BranchConditional
    // JC rel8
    if isRel8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x72)
            m.imm1(relv(v[0]))
        })
    }
    // JC rel32
    if isRel32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x82)
            m.imm4(relv(v[0]))
        })
    }
    // JC label
    if isLabel(v0) {
        p.add(_REL1, func(m *_Encoding, v []interface{}) {
            m.emit(0x72)
            m.imm1(relv(v[0]))
        })
        p.add(_REL4, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x82)
            m.imm4(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for JC")
    }
    return p
}

// JE performs "Jump if equal (ZF == 1)".
//
// Mnemonic        : JE
// Supported forms : (2 forms)
//
//    * JE rel8
//    * JE rel32
//
func (self *Program) JE(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    p.branch = _BranchConditional
    // JE rel8
    if isRel8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x74)
            m.imm1(relv(v[0]))
        })
    }
    // JE rel32
    if isRel32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x84)
            m.imm4(relv(v[0]))
        })
    }
    // JE label
    if isLabel(v0) {
        p.add(_REL1, func(m *_Encoding, v []interface{}) {
            m.emit(0x74)
            m.imm1(relv(v[0]))
        })
        p.add(_REL4, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x84)
            m.imm4(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for JE")
    }
    return p
}

// JECXZ performs "Jump if ECX register is 0".
//
// Mnemonic        : JECXZ
// Supported forms : (1 form)
//
//    * JECXZ rel8
//
func (self *Program) JECXZ(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    p.branch = _BranchConditional
    // JECXZ rel8
    if isRel8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xe3)
            m.imm1(relv(v[0]))
        })
    }
    // JECXZ label
    if isLabel(v0) {
        p.add(_REL1, func(m *_Encoding, v []interface{}) {
            m.emit(0xe3)
            m.imm1(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for JECXZ")
    }
    return p
}

// JG performs "Jump if greater (ZF == 0 and SF == OF)".
//
// Mnemonic        : JG
// Supported forms : (2 forms)
//
//    * JG rel8
//    * JG rel32
//
func (self *Program) JG(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    p.branch = _BranchConditional
    // JG rel8
    if isRel8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x7f)
            m.imm1(relv(v[0]))
        })
    }
    // JG rel32
    if isRel32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x8f)
            m.imm4(relv(v[0]))
        })
    }
    // JG label
    if isLabel(v0) {
        p.add(_REL1, func(m *_Encoding, v []interface{}) {
            m.emit(0x7f)
            m.imm1(relv(v[0]))
        })
        p.add(_REL4, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x8f)
            m.imm4(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for JG")
    }
    return p
}

// JGE performs "Jump if greater or equal (SF == OF)".
//
// Mnemonic        : JGE
// Supported forms : (2 forms)
//
//    * JGE rel8
//    * JGE rel32
//
func (self *Program) JGE(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    p.branch = _BranchConditional
    // JGE rel8
    if isRel8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x7d)
            m.imm1(relv(v[0]))
        })
    }
    // JGE rel32
    if isRel32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x8d)
            m.imm4(relv(v[0]))
        })
    }
    // JGE label
    if isLabel(v0) {
        p.add(_REL1, func(m *_Encoding, v []interface{}) {
            m.emit(0x7d)
            m.imm1(relv(v[0]))
        })
        p.add(_REL4, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x8d)
            m.imm4(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for JGE")
    }
    return p
}

// JL performs "Jump if less (SF != OF)".
//
// Mnemonic        : JL
// Supported forms : (2 forms)
//
//    * JL rel8
//    * JL rel32
//
func (self *Program) JL(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    p.branch = _BranchConditional
    // JL rel8
    if isRel8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x7c)
            m.imm1(relv(v[0]))
        })
    }
    // JL rel32
    if isRel32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x8c)
            m.imm4(relv(v[0]))
        })
    }
    // JL label
    if isLabel(v0) {
        p.add(_REL1, func(m *_Encoding, v []interface{}) {
            m.emit(0x7c)
            m.imm1(relv(v[0]))
        })
        p.add(_REL4, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x8c)
            m.imm4(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for JL")
    }
    return p
}

// JLE performs "Jump if less or equal (ZF == 1 or SF != OF)".
//
// Mnemonic        : JLE
// Supported forms : (2 forms)
//
//    * JLE rel8
//    * JLE rel32
//
func (self *Program) JLE(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    p.branch = _BranchConditional
    // JLE rel8
    if isRel8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x7e)
            m.imm1(relv(v[0]))
        })
    }
    // JLE rel32
    if isRel32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x8e)
            m.imm4(relv(v[0]))
        })
    }
    // JLE label
    if isLabel(v0) {
        p.add(_REL1, func(m *_Encoding, v []interface{}) {
            m.emit(0x7e)
            m.imm1(relv(v[0]))
        })
        p.add(_REL4, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x8e)
            m.imm4(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for JLE")
    }
    return p
}

// JMP performs "Jump Unconditionally".
//
// Mnemonic        : JMP
// Supported forms : (2 forms)
//
//    * JMP rel8
//    * JMP rel32
//
func (self *Program) JMP(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    p.branch = _BranchUnconditional
    // JMP rel8
    if isRel8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xeb)
            m.imm1(relv(v[0]))
        })
    }
    // JMP rel32
    if isRel32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xe9)
            m.imm4(relv(v[0]))
        })
    }
    // JMP label
    if isLabel(v0) {
        p.add(_REL1, func(m *_Encoding, v []interface{}) {
            m.emit(0xeb)
            m.imm1(relv(v[0]))
        })
        p.add(_REL4, func(m *_Encoding, v []interface{}) {
            m.emit(0xe9)
            m.imm4(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for JMP")
    }
    return p
}

// JMPQ performs "Jump Unconditionally".
//
// Mnemonic        : JMP
// Supported forms : (2 forms)
//
//    * JMPQ r64
//    * JMPQ m64
//
func (self *Program) JMPQ(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // JMPQ r64
    if isReg64(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], false)
            m.emit(0xff)
            m.emit(0xe0 | lcode(v[0]))
        })
    }
    // JMPQ m64
    if isM64(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0xff)
            m.mrsd(4, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for JMPQ")
    }
    return p
}

// JNA performs "Jump if not above (CF == 1 or ZF == 1)".
//
// Mnemonic        : JNA
// Supported forms : (2 forms)
//
//    * JNA rel8
//    * JNA rel32
//
func (self *Program) JNA(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    p.branch = _BranchConditional
    // JNA rel8
    if isRel8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x76)
            m.imm1(relv(v[0]))
        })
    }
    // JNA rel32
    if isRel32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x86)
            m.imm4(relv(v[0]))
        })
    }
    // JNA label
    if isLabel(v0) {
        p.add(_REL1, func(m *_Encoding, v []interface{}) {
            m.emit(0x76)
            m.imm1(relv(v[0]))
        })
        p.add(_REL4, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x86)
            m.imm4(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for JNA")
    }
    return p
}

// JNAE performs "Jump if not above or equal (CF == 1)".
//
// Mnemonic        : JNAE
// Supported forms : (2 forms)
//
//    * JNAE rel8
//    * JNAE rel32
//
func (self *Program) JNAE(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    p.branch = _BranchConditional
    // JNAE rel8
    if isRel8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x72)
            m.imm1(relv(v[0]))
        })
    }
    // JNAE rel32
    if isRel32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x82)
            m.imm4(relv(v[0]))
        })
    }
    // JNAE label
    if isLabel(v0) {
        p.add(_REL1, func(m *_Encoding, v []interface{}) {
            m.emit(0x72)
            m.imm1(relv(v[0]))
        })
        p.add(_REL4, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x82)
            m.imm4(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for JNAE")
    }
    return p
}

// JNB performs "Jump if not below (CF == 0)".
//
// Mnemonic        : JNB
// Supported forms : (2 forms)
//
//    * JNB rel8
//    * JNB rel32
//
func (self *Program) JNB(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    p.branch = _BranchConditional
    // JNB rel8
    if isRel8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x73)
            m.imm1(relv(v[0]))
        })
    }
    // JNB rel32
    if isRel32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x83)
            m.imm4(relv(v[0]))
        })
    }
    // JNB label
    if isLabel(v0) {
        p.add(_REL1, func(m *_Encoding, v []interface{}) {
            m.emit(0x73)
            m.imm1(relv(v[0]))
        })
        p.add(_REL4, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x83)
            m.imm4(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for JNB")
    }
    return p
}

// JNBE performs "Jump if not below or equal (CF == 0 and ZF == 0)".
//
// Mnemonic        : JNBE
// Supported forms : (2 forms)
//
//    * JNBE rel8
//    * JNBE rel32
//
func (self *Program) JNBE(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    p.branch = _BranchConditional
    // JNBE rel8
    if isRel8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x77)
            m.imm1(relv(v[0]))
        })
    }
    // JNBE rel32
    if isRel32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x87)
            m.imm4(relv(v[0]))
        })
    }
    // JNBE label
    if isLabel(v0) {
        p.add(_REL1, func(m *_Encoding, v []interface{}) {
            m.emit(0x77)
            m.imm1(relv(v[0]))
        })
        p.add(_REL4, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x87)
            m.imm4(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for JNBE")
    }
    return p
}

// JNC performs "Jump if not carry (CF == 0)".
//
// Mnemonic        : JNC
// Supported forms : (2 forms)
//
//    * JNC rel8
//    * JNC rel32
//
func (self *Program) JNC(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    p.branch = _BranchConditional
    // JNC rel8
    if isRel8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x73)
            m.imm1(relv(v[0]))
        })
    }
    // JNC rel32
    if isRel32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x83)
            m.imm4(relv(v[0]))
        })
    }
    // JNC label
    if isLabel(v0) {
        p.add(_REL1, func(m *_Encoding, v []interface{}) {
            m.emit(0x73)
            m.imm1(relv(v[0]))
        })
        p.add(_REL4, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x83)
            m.imm4(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for JNC")
    }
    return p
}

// JNE performs "Jump if not equal (ZF == 0)".
//
// Mnemonic        : JNE
// Supported forms : (2 forms)
//
//    * JNE rel8
//    * JNE rel32
//
func (self *Program) JNE(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    p.branch = _BranchConditional
    // JNE rel8
    if isRel8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x75)
            m.imm1(relv(v[0]))
        })
    }
    // JNE rel32
    if isRel32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x85)
            m.imm4(relv(v[0]))
        })
    }
    // JNE label
    if isLabel(v0) {
        p.add(_REL1, func(m *_Encoding, v []interface{}) {
            m.emit(0x75)
            m.imm1(relv(v[0]))
        })
        p.add(_REL4, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x85)
            m.imm4(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for JNE")
    }
    return p
}

// JNG performs "Jump if not greater (ZF == 1 or SF != OF)".
//
// Mnemonic        : JNG
// Supported forms : (2 forms)
//
//    * JNG rel8
//    * JNG rel32
//
func (self *Program) JNG(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    p.branch = _BranchConditional
    // JNG rel8
    if isRel8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x7e)
            m.imm1(relv(v[0]))
        })
    }
    // JNG rel32
    if isRel32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x8e)
            m.imm4(relv(v[0]))
        })
    }
    // JNG label
    if isLabel(v0) {
        p.add(_REL1, func(m *_Encoding, v []interface{}) {
            m.emit(0x7e)
            m.imm1(relv(v[0]))
        })
        p.add(_REL4, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x8e)
            m.imm4(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for JNG")
    }
    return p
}

// JNGE performs "Jump if not greater or equal (SF != OF)".
//
// Mnemonic        : JNGE
// Supported forms : (2 forms)
//
//    * JNGE rel8
//    * JNGE rel32
//
func (self *Program) JNGE(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    p.branch = _BranchConditional
    // JNGE rel8
    if isRel8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x7c)
            m.imm1(relv(v[0]))
        })
    }
    // JNGE rel32
    if isRel32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x8c)
            m.imm4(relv(v[0]))
        })
    }
    // JNGE label
    if isLabel(v0) {
        p.add(_REL1, func(m *_Encoding, v []interface{}) {
            m.emit(0x7c)
            m.imm1(relv(v[0]))
        })
        p.add(_REL4, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x8c)
            m.imm4(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for JNGE")
    }
    return p
}

// JNL performs "Jump if not less (SF == OF)".
//
// Mnemonic        : JNL
// Supported forms : (2 forms)
//
//    * JNL rel8
//    * JNL rel32
//
func (self *Program) JNL(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    p.branch = _BranchConditional
    // JNL rel8
    if isRel8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x7d)
            m.imm1(relv(v[0]))
        })
    }
    // JNL rel32
    if isRel32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x8d)
            m.imm4(relv(v[0]))
        })
    }
    // JNL label
    if isLabel(v0) {
        p.add(_REL1, func(m *_Encoding, v []interface{}) {
            m.emit(0x7d)
            m.imm1(relv(v[0]))
        })
        p.add(_REL4, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x8d)
            m.imm4(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for JNL")
    }
    return p
}

// JNLE performs "Jump if not less or equal (ZF == 0 and SF == OF)".
//
// Mnemonic        : JNLE
// Supported forms : (2 forms)
//
//    * JNLE rel8
//    * JNLE rel32
//
func (self *Program) JNLE(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    p.branch = _BranchConditional
    // JNLE rel8
    if isRel8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x7f)
            m.imm1(relv(v[0]))
        })
    }
    // JNLE rel32
    if isRel32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x8f)
            m.imm4(relv(v[0]))
        })
    }
    // JNLE label
    if isLabel(v0) {
        p.add(_REL1, func(m *_Encoding, v []interface{}) {
            m.emit(0x7f)
            m.imm1(relv(v[0]))
        })
        p.add(_REL4, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x8f)
            m.imm4(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for JNLE")
    }
    return p
}

// JNO performs "Jump if not overflow (OF == 0)".
//
// Mnemonic        : JNO
// Supported forms : (2 forms)
//
//    * JNO rel8
//    * JNO rel32
//
func (self *Program) JNO(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    p.branch = _BranchConditional
    // JNO rel8
    if isRel8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x71)
            m.imm1(relv(v[0]))
        })
    }
    // JNO rel32
    if isRel32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x81)
            m.imm4(relv(v[0]))
        })
    }
    // JNO label
    if isLabel(v0) {
        p.add(_REL1, func(m *_Encoding, v []interface{}) {
            m.emit(0x71)
            m.imm1(relv(v[0]))
        })
        p.add(_REL4, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x81)
            m.imm4(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for JNO")
    }
    return p
}

// JNP performs "Jump if not parity (PF == 0)".
//
// Mnemonic        : JNP
// Supported forms : (2 forms)
//
//    * JNP rel8
//    * JNP rel32
//
func (self *Program) JNP(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    p.branch = _BranchConditional
    // JNP rel8
    if isRel8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x7b)
            m.imm1(relv(v[0]))
        })
    }
    // JNP rel32
    if isRel32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x8b)
            m.imm4(relv(v[0]))
        })
    }
    // JNP label
    if isLabel(v0) {
        p.add(_REL1, func(m *_Encoding, v []interface{}) {
            m.emit(0x7b)
            m.imm1(relv(v[0]))
        })
        p.add(_REL4, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x8b)
            m.imm4(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for JNP")
    }
    return p
}

// JNS performs "Jump if not sign (SF == 0)".
//
// Mnemonic        : JNS
// Supported forms : (2 forms)
//
//    * JNS rel8
//    * JNS rel32
//
func (self *Program) JNS(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    p.branch = _BranchConditional
    // JNS rel8
    if isRel8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x79)
            m.imm1(relv(v[0]))
        })
    }
    // JNS rel32
    if isRel32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x89)
            m.imm4(relv(v[0]))
        })
    }
    // JNS label
    if isLabel(v0) {
        p.add(_REL1, func(m *_Encoding, v []interface{}) {
            m.emit(0x79)
            m.imm1(relv(v[0]))
        })
        p.add(_REL4, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x89)
            m.imm4(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for JNS")
    }
    return p
}

// JNZ performs "Jump if not zero (ZF == 0)".
//
// Mnemonic        : JNZ
// Supported forms : (2 forms)
//
//    * JNZ rel8
//    * JNZ rel32
//
func (self *Program) JNZ(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    p.branch = _BranchConditional
    // JNZ rel8
    if isRel8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x75)
            m.imm1(relv(v[0]))
        })
    }
    // JNZ rel32
    if isRel32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x85)
            m.imm4(relv(v[0]))
        })
    }
    // JNZ label
    if isLabel(v0) {
        p.add(_REL1, func(m *_Encoding, v []interface{}) {
            m.emit(0x75)
            m.imm1(relv(v[0]))
        })
        p.add(_REL4, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x85)
            m.imm4(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for JNZ")
    }
    return p
}

// JO performs "Jump if overflow (OF == 1)".
//
// Mnemonic        : JO
// Supported forms : (2 forms)
//
//    * JO rel8
//    * JO rel32
//
func (self *Program) JO(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    p.branch = _BranchConditional
    // JO rel8
    if isRel8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x70)
            m.imm1(relv(v[0]))
        })
    }
    // JO rel32
    if isRel32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x80)
            m.imm4(relv(v[0]))
        })
    }
    // JO label
    if isLabel(v0) {
        p.add(_REL1, func(m *_Encoding, v []interface{}) {
            m.emit(0x70)
            m.imm1(relv(v[0]))
        })
        p.add(_REL4, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x80)
            m.imm4(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for JO")
    }
    return p
}

// JP performs "Jump if parity (PF == 1)".
//
// Mnemonic        : JP
// Supported forms : (2 forms)
//
//    * JP rel8
//    * JP rel32
//
func (self *Program) JP(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    p.branch = _BranchConditional
    // JP rel8
    if isRel8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x7a)
            m.imm1(relv(v[0]))
        })
    }
    // JP rel32
    if isRel32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x8a)
            m.imm4(relv(v[0]))
        })
    }
    // JP label
    if isLabel(v0) {
        p.add(_REL1, func(m *_Encoding, v []interface{}) {
            m.emit(0x7a)
            m.imm1(relv(v[0]))
        })
        p.add(_REL4, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x8a)
            m.imm4(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for JP")
    }
    return p
}

// JPE performs "Jump if parity even (PF == 1)".
//
// Mnemonic        : JPE
// Supported forms : (2 forms)
//
//    * JPE rel8
//    * JPE rel32
//
func (self *Program) JPE(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    p.branch = _BranchConditional
    // JPE rel8
    if isRel8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x7a)
            m.imm1(relv(v[0]))
        })
    }
    // JPE rel32
    if isRel32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x8a)
            m.imm4(relv(v[0]))
        })
    }
    // JPE label
    if isLabel(v0) {
        p.add(_REL1, func(m *_Encoding, v []interface{}) {
            m.emit(0x7a)
            m.imm1(relv(v[0]))
        })
        p.add(_REL4, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x8a)
            m.imm4(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for JPE")
    }
    return p
}

// JPO performs "Jump if parity odd (PF == 0)".
//
// Mnemonic        : JPO
// Supported forms : (2 forms)
//
//    * JPO rel8
//    * JPO rel32
//
func (self *Program) JPO(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    p.branch = _BranchConditional
    // JPO rel8
    if isRel8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x7b)
            m.imm1(relv(v[0]))
        })
    }
    // JPO rel32
    if isRel32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x8b)
            m.imm4(relv(v[0]))
        })
    }
    // JPO label
    if isLabel(v0) {
        p.add(_REL1, func(m *_Encoding, v []interface{}) {
            m.emit(0x7b)
            m.imm1(relv(v[0]))
        })
        p.add(_REL4, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x8b)
            m.imm4(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for JPO")
    }
    return p
}

// JRCXZ performs "Jump if RCX register is 0".
//
// Mnemonic        : JRCXZ
// Supported forms : (1 form)
//
//    * JRCXZ rel8
//
func (self *Program) JRCXZ(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    p.branch = _BranchConditional
    // JRCXZ rel8
    if isRel8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xe3)
            m.imm1(relv(v[0]))
        })
    }
    // JRCXZ label
    if isLabel(v0) {
        p.add(_REL1, func(m *_Encoding, v []interface{}) {
            m.emit(0xe3)
            m.imm1(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for JRCXZ")
    }
    return p
}

// JS performs "Jump if sign (SF == 1)".
//
// Mnemonic        : JS
// Supported forms : (2 forms)
//
//    * JS rel8
//    * JS rel32
//
func (self *Program) JS(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    p.branch = _BranchConditional
    // JS rel8
    if isRel8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x78)
            m.imm1(relv(v[0]))
        })
    }
    // JS rel32
    if isRel32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x88)
            m.imm4(relv(v[0]))
        })
    }
    // JS label
    if isLabel(v0) {
        p.add(_REL1, func(m *_Encoding, v []interface{}) {
            m.emit(0x78)
            m.imm1(relv(v[0]))
        })
        p.add(_REL4, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x88)
            m.imm4(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for JS")
    }
    return p
}

// JZ performs "Jump if zero (ZF == 1)".
//
// Mnemonic        : JZ
// Supported forms : (2 forms)
//
//    * JZ rel8
//    * JZ rel32
//
func (self *Program) JZ(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    p.branch = _BranchConditional
    // JZ rel8
    if isRel8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x74)
            m.imm1(relv(v[0]))
        })
    }
    // JZ rel32
    if isRel32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x84)
            m.imm4(relv(v[0]))
        })
    }
    // JZ label
    if isLabel(v0) {
        p.add(_REL1, func(m *_Encoding, v []interface{}) {
            m.emit(0x74)
            m.imm1(relv(v[0]))
        })
        p.add(_REL4, func(m *_Encoding, v []interface{}) {
            m.emit(0x0f)
            m.emit(0x84)
            m.imm4(relv(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for JZ")
    }
    return p
}

// KADDB performs "ADD Two 8-bit Masks".
//
// Mnemonic        : KADDB
// ISA extensions  : AVX512DQ
// Supported forms : (1 form)
//
//    * KADDB k, k, k
//
func (self *Program) KADDB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KADDB k, k, k
    if isK(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, 0, nil, hlcode(v[1]))
            m.emit(0x4a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KADDB")
    }
    return p
}

// KADDD performs "ADD Two 32-bit Masks".
//
// Mnemonic        : KADDD
// ISA extensions  : AVX512BW
// Supported forms : (1 form)
//
//    * KADDD k, k, k
//
func (self *Program) KADDD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KADDD k, k, k
    if isK(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe1)
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit(0x4a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KADDD")
    }
    return p
}

// KADDQ performs "ADD Two 64-bit Masks".
//
// Mnemonic        : KADDQ
// ISA extensions  : AVX512BW
// Supported forms : (1 form)
//
//    * KADDQ k, k, k
//
func (self *Program) KADDQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KADDQ k, k, k
    if isK(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe1)
            m.emit(0xfc ^ (hlcode(v[1]) << 3))
            m.emit(0x4a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KADDQ")
    }
    return p
}

// KADDW performs "ADD Two 16-bit Masks".
//
// Mnemonic        : KADDW
// ISA extensions  : AVX512DQ
// Supported forms : (1 form)
//
//    * KADDW k, k, k
//
func (self *Program) KADDW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KADDW k, k, k
    if isK(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, 0, nil, hlcode(v[1]))
            m.emit(0x4a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KADDW")
    }
    return p
}

// KANDB performs "Bitwise Logical AND 8-bit Masks".
//
// Mnemonic        : KANDB
// ISA extensions  : AVX512DQ
// Supported forms : (1 form)
//
//    * KANDB k, k, k
//
func (self *Program) KANDB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KANDB k, k, k
    if isK(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, 0, nil, hlcode(v[1]))
            m.emit(0x41)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KANDB")
    }
    return p
}

// KANDD performs "Bitwise Logical AND 32-bit Masks".
//
// Mnemonic        : KANDD
// ISA extensions  : AVX512BW
// Supported forms : (1 form)
//
//    * KANDD k, k, k
//
func (self *Program) KANDD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KANDD k, k, k
    if isK(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe1)
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit(0x41)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KANDD")
    }
    return p
}

// KANDNB performs "Bitwise Logical AND NOT 8-bit Masks".
//
// Mnemonic        : KANDNB
// ISA extensions  : AVX512DQ
// Supported forms : (1 form)
//
//    * KANDNB k, k, k
//
func (self *Program) KANDNB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KANDNB k, k, k
    if isK(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, 0, nil, hlcode(v[1]))
            m.emit(0x42)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KANDNB")
    }
    return p
}

// KANDND performs "Bitwise Logical AND NOT 32-bit Masks".
//
// Mnemonic        : KANDND
// ISA extensions  : AVX512BW
// Supported forms : (1 form)
//
//    * KANDND k, k, k
//
func (self *Program) KANDND(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KANDND k, k, k
    if isK(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe1)
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit(0x42)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KANDND")
    }
    return p
}

// KANDNQ performs "Bitwise Logical AND NOT 64-bit Masks".
//
// Mnemonic        : KANDNQ
// ISA extensions  : AVX512BW
// Supported forms : (1 form)
//
//    * KANDNQ k, k, k
//
func (self *Program) KANDNQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KANDNQ k, k, k
    if isK(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe1)
            m.emit(0xfc ^ (hlcode(v[1]) << 3))
            m.emit(0x42)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KANDNQ")
    }
    return p
}

// KANDNW performs "Bitwise Logical AND NOT 16-bit Masks".
//
// Mnemonic        : KANDNW
// ISA extensions  : AVX512F
// Supported forms : (1 form)
//
//    * KANDNW k, k, k
//
func (self *Program) KANDNW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KANDNW k, k, k
    if isK(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, 0, nil, hlcode(v[1]))
            m.emit(0x42)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KANDNW")
    }
    return p
}

// KANDQ performs "Bitwise Logical AND 64-bit Masks".
//
// Mnemonic        : KANDQ
// ISA extensions  : AVX512BW
// Supported forms : (1 form)
//
//    * KANDQ k, k, k
//
func (self *Program) KANDQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KANDQ k, k, k
    if isK(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe1)
            m.emit(0xfc ^ (hlcode(v[1]) << 3))
            m.emit(0x41)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KANDQ")
    }
    return p
}

// KANDW performs "Bitwise Logical AND 16-bit Masks".
//
// Mnemonic        : KANDW
// ISA extensions  : AVX512F
// Supported forms : (1 form)
//
//    * KANDW k, k, k
//
func (self *Program) KANDW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KANDW k, k, k
    if isK(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, 0, nil, hlcode(v[1]))
            m.emit(0x41)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KANDW")
    }
    return p
}

// KMOVB performs "Move 8-bit Mask".
//
// Mnemonic        : KMOVB
// ISA extensions  : AVX512DQ
// Supported forms : (5 forms)
//
//    * KMOVB k, k
//    * KMOVB r32, k
//    * KMOVB m8, k
//    * KMOVB k, r32
//    * KMOVB k, m8
//
func (self *Program) KMOVB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // KMOVB k, k
    if isK(v0) && isK(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, 0, nil, 0)
            m.emit(0x90)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // KMOVB r32, k
    if isReg32(v0) && isK(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, 0, v[0], 0)
            m.emit(0x92)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // KMOVB m8, k
    if isM8(v0) && isK(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, 0, addr(v[0]), 0)
            m.emit(0x90)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // KMOVB k, r32
    if isK(v0) && isReg32(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[1]), nil, 0)
            m.emit(0x93)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // KMOVB k, m8
    if isK(v0) && isM8(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, 0, addr(v[1]), 0)
            m.emit(0x91)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for KMOVB")
    }
    return p
}

// KMOVD performs "Move 32-bit Mask".
//
// Mnemonic        : KMOVD
// ISA extensions  : AVX512BW
// Supported forms : (5 forms)
//
//    * KMOVD k, k
//    * KMOVD r32, k
//    * KMOVD m32, k
//    * KMOVD k, r32
//    * KMOVD k, m32
//
func (self *Program) KMOVD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // KMOVD k, k
    if isK(v0) && isK(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe1)
            m.emit(0xf9)
            m.emit(0x90)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // KMOVD r32, k
    if isReg32(v0) && isK(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, 0, v[0], 0)
            m.emit(0x92)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // KMOVD m32, k
    if isM32(v0) && isK(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b1, 0x81, 0, addr(v[0]), 0)
            m.emit(0x90)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // KMOVD k, r32
    if isK(v0) && isReg32(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[1]), nil, 0)
            m.emit(0x93)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // KMOVD k, m32
    if isK(v0) && isM32(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b1, 0x81, 0, addr(v[1]), 0)
            m.emit(0x91)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for KMOVD")
    }
    return p
}

// KMOVQ performs "Move 64-bit Mask".
//
// Mnemonic        : KMOVQ
// ISA extensions  : AVX512BW
// Supported forms : (5 forms)
//
//    * KMOVQ k, k
//    * KMOVQ r64, k
//    * KMOVQ m64, k
//    * KMOVQ k, r64
//    * KMOVQ k, m64
//
func (self *Program) KMOVQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // KMOVQ k, k
    if isK(v0) && isK(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe1)
            m.emit(0xf8)
            m.emit(0x90)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // KMOVQ r64, k
    if isReg64(v0) && isK(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe1 ^ (hcode(v[0]) << 5))
            m.emit(0xfb)
            m.emit(0x92)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // KMOVQ m64, k
    if isM64(v0) && isK(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b1, 0x80, 0, addr(v[0]), 0)
            m.emit(0x90)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // KMOVQ k, r64
    if isK(v0) && isReg64(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe1 ^ (hcode(v[1]) << 7))
            m.emit(0xfb)
            m.emit(0x93)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // KMOVQ k, m64
    if isK(v0) && isM64(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b1, 0x80, 0, addr(v[1]), 0)
            m.emit(0x91)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for KMOVQ")
    }
    return p
}

// KMOVW performs "Move 16-bit Mask".
//
// Mnemonic        : KMOVW
// ISA extensions  : AVX512F
// Supported forms : (5 forms)
//
//    * KMOVW k, k
//    * KMOVW r32, k
//    * KMOVW m16, k
//    * KMOVW k, r32
//    * KMOVW k, m16
//
func (self *Program) KMOVW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // KMOVW k, k
    if isK(v0) && isK(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, 0, nil, 0)
            m.emit(0x90)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // KMOVW r32, k
    if isReg32(v0) && isK(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, 0, v[0], 0)
            m.emit(0x92)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // KMOVW m16, k
    if isM16(v0) && isK(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, 0, addr(v[0]), 0)
            m.emit(0x90)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // KMOVW k, r32
    if isK(v0) && isReg32(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[1]), nil, 0)
            m.emit(0x93)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // KMOVW k, m16
    if isK(v0) && isM16(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, 0, addr(v[1]), 0)
            m.emit(0x91)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for KMOVW")
    }
    return p
}

// KNOTB performs "NOT 8-bit Mask Register".
//
// Mnemonic        : KNOTB
// ISA extensions  : AVX512DQ
// Supported forms : (1 form)
//
//    * KNOTB k, k
//
func (self *Program) KNOTB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // KNOTB k, k
    if isK(v0) && isK(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, 0, nil, 0)
            m.emit(0x44)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KNOTB")
    }
    return p
}

// KNOTD performs "NOT 32-bit Mask Register".
//
// Mnemonic        : KNOTD
// ISA extensions  : AVX512BW
// Supported forms : (1 form)
//
//    * KNOTD k, k
//
func (self *Program) KNOTD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // KNOTD k, k
    if isK(v0) && isK(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe1)
            m.emit(0xf9)
            m.emit(0x44)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KNOTD")
    }
    return p
}

// KNOTQ performs "NOT 64-bit Mask Register".
//
// Mnemonic        : KNOTQ
// ISA extensions  : AVX512BW
// Supported forms : (1 form)
//
//    * KNOTQ k, k
//
func (self *Program) KNOTQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // KNOTQ k, k
    if isK(v0) && isK(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe1)
            m.emit(0xf8)
            m.emit(0x44)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KNOTQ")
    }
    return p
}

// KNOTW performs "NOT 16-bit Mask Register".
//
// Mnemonic        : KNOTW
// ISA extensions  : AVX512F
// Supported forms : (1 form)
//
//    * KNOTW k, k
//
func (self *Program) KNOTW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // KNOTW k, k
    if isK(v0) && isK(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, 0, nil, 0)
            m.emit(0x44)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KNOTW")
    }
    return p
}

// KORB performs "Bitwise Logical OR 8-bit Masks".
//
// Mnemonic        : KORB
// ISA extensions  : AVX512DQ
// Supported forms : (1 form)
//
//    * KORB k, k, k
//
func (self *Program) KORB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KORB k, k, k
    if isK(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, 0, nil, hlcode(v[1]))
            m.emit(0x45)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KORB")
    }
    return p
}

// KORD performs "Bitwise Logical OR 32-bit Masks".
//
// Mnemonic        : KORD
// ISA extensions  : AVX512BW
// Supported forms : (1 form)
//
//    * KORD k, k, k
//
func (self *Program) KORD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KORD k, k, k
    if isK(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe1)
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit(0x45)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KORD")
    }
    return p
}

// KORQ performs "Bitwise Logical OR 64-bit Masks".
//
// Mnemonic        : KORQ
// ISA extensions  : AVX512BW
// Supported forms : (1 form)
//
//    * KORQ k, k, k
//
func (self *Program) KORQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KORQ k, k, k
    if isK(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe1)
            m.emit(0xfc ^ (hlcode(v[1]) << 3))
            m.emit(0x45)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KORQ")
    }
    return p
}

// KORTESTB performs "OR 8-bit Masks and Set Flags".
//
// Mnemonic        : KORTESTB
// ISA extensions  : AVX512DQ
// Supported forms : (1 form)
//
//    * KORTESTB k, k
//
func (self *Program) KORTESTB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // KORTESTB k, k
    if isK(v0) && isK(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, 0, nil, 0)
            m.emit(0x98)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KORTESTB")
    }
    return p
}

// KORTESTD performs "OR 32-bit Masks and Set Flags".
//
// Mnemonic        : KORTESTD
// ISA extensions  : AVX512BW
// Supported forms : (1 form)
//
//    * KORTESTD k, k
//
func (self *Program) KORTESTD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // KORTESTD k, k
    if isK(v0) && isK(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe1)
            m.emit(0xf9)
            m.emit(0x98)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KORTESTD")
    }
    return p
}

// KORTESTQ performs "OR 64-bit Masks and Set Flags".
//
// Mnemonic        : KORTESTQ
// ISA extensions  : AVX512BW
// Supported forms : (1 form)
//
//    * KORTESTQ k, k
//
func (self *Program) KORTESTQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // KORTESTQ k, k
    if isK(v0) && isK(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe1)
            m.emit(0xf8)
            m.emit(0x98)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KORTESTQ")
    }
    return p
}

// KORTESTW performs "OR 16-bit Masks and Set Flags".
//
// Mnemonic        : KORTESTW
// ISA extensions  : AVX512F
// Supported forms : (1 form)
//
//    * KORTESTW k, k
//
func (self *Program) KORTESTW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // KORTESTW k, k
    if isK(v0) && isK(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, 0, nil, 0)
            m.emit(0x98)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KORTESTW")
    }
    return p
}

// KORW performs "Bitwise Logical OR 16-bit Masks".
//
// Mnemonic        : KORW
// ISA extensions  : AVX512F
// Supported forms : (1 form)
//
//    * KORW k, k, k
//
func (self *Program) KORW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KORW k, k, k
    if isK(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, 0, nil, hlcode(v[1]))
            m.emit(0x45)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KORW")
    }
    return p
}

// KSHIFTLB performs "Shift Left 8-bit Masks".
//
// Mnemonic        : KSHIFTLB
// ISA extensions  : AVX512DQ
// Supported forms : (1 form)
//
//    * KSHIFTLB imm8, k, k
//
func (self *Program) KSHIFTLB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KSHIFTLB imm8, k, k
    if isImm8(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3)
            m.emit(0x79)
            m.emit(0x32)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KSHIFTLB")
    }
    return p
}

// KSHIFTLD performs "Shift Left 32-bit Masks".
//
// Mnemonic        : KSHIFTLD
// ISA extensions  : AVX512BW
// Supported forms : (1 form)
//
//    * KSHIFTLD imm8, k, k
//
func (self *Program) KSHIFTLD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KSHIFTLD imm8, k, k
    if isImm8(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3)
            m.emit(0x79)
            m.emit(0x33)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KSHIFTLD")
    }
    return p
}

// KSHIFTLQ performs "Shift Left 64-bit Masks".
//
// Mnemonic        : KSHIFTLQ
// ISA extensions  : AVX512BW
// Supported forms : (1 form)
//
//    * KSHIFTLQ imm8, k, k
//
func (self *Program) KSHIFTLQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KSHIFTLQ imm8, k, k
    if isImm8(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3)
            m.emit(0xf9)
            m.emit(0x33)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KSHIFTLQ")
    }
    return p
}

// KSHIFTLW performs "Shift Left 16-bit Masks".
//
// Mnemonic        : KSHIFTLW
// ISA extensions  : AVX512F
// Supported forms : (1 form)
//
//    * KSHIFTLW imm8, k, k
//
func (self *Program) KSHIFTLW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KSHIFTLW imm8, k, k
    if isImm8(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3)
            m.emit(0xf9)
            m.emit(0x32)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KSHIFTLW")
    }
    return p
}

// KSHIFTRB performs "Shift Right 8-bit Masks".
//
// Mnemonic        : KSHIFTRB
// ISA extensions  : AVX512DQ
// Supported forms : (1 form)
//
//    * KSHIFTRB imm8, k, k
//
func (self *Program) KSHIFTRB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KSHIFTRB imm8, k, k
    if isImm8(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3)
            m.emit(0x79)
            m.emit(0x30)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KSHIFTRB")
    }
    return p
}

// KSHIFTRD performs "Shift Right 32-bit Masks".
//
// Mnemonic        : KSHIFTRD
// ISA extensions  : AVX512BW
// Supported forms : (1 form)
//
//    * KSHIFTRD imm8, k, k
//
func (self *Program) KSHIFTRD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KSHIFTRD imm8, k, k
    if isImm8(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3)
            m.emit(0x79)
            m.emit(0x31)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KSHIFTRD")
    }
    return p
}

// KSHIFTRQ performs "Shift Right 64-bit Masks".
//
// Mnemonic        : KSHIFTRQ
// ISA extensions  : AVX512BW
// Supported forms : (1 form)
//
//    * KSHIFTRQ imm8, k, k
//
func (self *Program) KSHIFTRQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KSHIFTRQ imm8, k, k
    if isImm8(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3)
            m.emit(0xf9)
            m.emit(0x31)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KSHIFTRQ")
    }
    return p
}

// KSHIFTRW performs "Shift Right 16-bit Masks".
//
// Mnemonic        : KSHIFTRW
// ISA extensions  : AVX512F
// Supported forms : (1 form)
//
//    * KSHIFTRW imm8, k, k
//
func (self *Program) KSHIFTRW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KSHIFTRW imm8, k, k
    if isImm8(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3)
            m.emit(0xf9)
            m.emit(0x30)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KSHIFTRW")
    }
    return p
}

// KTESTB performs "Bit Test 8-bit Masks and Set Flags".
//
// Mnemonic        : KTESTB
// ISA extensions  : AVX512DQ
// Supported forms : (1 form)
//
//    * KTESTB k, k
//
func (self *Program) KTESTB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // KTESTB k, k
    if isK(v0) && isK(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, 0, nil, 0)
            m.emit(0x99)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KTESTB")
    }
    return p
}

// KTESTD performs "Bit Test 32-bit Masks and Set Flags".
//
// Mnemonic        : KTESTD
// ISA extensions  : AVX512BW
// Supported forms : (1 form)
//
//    * KTESTD k, k
//
func (self *Program) KTESTD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // KTESTD k, k
    if isK(v0) && isK(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe1)
            m.emit(0xf9)
            m.emit(0x99)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KTESTD")
    }
    return p
}

// KTESTQ performs "Bit Test 64-bit Masks and Set Flags".
//
// Mnemonic        : KTESTQ
// ISA extensions  : AVX512BW
// Supported forms : (1 form)
//
//    * KTESTQ k, k
//
func (self *Program) KTESTQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // KTESTQ k, k
    if isK(v0) && isK(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe1)
            m.emit(0xf8)
            m.emit(0x99)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KTESTQ")
    }
    return p
}

// KTESTW performs "Bit Test 16-bit Masks and Set Flags".
//
// Mnemonic        : KTESTW
// ISA extensions  : AVX512DQ
// Supported forms : (1 form)
//
//    * KTESTW k, k
//
func (self *Program) KTESTW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // KTESTW k, k
    if isK(v0) && isK(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, 0, nil, 0)
            m.emit(0x99)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KTESTW")
    }
    return p
}

// KUNPCKBW performs "Unpack and Interleave 8-bit Masks".
//
// Mnemonic        : KUNPCKBW
// ISA extensions  : AVX512F
// Supported forms : (1 form)
//
//    * KUNPCKBW k, k, k
//
func (self *Program) KUNPCKBW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KUNPCKBW k, k, k
    if isK(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, 0, nil, hlcode(v[1]))
            m.emit(0x4b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KUNPCKBW")
    }
    return p
}

// KUNPCKDQ performs "Unpack and Interleave 32-bit Masks".
//
// Mnemonic        : KUNPCKDQ
// ISA extensions  : AVX512BW
// Supported forms : (1 form)
//
//    * KUNPCKDQ k, k, k
//
func (self *Program) KUNPCKDQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KUNPCKDQ k, k, k
    if isK(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe1)
            m.emit(0xfc ^ (hlcode(v[1]) << 3))
            m.emit(0x4b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KUNPCKDQ")
    }
    return p
}

// KUNPCKWD performs "Unpack and Interleave 16-bit Masks".
//
// Mnemonic        : KUNPCKWD
// ISA extensions  : AVX512BW
// Supported forms : (1 form)
//
//    * KUNPCKWD k, k, k
//
func (self *Program) KUNPCKWD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KUNPCKWD k, k, k
    if isK(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, 0, nil, hlcode(v[1]))
            m.emit(0x4b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KUNPCKWD")
    }
    return p
}

// KXNORB performs "Bitwise Logical XNOR 8-bit Masks".
//
// Mnemonic        : KXNORB
// ISA extensions  : AVX512DQ
// Supported forms : (1 form)
//
//    * KXNORB k, k, k
//
func (self *Program) KXNORB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KXNORB k, k, k
    if isK(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, 0, nil, hlcode(v[1]))
            m.emit(0x46)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KXNORB")
    }
    return p
}

// KXNORD performs "Bitwise Logical XNOR 32-bit Masks".
//
// Mnemonic        : KXNORD
// ISA extensions  : AVX512BW
// Supported forms : (1 form)
//
//    * KXNORD k, k, k
//
func (self *Program) KXNORD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KXNORD k, k, k
    if isK(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe1)
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit(0x46)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KXNORD")
    }
    return p
}

// KXNORQ performs "Bitwise Logical XNOR 64-bit Masks".
//
// Mnemonic        : KXNORQ
// ISA extensions  : AVX512BW
// Supported forms : (1 form)
//
//    * KXNORQ k, k, k
//
func (self *Program) KXNORQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KXNORQ k, k, k
    if isK(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe1)
            m.emit(0xfc ^ (hlcode(v[1]) << 3))
            m.emit(0x46)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KXNORQ")
    }
    return p
}

// KXNORW performs "Bitwise Logical XNOR 16-bit Masks".
//
// Mnemonic        : KXNORW
// ISA extensions  : AVX512F
// Supported forms : (1 form)
//
//    * KXNORW k, k, k
//
func (self *Program) KXNORW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KXNORW k, k, k
    if isK(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, 0, nil, hlcode(v[1]))
            m.emit(0x46)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KXNORW")
    }
    return p
}

// KXORB performs "Bitwise Logical XOR 8-bit Masks".
//
// Mnemonic        : KXORB
// ISA extensions  : AVX512DQ
// Supported forms : (1 form)
//
//    * KXORB k, k, k
//
func (self *Program) KXORB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KXORB k, k, k
    if isK(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, 0, nil, hlcode(v[1]))
            m.emit(0x47)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KXORB")
    }
    return p
}

// KXORD performs "Bitwise Logical XOR 32-bit Masks".
//
// Mnemonic        : KXORD
// ISA extensions  : AVX512BW
// Supported forms : (1 form)
//
//    * KXORD k, k, k
//
func (self *Program) KXORD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KXORD k, k, k
    if isK(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe1)
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit(0x47)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KXORD")
    }
    return p
}

// KXORQ performs "Bitwise Logical XOR 64-bit Masks".
//
// Mnemonic        : KXORQ
// ISA extensions  : AVX512BW
// Supported forms : (1 form)
//
//    * KXORQ k, k, k
//
func (self *Program) KXORQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KXORQ k, k, k
    if isK(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe1)
            m.emit(0xfc ^ (hlcode(v[1]) << 3))
            m.emit(0x47)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KXORQ")
    }
    return p
}

// KXORW performs "Bitwise Logical XOR 16-bit Masks".
//
// Mnemonic        : KXORW
// ISA extensions  : AVX512F
// Supported forms : (1 form)
//
//    * KXORW k, k, k
//
func (self *Program) KXORW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // KXORW k, k, k
    if isK(v0) && isK(v1) && isK(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, 0, nil, hlcode(v[1]))
            m.emit(0x47)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for KXORW")
    }
    return p
}

// LDDQU performs "Load Unaligned Integer 128 Bits".
//
// Mnemonic        : LDDQU
// ISA extensions  : SSE3
// Supported forms : (1 form)
//
//    * LDDQU m128, xmm
//
func (self *Program) LDDQU(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // LDDQU m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xf0)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for LDDQU")
    }
    return p
}

// LDMXCSR performs "Load MXCSR Register".
//
// Mnemonic        : LDMXCSR
// ISA extensions  : SSE
// Supported forms : (1 form)
//
//    * LDMXCSR m32
//
func (self *Program) LDMXCSR(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // LDMXCSR m32
    if isM32(v0) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xae)
            m.mrsd(2, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for LDMXCSR")
    }
    return p
}

// LEAL performs "Load Effective Address".
//
// Mnemonic        : LEA
// Supported forms : (1 form)
//
//    * LEAL m, r32
//
func (self *Program) LEAL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // LEAL m, r32
    if isM(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x8d)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for LEAL")
    }
    return p
}

// LEAQ performs "Load Effective Address".
//
// Mnemonic        : LEA
// Supported forms : (1 form)
//
//    * LEAQ m, r64
//
func (self *Program) LEAQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // LEAQ m, r64
    if isM(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x8d)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for LEAQ")
    }
    return p
}

// LEAW performs "Load Effective Address".
//
// Mnemonic        : LEA
// Supported forms : (1 form)
//
//    * LEAW m, r16
//
func (self *Program) LEAW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // LEAW m, r16
    if isM(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x8d)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for LEAW")
    }
    return p
}

// LFENCE performs "Load Fence".
//
// Mnemonic        : LFENCE
// ISA extensions  : SSE2
// Supported forms : (1 form)
//
//    * LFENCE
//
func (self *Program) LFENCE() *Instruction {
    p := self.alloc(0, Operands{})
    // LFENCE
    self.require(ISA_SSE2)
    p.add(0, func(m *_Encoding, v []interface{}) {
        m.emit(0x0f)
        m.emit(0xae)
        m.emit(0xe8)
    })
    return p
}

// LZCNTL performs "Count the Number of Leading Zero Bits".
//
// Mnemonic        : LZCNT
// ISA extensions  : LZCNT
// Supported forms : (2 forms)
//
//    * LZCNTL r32, r32
//    * LZCNTL m32, r32
//
func (self *Program) LZCNTL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // LZCNTL r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_LZCNT)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xbd)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // LZCNTL m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_LZCNT)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xbd)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for LZCNTL")
    }
    return p
}

// LZCNTQ performs "Count the Number of Leading Zero Bits".
//
// Mnemonic        : LZCNT
// ISA extensions  : LZCNT
// Supported forms : (2 forms)
//
//    * LZCNTQ r64, r64
//    * LZCNTQ m64, r64
//
func (self *Program) LZCNTQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // LZCNTQ r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_LZCNT)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0xbd)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // LZCNTQ m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_LZCNT)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0xbd)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for LZCNTQ")
    }
    return p
}

// LZCNTW performs "Count the Number of Leading Zero Bits".
//
// Mnemonic        : LZCNT
// ISA extensions  : LZCNT
// Supported forms : (2 forms)
//
//    * LZCNTW r16, r16
//    * LZCNTW m16, r16
//
func (self *Program) LZCNTW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // LZCNTW r16, r16
    if isReg16(v0) && isReg16(v1) {
        self.require(ISA_LZCNT)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.emit(0xf3)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xbd)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // LZCNTW m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_LZCNT)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.emit(0xf3)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xbd)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for LZCNTW")
    }
    return p
}

// MASKMOVDQU performs "Store Selected Bytes of Double Quadword".
//
// Mnemonic        : MASKMOVDQU
// ISA extensions  : SSE2
// Supported forms : (1 form)
//
//    * MASKMOVDQU xmm, xmm
//
func (self *Program) MASKMOVDQU(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MASKMOVDQU xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xf7)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for MASKMOVDQU")
    }
    return p
}

// MASKMOVQ performs "Store Selected Bytes of Quadword".
//
// Mnemonic        : MASKMOVQ
// ISA extensions  : MMX+
// Supported forms : (1 form)
//
//    * MASKMOVQ mm, mm
//
func (self *Program) MASKMOVQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MASKMOVQ mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xf7)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for MASKMOVQ")
    }
    return p
}

// MAXPD performs "Return Maximum Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : MAXPD
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * MAXPD xmm, xmm
//    * MAXPD m128, xmm
//
func (self *Program) MAXPD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MAXPD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x5f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MAXPD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x5f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MAXPD")
    }
    return p
}

// MAXPS performs "Return Maximum Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : MAXPS
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * MAXPS xmm, xmm
//    * MAXPS m128, xmm
//
func (self *Program) MAXPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MAXPS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x5f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MAXPS m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x5f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MAXPS")
    }
    return p
}

// MAXSD performs "Return Maximum Scalar Double-Precision Floating-Point Value".
//
// Mnemonic        : MAXSD
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * MAXSD xmm, xmm
//    * MAXSD m64, xmm
//
func (self *Program) MAXSD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MAXSD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x5f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MAXSD m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x5f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MAXSD")
    }
    return p
}

// MAXSS performs "Return Maximum Scalar Single-Precision Floating-Point Value".
//
// Mnemonic        : MAXSS
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * MAXSS xmm, xmm
//    * MAXSS m32, xmm
//
func (self *Program) MAXSS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MAXSS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x5f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MAXSS m32, xmm
    if isM32(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x5f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MAXSS")
    }
    return p
}

// MFENCE performs "Memory Fence".
//
// Mnemonic        : MFENCE
// ISA extensions  : SSE2
// Supported forms : (1 form)
//
//    * MFENCE
//
func (self *Program) MFENCE() *Instruction {
    p := self.alloc(0, Operands{})
    // MFENCE
    self.require(ISA_SSE2)
    p.add(0, func(m *_Encoding, v []interface{}) {
        m.emit(0x0f)
        m.emit(0xae)
        m.emit(0xf0)
    })
    return p
}

// MINPD performs "Return Minimum Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : MINPD
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * MINPD xmm, xmm
//    * MINPD m128, xmm
//
func (self *Program) MINPD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MINPD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x5d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MINPD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x5d)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MINPD")
    }
    return p
}

// MINPS performs "Return Minimum Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : MINPS
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * MINPS xmm, xmm
//    * MINPS m128, xmm
//
func (self *Program) MINPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MINPS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x5d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MINPS m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x5d)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MINPS")
    }
    return p
}

// MINSD performs "Return Minimum Scalar Double-Precision Floating-Point Value".
//
// Mnemonic        : MINSD
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * MINSD xmm, xmm
//    * MINSD m64, xmm
//
func (self *Program) MINSD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MINSD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x5d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MINSD m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x5d)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MINSD")
    }
    return p
}

// MINSS performs "Return Minimum Scalar Single-Precision Floating-Point Value".
//
// Mnemonic        : MINSS
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * MINSS xmm, xmm
//    * MINSS m32, xmm
//
func (self *Program) MINSS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MINSS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x5d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MINSS m32, xmm
    if isM32(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x5d)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MINSS")
    }
    return p
}

// MONITOR performs "Monitor a Linear Address Range".
//
// Mnemonic        : MONITOR
// ISA extensions  : MONITOR
// Supported forms : (1 form)
//
//    * MONITOR
//
func (self *Program) MONITOR() *Instruction {
    p := self.alloc(0, Operands{})
    // MONITOR
    self.require(ISA_MONITOR)
    p.add(0, func(m *_Encoding, v []interface{}) {
        m.emit(0x0f)
        m.emit(0x01)
        m.emit(0xc8)
    })
    return p
}

// MONITORX performs "Monitor a Linear Address Range with Timeout".
//
// Mnemonic        : MONITORX
// ISA extensions  : MONITORX
// Supported forms : (1 form)
//
//    * MONITORX
//
func (self *Program) MONITORX() *Instruction {
    p := self.alloc(0, Operands{})
    // MONITORX
    self.require(ISA_MONITORX)
    p.add(0, func(m *_Encoding, v []interface{}) {
        m.emit(0x0f)
        m.emit(0x01)
        m.emit(0xfa)
    })
    return p
}

// MOVAPD performs "Move Aligned Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : MOVAPD
// ISA extensions  : SSE2
// Supported forms : (3 forms)
//
//    * MOVAPD xmm, xmm
//    * MOVAPD m128, xmm
//    * MOVAPD xmm, m128
//
func (self *Program) MOVAPD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVAPD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x28)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x0f)
            m.emit(0x29)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // MOVAPD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x28)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // MOVAPD xmm, m128
    if isXMM(v0) && isM128(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x29)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVAPD")
    }
    return p
}

// MOVAPS performs "Move Aligned Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : MOVAPS
// ISA extensions  : SSE
// Supported forms : (3 forms)
//
//    * MOVAPS xmm, xmm
//    * MOVAPS m128, xmm
//    * MOVAPS xmm, m128
//
func (self *Program) MOVAPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVAPS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x28)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x0f)
            m.emit(0x29)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // MOVAPS m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x28)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // MOVAPS xmm, m128
    if isXMM(v0) && isM128(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x29)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVAPS")
    }
    return p
}

// MOVB performs "Move".
//
// Mnemonic        : MOV
// Supported forms : (5 forms)
//
//    * MOVB imm8, r8
//    * MOVB r8, r8
//    * MOVB m8, r8
//    * MOVB imm8, m8
//    * MOVB r8, m8
//
func (self *Program) MOVB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVB imm8, r8
    if isImm8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0xc6)
            m.emit(0xc0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0xb0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // MOVB r8, r8
    if isReg8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], isReg8REX(v[0]) || isReg8REX(v[1]))
            m.emit(0x88)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], isReg8REX(v[0]) || isReg8REX(v[1]))
            m.emit(0x8a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MOVB m8, r8
    if isM8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), isReg8REX(v[1]))
            m.emit(0x8a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // MOVB imm8, m8
    if isImm8(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xc6)
            m.mrsd(0, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // MOVB r8, m8
    if isReg8(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), isReg8REX(v[0]))
            m.emit(0x88)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVB")
    }
    return p
}

// MOVBEL performs "Move Data After Swapping Bytes".
//
// Mnemonic        : MOVBE
// ISA extensions  : MOVBE
// Supported forms : (2 forms)
//
//    * MOVBEL m32, r32
//    * MOVBEL r32, m32
//
func (self *Program) MOVBEL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVBEL m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_MOVBE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xf0)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // MOVBEL r32, m32
    if isReg32(v0) && isM32(v1) {
        self.require(ISA_MOVBE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xf1)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVBEL")
    }
    return p
}

// MOVBEQ performs "Move Data After Swapping Bytes".
//
// Mnemonic        : MOVBE
// ISA extensions  : MOVBE
// Supported forms : (2 forms)
//
//    * MOVBEQ m64, r64
//    * MOVBEQ r64, m64
//
func (self *Program) MOVBEQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVBEQ m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_MOVBE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xf0)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // MOVBEQ r64, m64
    if isReg64(v0) && isM64(v1) {
        self.require(ISA_MOVBE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[0]), addr(v[1]))
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xf1)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVBEQ")
    }
    return p
}

// MOVBEW performs "Move Data After Swapping Bytes".
//
// Mnemonic        : MOVBE
// ISA extensions  : MOVBE
// Supported forms : (2 forms)
//
//    * MOVBEW m16, r16
//    * MOVBEW r16, m16
//
func (self *Program) MOVBEW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVBEW m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_MOVBE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xf0)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // MOVBEW r16, m16
    if isReg16(v0) && isM16(v1) {
        self.require(ISA_MOVBE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xf1)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVBEW")
    }
    return p
}

// MOVD performs "Move Doubleword".
//
// Mnemonic        : MOVD
// ISA extensions  : MMX, SSE2
// Supported forms : (8 forms)
//
//    * MOVD mm, r32
//    * MOVD xmm, r32
//    * MOVD r32, mm
//    * MOVD m32, mm
//    * MOVD r32, xmm
//    * MOVD m32, xmm
//    * MOVD mm, m32
//    * MOVD xmm, m32
//
func (self *Program) MOVD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVD mm, r32
    if isMM(v0) && isReg32(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x0f)
            m.emit(0x7e)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // MOVD xmm, r32
    if isXMM(v0) && isReg32(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x0f)
            m.emit(0x7e)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // MOVD r32, mm
    if isReg32(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x6e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MOVD m32, mm
    if isM32(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x6e)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // MOVD r32, xmm
    if isReg32(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x6e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MOVD m32, xmm
    if isM32(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x6e)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // MOVD mm, m32
    if isMM(v0) && isM32(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x7e)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    // MOVD xmm, m32
    if isXMM(v0) && isM32(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x7e)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVD")
    }
    return p
}

// MOVDDUP performs "Move One Double-FP and Duplicate".
//
// Mnemonic        : MOVDDUP
// ISA extensions  : SSE3
// Supported forms : (2 forms)
//
//    * MOVDDUP xmm, xmm
//    * MOVDDUP m64, xmm
//
func (self *Program) MOVDDUP(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVDDUP xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x12)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MOVDDUP m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_SSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x12)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVDDUP")
    }
    return p
}

// MOVDQ2Q performs "Move Quadword from XMM to MMX Technology Register".
//
// Mnemonic        : MOVDQ2Q
// ISA extensions  : SSE2
// Supported forms : (1 form)
//
//    * MOVDQ2Q xmm, mm
//
func (self *Program) MOVDQ2Q(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVDQ2Q xmm, mm
    if isXMM(v0) && isMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xd6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVDQ2Q")
    }
    return p
}

// MOVDQA performs "Move Aligned Double Quadword".
//
// Mnemonic        : MOVDQA
// ISA extensions  : SSE2
// Supported forms : (3 forms)
//
//    * MOVDQA xmm, xmm
//    * MOVDQA m128, xmm
//    * MOVDQA xmm, m128
//
func (self *Program) MOVDQA(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVDQA xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x6f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x0f)
            m.emit(0x7f)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // MOVDQA m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x6f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // MOVDQA xmm, m128
    if isXMM(v0) && isM128(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x7f)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVDQA")
    }
    return p
}

// MOVDQU performs "Move Unaligned Double Quadword".
//
// Mnemonic        : MOVDQU
// ISA extensions  : SSE2
// Supported forms : (3 forms)
//
//    * MOVDQU xmm, xmm
//    * MOVDQU m128, xmm
//    * MOVDQU xmm, m128
//
func (self *Program) MOVDQU(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVDQU xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x6f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x0f)
            m.emit(0x7f)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // MOVDQU m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x6f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // MOVDQU xmm, m128
    if isXMM(v0) && isM128(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x7f)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVDQU")
    }
    return p
}

// MOVHLPS performs "Move Packed Single-Precision Floating-Point Values High to Low".
//
// Mnemonic        : MOVHLPS
// ISA extensions  : SSE
// Supported forms : (1 form)
//
//    * MOVHLPS xmm, xmm
//
func (self *Program) MOVHLPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVHLPS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x12)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVHLPS")
    }
    return p
}

// MOVHPD performs "Move High Packed Double-Precision Floating-Point Value".
//
// Mnemonic        : MOVHPD
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * MOVHPD m64, xmm
//    * MOVHPD xmm, m64
//
func (self *Program) MOVHPD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVHPD m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x16)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // MOVHPD xmm, m64
    if isXMM(v0) && isM64(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x17)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVHPD")
    }
    return p
}

// MOVHPS performs "Move High Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : MOVHPS
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * MOVHPS m64, xmm
//    * MOVHPS xmm, m64
//
func (self *Program) MOVHPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVHPS m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x16)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // MOVHPS xmm, m64
    if isXMM(v0) && isM64(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x17)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVHPS")
    }
    return p
}

// MOVL performs "Move".
//
// Mnemonic        : MOV
// Supported forms : (5 forms)
//
//    * MOVL imm32, r32
//    * MOVL r32, r32
//    * MOVL m32, r32
//    * MOVL imm32, m32
//    * MOVL r32, m32
//
func (self *Program) MOVL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVL imm32, r32
    if isImm32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0xc7)
            m.emit(0xc0 | lcode(v[1]))
            m.imm4(toImmAny(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0xb8 | lcode(v[1]))
            m.imm4(toImmAny(v[0]))
        })
    }
    // MOVL r32, r32
    if isReg32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x89)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x8b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MOVL m32, r32
    if isM32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x8b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // MOVL imm32, m32
    if isImm32(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xc7)
            m.mrsd(0, addr(v[1]), 1)
            m.imm4(toImmAny(v[0]))
        })
    }
    // MOVL r32, m32
    if isReg32(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x89)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVL")
    }
    return p
}

// MOVLHPS performs "Move Packed Single-Precision Floating-Point Values Low to High".
//
// Mnemonic        : MOVLHPS
// ISA extensions  : SSE
// Supported forms : (1 form)
//
//    * MOVLHPS xmm, xmm
//
func (self *Program) MOVLHPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVLHPS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x16)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVLHPS")
    }
    return p
}

// MOVLPD performs "Move Low Packed Double-Precision Floating-Point Value".
//
// Mnemonic        : MOVLPD
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * MOVLPD m64, xmm
//    * MOVLPD xmm, m64
//
func (self *Program) MOVLPD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVLPD m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x12)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // MOVLPD xmm, m64
    if isXMM(v0) && isM64(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x13)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVLPD")
    }
    return p
}

// MOVLPS performs "Move Low Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : MOVLPS
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * MOVLPS m64, xmm
//    * MOVLPS xmm, m64
//
func (self *Program) MOVLPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVLPS m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x12)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // MOVLPS xmm, m64
    if isXMM(v0) && isM64(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x13)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVLPS")
    }
    return p
}

// MOVMSKPD performs "Extract Packed Double-Precision Floating-Point Sign Mask".
//
// Mnemonic        : MOVMSKPD
// ISA extensions  : SSE2
// Supported forms : (1 form)
//
//    * MOVMSKPD xmm, r32
//
func (self *Program) MOVMSKPD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVMSKPD xmm, r32
    if isXMM(v0) && isReg32(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x50)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVMSKPD")
    }
    return p
}

// MOVMSKPS performs "Extract Packed Single-Precision Floating-Point Sign Mask".
//
// Mnemonic        : MOVMSKPS
// ISA extensions  : SSE
// Supported forms : (1 form)
//
//    * MOVMSKPS xmm, r32
//
func (self *Program) MOVMSKPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVMSKPS xmm, r32
    if isXMM(v0) && isReg32(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x50)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVMSKPS")
    }
    return p
}

// MOVNTDQ performs "Store Double Quadword Using Non-Temporal Hint".
//
// Mnemonic        : MOVNTDQ
// ISA extensions  : SSE2
// Supported forms : (1 form)
//
//    * MOVNTDQ xmm, m128
//
func (self *Program) MOVNTDQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVNTDQ xmm, m128
    if isXMM(v0) && isM128(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0xe7)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVNTDQ")
    }
    return p
}

// MOVNTDQA performs "Load Double Quadword Non-Temporal Aligned Hint".
//
// Mnemonic        : MOVNTDQA
// ISA extensions  : SSE4.1
// Supported forms : (1 form)
//
//    * MOVNTDQA m128, xmm
//
func (self *Program) MOVNTDQA(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVNTDQA m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x2a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVNTDQA")
    }
    return p
}

// MOVNTIL performs "Store Doubleword Using Non-Temporal Hint".
//
// Mnemonic        : MOVNTI
// ISA extensions  : SSE2
// Supported forms : (1 form)
//
//    * MOVNTIL r32, m32
//
func (self *Program) MOVNTIL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVNTIL r32, m32
    if isReg32(v0) && isM32(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0xc3)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVNTIL")
    }
    return p
}

// MOVNTIQ performs "Store Doubleword Using Non-Temporal Hint".
//
// Mnemonic        : MOVNTI
// ISA extensions  : SSE2
// Supported forms : (1 form)
//
//    * MOVNTIQ r64, m64
//
func (self *Program) MOVNTIQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVNTIQ r64, m64
    if isReg64(v0) && isM64(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[0]), addr(v[1]))
            m.emit(0x0f)
            m.emit(0xc3)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVNTIQ")
    }
    return p
}

// MOVNTPD performs "Store Packed Double-Precision Floating-Point Values Using Non-Temporal Hint".
//
// Mnemonic        : MOVNTPD
// ISA extensions  : SSE2
// Supported forms : (1 form)
//
//    * MOVNTPD xmm, m128
//
func (self *Program) MOVNTPD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVNTPD xmm, m128
    if isXMM(v0) && isM128(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x2b)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVNTPD")
    }
    return p
}

// MOVNTPS performs "Store Packed Single-Precision Floating-Point Values Using Non-Temporal Hint".
//
// Mnemonic        : MOVNTPS
// ISA extensions  : SSE
// Supported forms : (1 form)
//
//    * MOVNTPS xmm, m128
//
func (self *Program) MOVNTPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVNTPS xmm, m128
    if isXMM(v0) && isM128(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x2b)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVNTPS")
    }
    return p
}

// MOVNTQ performs "Store of Quadword Using Non-Temporal Hint".
//
// Mnemonic        : MOVNTQ
// ISA extensions  : MMX+
// Supported forms : (1 form)
//
//    * MOVNTQ mm, m64
//
func (self *Program) MOVNTQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVNTQ mm, m64
    if isMM(v0) && isM64(v1) {
        self.require(ISA_MMX_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0xe7)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVNTQ")
    }
    return p
}

// MOVNTSD performs "Store Scalar Double-Precision Floating-Point Values Using Non-Temporal Hint".
//
// Mnemonic        : MOVNTSD
// ISA extensions  : SSE4A
// Supported forms : (1 form)
//
//    * MOVNTSD xmm, m64
//
func (self *Program) MOVNTSD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVNTSD xmm, m64
    if isXMM(v0) && isM64(v1) {
        self.require(ISA_SSE4A)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x2b)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVNTSD")
    }
    return p
}

// MOVNTSS performs "Store Scalar Single-Precision Floating-Point Values Using Non-Temporal Hint".
//
// Mnemonic        : MOVNTSS
// ISA extensions  : SSE4A
// Supported forms : (1 form)
//
//    * MOVNTSS xmm, m32
//
func (self *Program) MOVNTSS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVNTSS xmm, m32
    if isXMM(v0) && isM32(v1) {
        self.require(ISA_SSE4A)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x2b)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVNTSS")
    }
    return p
}

// MOVQ performs "Move".
//
// Mnemonic        : MOV
// ISA extensions  : MMX, SSE2
// Supported forms : (16 forms)
//
//    * MOVQ imm32, r64
//    * MOVQ imm64, r64
//    * MOVQ r64, r64
//    * MOVQ m64, r64
//    * MOVQ imm32, m64
//    * MOVQ r64, m64
//    * MOVQ mm, r64
//    * MOVQ xmm, r64
//    * MOVQ r64, mm
//    * MOVQ mm, mm
//    * MOVQ m64, mm
//    * MOVQ r64, xmm
//    * MOVQ xmm, xmm
//    * MOVQ m64, xmm
//    * MOVQ mm, m64
//    * MOVQ xmm, m64
//
func (self *Program) MOVQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVQ imm32, r64
    if isImm32Ext(v0, 8) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0xc7)
            m.emit(0xc0 | lcode(v[1]))
            m.imm4(toImmAny(v[0]))
        })
    }
    // MOVQ imm64, r64
    if isImm64(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0xb8 | lcode(v[1]))
            m.imm8(toImmAny(v[0]))
        })
    }
    // MOVQ r64, r64
    if isReg64(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[0]) << 2 | hcode(v[1]))
            m.emit(0x89)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x8b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MOVQ m64, r64
    if isM64(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x8b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // MOVQ imm32, m64
    if isImm32Ext(v0, 8) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0xc7)
            m.mrsd(0, addr(v[1]), 1)
            m.imm4(toImmAny(v[0]))
        })
    }
    // MOVQ r64, m64
    if isReg64(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[0]), addr(v[1]))
            m.emit(0x89)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    // MOVQ mm, r64
    if isMM(v0) && isReg64(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[0]) << 2 | hcode(v[1]))
            m.emit(0x0f)
            m.emit(0x7e)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // MOVQ xmm, r64
    if isXMM(v0) && isReg64(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.emit(0x48 | hcode(v[0]) << 2 | hcode(v[1]))
            m.emit(0x0f)
            m.emit(0x7e)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // MOVQ r64, mm
    if isReg64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x6e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MOVQ mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x6f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x0f)
            m.emit(0x7f)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // MOVQ m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x6f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x6e)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // MOVQ r64, xmm
    if isReg64(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0x6e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MOVQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x7e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x0f)
            m.emit(0xd6)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // MOVQ m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x7e)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0x6e)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // MOVQ mm, m64
    if isMM(v0) && isM64(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x7f)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[0]), addr(v[1]))
            m.emit(0x0f)
            m.emit(0x7e)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    // MOVQ xmm, m64
    if isXMM(v0) && isM64(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0xd6)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexm(1, hcode(v[0]), addr(v[1]))
            m.emit(0x0f)
            m.emit(0x7e)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVQ")
    }
    return p
}

// MOVQ2DQ performs "Move Quadword from MMX Technology to XMM Register".
//
// Mnemonic        : MOVQ2DQ
// ISA extensions  : SSE2
// Supported forms : (1 form)
//
//    * MOVQ2DQ mm, xmm
//
func (self *Program) MOVQ2DQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVQ2DQ mm, xmm
    if isMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xd6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVQ2DQ")
    }
    return p
}

// MOVSBL performs "Move with Sign-Extension".
//
// Mnemonic        : MOVSX
// Supported forms : (2 forms)
//
//    * MOVSBL r8, r32
//    * MOVSBL m8, r32
//
func (self *Program) MOVSBL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVSBL r8, r32
    if isReg8(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0xbe)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MOVSBL m8, r32
    if isM8(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xbe)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVSBL")
    }
    return p
}

// MOVSBQ performs "Move with Sign-Extension".
//
// Mnemonic        : MOVSX
// Supported forms : (2 forms)
//
//    * MOVSBQ r8, r64
//    * MOVSBQ m8, r64
//
func (self *Program) MOVSBQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVSBQ r8, r64
    if isReg8(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0xbe)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MOVSBQ m8, r64
    if isM8(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0xbe)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVSBQ")
    }
    return p
}

// MOVSBW performs "Move with Sign-Extension".
//
// Mnemonic        : MOVSX
// Supported forms : (2 forms)
//
//    * MOVSBW r8, r16
//    * MOVSBW m8, r16
//
func (self *Program) MOVSBW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVSBW r8, r16
    if isReg8(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0xbe)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MOVSBW m8, r16
    if isM8(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xbe)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVSBW")
    }
    return p
}

// MOVSD performs "Move Scalar Double-Precision Floating-Point Value".
//
// Mnemonic        : MOVSD
// ISA extensions  : SSE2
// Supported forms : (3 forms)
//
//    * MOVSD xmm, xmm
//    * MOVSD m64, xmm
//    * MOVSD xmm, m64
//
func (self *Program) MOVSD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVSD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x10)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x0f)
            m.emit(0x11)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // MOVSD m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x10)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // MOVSD xmm, m64
    if isXMM(v0) && isM64(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x11)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVSD")
    }
    return p
}

// MOVSHDUP performs "Move Packed Single-FP High and Duplicate".
//
// Mnemonic        : MOVSHDUP
// ISA extensions  : SSE3
// Supported forms : (2 forms)
//
//    * MOVSHDUP xmm, xmm
//    * MOVSHDUP m128, xmm
//
func (self *Program) MOVSHDUP(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVSHDUP xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x16)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MOVSHDUP m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x16)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVSHDUP")
    }
    return p
}

// MOVSLDUP performs "Move Packed Single-FP Low and Duplicate".
//
// Mnemonic        : MOVSLDUP
// ISA extensions  : SSE3
// Supported forms : (2 forms)
//
//    * MOVSLDUP xmm, xmm
//    * MOVSLDUP m128, xmm
//
func (self *Program) MOVSLDUP(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVSLDUP xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x12)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MOVSLDUP m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x12)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVSLDUP")
    }
    return p
}

// MOVSLQ performs "Move Doubleword to Quadword with Sign-Extension".
//
// Mnemonic        : MOVSXD
// Supported forms : (2 forms)
//
//    * MOVSLQ r32, r64
//    * MOVSLQ m32, r64
//
func (self *Program) MOVSLQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVSLQ r32, r64
    if isReg32(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x63)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MOVSLQ m32, r64
    if isM32(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x63)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVSLQ")
    }
    return p
}

// MOVSS performs "Move Scalar Single-Precision Floating-Point Values".
//
// Mnemonic        : MOVSS
// ISA extensions  : SSE
// Supported forms : (3 forms)
//
//    * MOVSS xmm, xmm
//    * MOVSS m32, xmm
//    * MOVSS xmm, m32
//
func (self *Program) MOVSS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVSS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x10)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x0f)
            m.emit(0x11)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // MOVSS m32, xmm
    if isM32(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x10)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // MOVSS xmm, m32
    if isXMM(v0) && isM32(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x11)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVSS")
    }
    return p
}

// MOVSWL performs "Move with Sign-Extension".
//
// Mnemonic        : MOVSX
// Supported forms : (2 forms)
//
//    * MOVSWL r16, r32
//    * MOVSWL m16, r32
//
func (self *Program) MOVSWL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVSWL r16, r32
    if isReg16(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xbf)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MOVSWL m16, r32
    if isM16(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xbf)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVSWL")
    }
    return p
}

// MOVSWQ performs "Move with Sign-Extension".
//
// Mnemonic        : MOVSX
// Supported forms : (2 forms)
//
//    * MOVSWQ r16, r64
//    * MOVSWQ m16, r64
//
func (self *Program) MOVSWQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVSWQ r16, r64
    if isReg16(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0xbf)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MOVSWQ m16, r64
    if isM16(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0xbf)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVSWQ")
    }
    return p
}

// MOVUPD performs "Move Unaligned Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : MOVUPD
// ISA extensions  : SSE2
// Supported forms : (3 forms)
//
//    * MOVUPD xmm, xmm
//    * MOVUPD m128, xmm
//    * MOVUPD xmm, m128
//
func (self *Program) MOVUPD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVUPD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x10)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x0f)
            m.emit(0x11)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // MOVUPD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x10)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // MOVUPD xmm, m128
    if isXMM(v0) && isM128(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x11)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVUPD")
    }
    return p
}

// MOVUPS performs "Move Unaligned Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : MOVUPS
// ISA extensions  : SSE
// Supported forms : (3 forms)
//
//    * MOVUPS xmm, xmm
//    * MOVUPS m128, xmm
//    * MOVUPS xmm, m128
//
func (self *Program) MOVUPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVUPS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x10)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x0f)
            m.emit(0x11)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // MOVUPS m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x10)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // MOVUPS xmm, m128
    if isXMM(v0) && isM128(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x11)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVUPS")
    }
    return p
}

// MOVW performs "Move".
//
// Mnemonic        : MOV
// Supported forms : (5 forms)
//
//    * MOVW imm16, r16
//    * MOVW r16, r16
//    * MOVW m16, r16
//    * MOVW imm16, m16
//    * MOVW r16, m16
//
func (self *Program) MOVW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVW imm16, r16
    if isImm16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0xc7)
            m.emit(0xc0 | lcode(v[1]))
            m.imm2(toImmAny(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0xb8 | lcode(v[1]))
            m.imm2(toImmAny(v[0]))
        })
    }
    // MOVW r16, r16
    if isReg16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x89)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x8b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MOVW m16, r16
    if isM16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x8b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // MOVW imm16, m16
    if isImm16(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0xc7)
            m.mrsd(0, addr(v[1]), 1)
            m.imm2(toImmAny(v[0]))
        })
    }
    // MOVW r16, m16
    if isReg16(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x89)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVW")
    }
    return p
}

// MOVZBL performs "Move with Zero-Extend".
//
// Mnemonic        : MOVZX
// Supported forms : (2 forms)
//
//    * MOVZBL r8, r32
//    * MOVZBL m8, r32
//
func (self *Program) MOVZBL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVZBL r8, r32
    if isReg8(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0xb6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MOVZBL m8, r32
    if isM8(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xb6)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVZBL")
    }
    return p
}

// MOVZBQ performs "Move with Zero-Extend".
//
// Mnemonic        : MOVZX
// Supported forms : (2 forms)
//
//    * MOVZBQ r8, r64
//    * MOVZBQ m8, r64
//
func (self *Program) MOVZBQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVZBQ r8, r64
    if isReg8(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0xb6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MOVZBQ m8, r64
    if isM8(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0xb6)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVZBQ")
    }
    return p
}

// MOVZBW performs "Move with Zero-Extend".
//
// Mnemonic        : MOVZX
// Supported forms : (2 forms)
//
//    * MOVZBW r8, r16
//    * MOVZBW m8, r16
//
func (self *Program) MOVZBW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVZBW r8, r16
    if isReg8(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0xb6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MOVZBW m8, r16
    if isM8(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xb6)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVZBW")
    }
    return p
}

// MOVZWL performs "Move with Zero-Extend".
//
// Mnemonic        : MOVZX
// Supported forms : (2 forms)
//
//    * MOVZWL r16, r32
//    * MOVZWL m16, r32
//
func (self *Program) MOVZWL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVZWL r16, r32
    if isReg16(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xb7)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MOVZWL m16, r32
    if isM16(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xb7)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVZWL")
    }
    return p
}

// MOVZWQ performs "Move with Zero-Extend".
//
// Mnemonic        : MOVZX
// Supported forms : (2 forms)
//
//    * MOVZWQ r16, r64
//    * MOVZWQ m16, r64
//
func (self *Program) MOVZWQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MOVZWQ r16, r64
    if isReg16(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0xb7)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MOVZWQ m16, r64
    if isM16(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0xb7)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MOVZWQ")
    }
    return p
}

// MPSADBW performs "Compute Multiple Packed Sums of Absolute Difference".
//
// Mnemonic        : MPSADBW
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * MPSADBW imm8, xmm, xmm
//    * MPSADBW imm8, m128, xmm
//
func (self *Program) MPSADBW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // MPSADBW imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x42)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // MPSADBW imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x42)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for MPSADBW")
    }
    return p
}

// MULB performs "Unsigned Multiply".
//
// Mnemonic        : MUL
// Supported forms : (2 forms)
//
//    * MULB r8
//    * MULB m8
//
func (self *Program) MULB(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // MULB r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0xf6)
            m.emit(0xe0 | lcode(v[0]))
        })
    }
    // MULB m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0xf6)
            m.mrsd(4, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MULB")
    }
    return p
}

// MULL performs "Unsigned Multiply".
//
// Mnemonic        : MUL
// Supported forms : (2 forms)
//
//    * MULL r32
//    * MULL m32
//
func (self *Program) MULL(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // MULL r32
    if isReg32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], false)
            m.emit(0xf7)
            m.emit(0xe0 | lcode(v[0]))
        })
    }
    // MULL m32
    if isM32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0xf7)
            m.mrsd(4, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MULL")
    }
    return p
}

// MULPD performs "Multiply Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : MULPD
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * MULPD xmm, xmm
//    * MULPD m128, xmm
//
func (self *Program) MULPD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MULPD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x59)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MULPD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x59)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MULPD")
    }
    return p
}

// MULPS performs "Multiply Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : MULPS
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * MULPS xmm, xmm
//    * MULPS m128, xmm
//
func (self *Program) MULPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MULPS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x59)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MULPS m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x59)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MULPS")
    }
    return p
}

// MULQ performs "Unsigned Multiply".
//
// Mnemonic        : MUL
// Supported forms : (2 forms)
//
//    * MULQ r64
//    * MULQ m64
//
func (self *Program) MULQ(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // MULQ r64
    if isReg64(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[0]))
            m.emit(0xf7)
            m.emit(0xe0 | lcode(v[0]))
        })
    }
    // MULQ m64
    if isM64(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[0]))
            m.emit(0xf7)
            m.mrsd(4, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MULQ")
    }
    return p
}

// MULSD performs "Multiply Scalar Double-Precision Floating-Point Values".
//
// Mnemonic        : MULSD
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * MULSD xmm, xmm
//    * MULSD m64, xmm
//
func (self *Program) MULSD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MULSD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x59)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MULSD m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x59)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MULSD")
    }
    return p
}

// MULSS performs "Multiply Scalar Single-Precision Floating-Point Values".
//
// Mnemonic        : MULSS
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * MULSS xmm, xmm
//    * MULSS m32, xmm
//
func (self *Program) MULSS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // MULSS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x59)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // MULSS m32, xmm
    if isM32(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x59)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MULSS")
    }
    return p
}

// MULW performs "Unsigned Multiply".
//
// Mnemonic        : MUL
// Supported forms : (2 forms)
//
//    * MULW r16
//    * MULW m16
//
func (self *Program) MULW(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // MULW r16
    if isReg16(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[0], false)
            m.emit(0xf7)
            m.emit(0xe0 | lcode(v[0]))
        })
    }
    // MULW m16
    if isM16(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[0]), false)
            m.emit(0xf7)
            m.mrsd(4, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MULW")
    }
    return p
}

// MULXL performs "Unsigned Multiply Without Affecting Flags".
//
// Mnemonic        : MULX
// ISA extensions  : BMI2
// Supported forms : (2 forms)
//
//    * MULXL r32, r32, r32
//    * MULXL m32, r32, r32
//
func (self *Program) MULXL(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // MULXL r32, r32, r32
    if isReg32(v0) && isReg32(v1) && isReg32(v2) {
        self.require(ISA_BMI2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7b ^ (hlcode(v[1]) << 3))
            m.emit(0xf6)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // MULXL m32, r32, r32
    if isM32(v0) && isReg32(v1) && isReg32(v2) {
        self.require(ISA_BMI2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x03, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xf6)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MULXL")
    }
    return p
}

// MULXQ performs "Unsigned Multiply Without Affecting Flags".
//
// Mnemonic        : MULX
// ISA extensions  : BMI2
// Supported forms : (2 forms)
//
//    * MULXQ r64, r64, r64
//    * MULXQ m64, r64, r64
//
func (self *Program) MULXQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // MULXQ r64, r64, r64
    if isReg64(v0) && isReg64(v1) && isReg64(v2) {
        self.require(ISA_BMI2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfb ^ (hlcode(v[1]) << 3))
            m.emit(0xf6)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // MULXQ m64, r64, r64
    if isM64(v0) && isReg64(v1) && isReg64(v2) {
        self.require(ISA_BMI2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x83, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xf6)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for MULXQ")
    }
    return p
}

// MWAIT performs "Monitor Wait".
//
// Mnemonic        : MWAIT
// ISA extensions  : MONITOR
// Supported forms : (1 form)
//
//    * MWAIT
//
func (self *Program) MWAIT() *Instruction {
    p := self.alloc(0, Operands{})
    // MWAIT
    self.require(ISA_MONITOR)
    p.add(0, func(m *_Encoding, v []interface{}) {
        m.emit(0x0f)
        m.emit(0x01)
        m.emit(0xc9)
    })
    return p
}

// MWAITX performs "Monitor Wait with Timeout".
//
// Mnemonic        : MWAITX
// ISA extensions  : MONITORX
// Supported forms : (1 form)
//
//    * MWAITX
//
func (self *Program) MWAITX() *Instruction {
    p := self.alloc(0, Operands{})
    // MWAITX
    self.require(ISA_MONITORX)
    p.add(0, func(m *_Encoding, v []interface{}) {
        m.emit(0x0f)
        m.emit(0x01)
        m.emit(0xfb)
    })
    return p
}

// NEGB performs "Two's Complement Negation".
//
// Mnemonic        : NEG
// Supported forms : (2 forms)
//
//    * NEGB r8
//    * NEGB m8
//
func (self *Program) NEGB(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // NEGB r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0xf6)
            m.emit(0xd8 | lcode(v[0]))
        })
    }
    // NEGB m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0xf6)
            m.mrsd(3, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for NEGB")
    }
    return p
}

// NEGL performs "Two's Complement Negation".
//
// Mnemonic        : NEG
// Supported forms : (2 forms)
//
//    * NEGL r32
//    * NEGL m32
//
func (self *Program) NEGL(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // NEGL r32
    if isReg32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], false)
            m.emit(0xf7)
            m.emit(0xd8 | lcode(v[0]))
        })
    }
    // NEGL m32
    if isM32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0xf7)
            m.mrsd(3, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for NEGL")
    }
    return p
}

// NEGQ performs "Two's Complement Negation".
//
// Mnemonic        : NEG
// Supported forms : (2 forms)
//
//    * NEGQ r64
//    * NEGQ m64
//
func (self *Program) NEGQ(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // NEGQ r64
    if isReg64(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[0]))
            m.emit(0xf7)
            m.emit(0xd8 | lcode(v[0]))
        })
    }
    // NEGQ m64
    if isM64(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[0]))
            m.emit(0xf7)
            m.mrsd(3, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for NEGQ")
    }
    return p
}

// NEGW performs "Two's Complement Negation".
//
// Mnemonic        : NEG
// Supported forms : (2 forms)
//
//    * NEGW r16
//    * NEGW m16
//
func (self *Program) NEGW(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // NEGW r16
    if isReg16(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[0], false)
            m.emit(0xf7)
            m.emit(0xd8 | lcode(v[0]))
        })
    }
    // NEGW m16
    if isM16(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[0]), false)
            m.emit(0xf7)
            m.mrsd(3, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for NEGW")
    }
    return p
}

// NOP performs "No Operation".
//
// Mnemonic        : NOP
// Supported forms : (1 form)
//
//    * NOP
//
func (self *Program) NOP() *Instruction {
    p := self.alloc(0, Operands{})
    // NOP
    p.add(0, func(m *_Encoding, v []interface{}) {
        m.emit(0x90)
    })
    return p
}

// NOTB performs "One's Complement Negation".
//
// Mnemonic        : NOT
// Supported forms : (2 forms)
//
//    * NOTB r8
//    * NOTB m8
//
func (self *Program) NOTB(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // NOTB r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0xf6)
            m.emit(0xd0 | lcode(v[0]))
        })
    }
    // NOTB m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0xf6)
            m.mrsd(2, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for NOTB")
    }
    return p
}

// NOTL performs "One's Complement Negation".
//
// Mnemonic        : NOT
// Supported forms : (2 forms)
//
//    * NOTL r32
//    * NOTL m32
//
func (self *Program) NOTL(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // NOTL r32
    if isReg32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], false)
            m.emit(0xf7)
            m.emit(0xd0 | lcode(v[0]))
        })
    }
    // NOTL m32
    if isM32(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0xf7)
            m.mrsd(2, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for NOTL")
    }
    return p
}

// NOTQ performs "One's Complement Negation".
//
// Mnemonic        : NOT
// Supported forms : (2 forms)
//
//    * NOTQ r64
//    * NOTQ m64
//
func (self *Program) NOTQ(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // NOTQ r64
    if isReg64(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[0]))
            m.emit(0xf7)
            m.emit(0xd0 | lcode(v[0]))
        })
    }
    // NOTQ m64
    if isM64(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[0]))
            m.emit(0xf7)
            m.mrsd(2, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for NOTQ")
    }
    return p
}

// NOTW performs "One's Complement Negation".
//
// Mnemonic        : NOT
// Supported forms : (2 forms)
//
//    * NOTW r16
//    * NOTW m16
//
func (self *Program) NOTW(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // NOTW r16
    if isReg16(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[0], false)
            m.emit(0xf7)
            m.emit(0xd0 | lcode(v[0]))
        })
    }
    // NOTW m16
    if isM16(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[0]), false)
            m.emit(0xf7)
            m.mrsd(2, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for NOTW")
    }
    return p
}

// ORB performs "Logical Inclusive OR".
//
// Mnemonic        : OR
// Supported forms : (6 forms)
//
//    * ORB imm8, al
//    * ORB imm8, r8
//    * ORB r8, r8
//    * ORB m8, r8
//    * ORB imm8, m8
//    * ORB r8, m8
//
func (self *Program) ORB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ORB imm8, al
    if isImm8(v0) && v1 == AL {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x0c)
            m.imm1(toImmAny(v[0]))
        })
    }
    // ORB imm8, r8
    if isImm8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0x80)
            m.emit(0xc8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // ORB r8, r8
    if isReg8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], isReg8REX(v[0]) || isReg8REX(v[1]))
            m.emit(0x08)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], isReg8REX(v[0]) || isReg8REX(v[1]))
            m.emit(0x0a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // ORB m8, r8
    if isM8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), isReg8REX(v[1]))
            m.emit(0x0a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // ORB imm8, m8
    if isImm8(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0x80)
            m.mrsd(1, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // ORB r8, m8
    if isReg8(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), isReg8REX(v[0]))
            m.emit(0x08)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ORB")
    }
    return p
}

// ORL performs "Logical Inclusive OR".
//
// Mnemonic        : OR
// Supported forms : (8 forms)
//
//    * ORL imm32, eax
//    * ORL imm8, r32
//    * ORL imm32, r32
//    * ORL r32, r32
//    * ORL m32, r32
//    * ORL imm8, m32
//    * ORL imm32, m32
//    * ORL r32, m32
//
func (self *Program) ORL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ORL imm32, eax
    if isImm32(v0) && v1 == EAX {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x0d)
            m.imm4(toImmAny(v[0]))
        })
    }
    // ORL imm8, r32
    if isImm8Ext(v0, 4) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0x83)
            m.emit(0xc8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // ORL imm32, r32
    if isImm32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0x81)
            m.emit(0xc8 | lcode(v[1]))
            m.imm4(toImmAny(v[0]))
        })
    }
    // ORL r32, r32
    if isReg32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x09)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // ORL m32, r32
    if isM32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // ORL imm8, m32
    if isImm8Ext(v0, 4) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0x83)
            m.mrsd(1, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // ORL imm32, m32
    if isImm32(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0x81)
            m.mrsd(1, addr(v[1]), 1)
            m.imm4(toImmAny(v[0]))
        })
    }
    // ORL r32, m32
    if isReg32(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x09)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ORL")
    }
    return p
}

// ORPD performs "Bitwise Logical OR of Double-Precision Floating-Point Values".
//
// Mnemonic        : ORPD
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * ORPD xmm, xmm
//    * ORPD m128, xmm
//
func (self *Program) ORPD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ORPD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x56)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // ORPD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x56)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ORPD")
    }
    return p
}

// ORPS performs "Bitwise Logical OR of Single-Precision Floating-Point Values".
//
// Mnemonic        : ORPS
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * ORPS xmm, xmm
//    * ORPS m128, xmm
//
func (self *Program) ORPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ORPS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x56)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // ORPS m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x56)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ORPS")
    }
    return p
}

// ORQ performs "Logical Inclusive OR".
//
// Mnemonic        : OR
// Supported forms : (8 forms)
//
//    * ORQ imm32, rax
//    * ORQ imm8, r64
//    * ORQ imm32, r64
//    * ORQ r64, r64
//    * ORQ m64, r64
//    * ORQ imm8, m64
//    * ORQ imm32, m64
//    * ORQ r64, m64
//
func (self *Program) ORQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ORQ imm32, rax
    if isImm32(v0) && v1 == RAX {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48)
            m.emit(0x0d)
            m.imm4(toImmAny(v[0]))
        })
    }
    // ORQ imm8, r64
    if isImm8Ext(v0, 8) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0x83)
            m.emit(0xc8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // ORQ imm32, r64
    if isImm32Ext(v0, 8) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0x81)
            m.emit(0xc8 | lcode(v[1]))
            m.imm4(toImmAny(v[0]))
        })
    }
    // ORQ r64, r64
    if isReg64(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[0]) << 2 | hcode(v[1]))
            m.emit(0x09)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // ORQ m64, r64
    if isM64(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // ORQ imm8, m64
    if isImm8Ext(v0, 8) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0x83)
            m.mrsd(1, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // ORQ imm32, m64
    if isImm32Ext(v0, 8) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0x81)
            m.mrsd(1, addr(v[1]), 1)
            m.imm4(toImmAny(v[0]))
        })
    }
    // ORQ r64, m64
    if isReg64(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[0]), addr(v[1]))
            m.emit(0x09)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ORQ")
    }
    return p
}

// ORW performs "Logical Inclusive OR".
//
// Mnemonic        : OR
// Supported forms : (8 forms)
//
//    * ORW imm16, ax
//    * ORW imm8, r16
//    * ORW imm16, r16
//    * ORW r16, r16
//    * ORW m16, r16
//    * ORW imm8, m16
//    * ORW imm16, m16
//    * ORW r16, m16
//
func (self *Program) ORW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ORW imm16, ax
    if isImm16(v0) && v1 == AX {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.emit(0x0d)
            m.imm2(toImmAny(v[0]))
        })
    }
    // ORW imm8, r16
    if isImm8Ext(v0, 2) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0x83)
            m.emit(0xc8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // ORW imm16, r16
    if isImm16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0x81)
            m.emit(0xc8 | lcode(v[1]))
            m.imm2(toImmAny(v[0]))
        })
    }
    // ORW r16, r16
    if isReg16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x09)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // ORW m16, r16
    if isM16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // ORW imm8, m16
    if isImm8Ext(v0, 2) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0x83)
            m.mrsd(1, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // ORW imm16, m16
    if isImm16(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0x81)
            m.mrsd(1, addr(v[1]), 1)
            m.imm2(toImmAny(v[0]))
        })
    }
    // ORW r16, m16
    if isReg16(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x09)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ORW")
    }
    return p
}

// PABSB performs "Packed Absolute Value of Byte Integers".
//
// Mnemonic        : PABSB
// ISA extensions  : SSSE3
// Supported forms : (4 forms)
//
//    * PABSB mm, mm
//    * PABSB m64, mm
//    * PABSB xmm, xmm
//    * PABSB m128, xmm
//
func (self *Program) PABSB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PABSB mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x1c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PABSB m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x1c)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PABSB xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x1c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PABSB m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x1c)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PABSB")
    }
    return p
}

// PABSD performs "Packed Absolute Value of Doubleword Integers".
//
// Mnemonic        : PABSD
// ISA extensions  : SSSE3
// Supported forms : (4 forms)
//
//    * PABSD mm, mm
//    * PABSD m64, mm
//    * PABSD xmm, xmm
//    * PABSD m128, xmm
//
func (self *Program) PABSD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PABSD mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x1e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PABSD m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x1e)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PABSD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x1e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PABSD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x1e)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PABSD")
    }
    return p
}

// PABSW performs "Packed Absolute Value of Word Integers".
//
// Mnemonic        : PABSW
// ISA extensions  : SSSE3
// Supported forms : (4 forms)
//
//    * PABSW mm, mm
//    * PABSW m64, mm
//    * PABSW xmm, xmm
//    * PABSW m128, xmm
//
func (self *Program) PABSW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PABSW mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x1d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PABSW m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x1d)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PABSW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x1d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PABSW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x1d)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PABSW")
    }
    return p
}

// PACKSSDW performs "Pack Doublewords into Words with Signed Saturation".
//
// Mnemonic        : PACKSSDW
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PACKSSDW mm, mm
//    * PACKSSDW m64, mm
//    * PACKSSDW xmm, xmm
//    * PACKSSDW m128, xmm
//
func (self *Program) PACKSSDW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PACKSSDW mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x6b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PACKSSDW m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x6b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PACKSSDW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x6b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PACKSSDW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x6b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PACKSSDW")
    }
    return p
}

// PACKSSWB performs "Pack Words into Bytes with Signed Saturation".
//
// Mnemonic        : PACKSSWB
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PACKSSWB mm, mm
//    * PACKSSWB m64, mm
//    * PACKSSWB xmm, xmm
//    * PACKSSWB m128, xmm
//
func (self *Program) PACKSSWB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PACKSSWB mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x63)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PACKSSWB m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x63)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PACKSSWB xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x63)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PACKSSWB m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x63)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PACKSSWB")
    }
    return p
}

// PACKUSDW performs "Pack Doublewords into Words with Unsigned Saturation".
//
// Mnemonic        : PACKUSDW
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PACKUSDW xmm, xmm
//    * PACKUSDW m128, xmm
//
func (self *Program) PACKUSDW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PACKUSDW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x2b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PACKUSDW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x2b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PACKUSDW")
    }
    return p
}

// PACKUSWB performs "Pack Words into Bytes with Unsigned Saturation".
//
// Mnemonic        : PACKUSWB
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PACKUSWB mm, mm
//    * PACKUSWB m64, mm
//    * PACKUSWB xmm, xmm
//    * PACKUSWB m128, xmm
//
func (self *Program) PACKUSWB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PACKUSWB mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x67)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PACKUSWB m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x67)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PACKUSWB xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x67)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PACKUSWB m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x67)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PACKUSWB")
    }
    return p
}

// PADDB performs "Add Packed Byte Integers".
//
// Mnemonic        : PADDB
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PADDB mm, mm
//    * PADDB m64, mm
//    * PADDB xmm, xmm
//    * PADDB m128, xmm
//
func (self *Program) PADDB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PADDB mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xfc)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PADDB m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xfc)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PADDB xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xfc)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PADDB m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xfc)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PADDB")
    }
    return p
}

// PADDD performs "Add Packed Doubleword Integers".
//
// Mnemonic        : PADDD
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PADDD mm, mm
//    * PADDD m64, mm
//    * PADDD xmm, xmm
//    * PADDD m128, xmm
//
func (self *Program) PADDD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PADDD mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xfe)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PADDD m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xfe)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PADDD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xfe)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PADDD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xfe)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PADDD")
    }
    return p
}

// PADDQ performs "Add Packed Quadword Integers".
//
// Mnemonic        : PADDQ
// ISA extensions  : SSE2
// Supported forms : (4 forms)
//
//    * PADDQ mm, mm
//    * PADDQ m64, mm
//    * PADDQ xmm, xmm
//    * PADDQ m128, xmm
//
func (self *Program) PADDQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PADDQ mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xd4)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PADDQ m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xd4)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PADDQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xd4)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PADDQ m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xd4)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PADDQ")
    }
    return p
}

// PADDSB performs "Add Packed Signed Byte Integers with Signed Saturation".
//
// Mnemonic        : PADDSB
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PADDSB mm, mm
//    * PADDSB m64, mm
//    * PADDSB xmm, xmm
//    * PADDSB m128, xmm
//
func (self *Program) PADDSB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PADDSB mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xec)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PADDSB m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xec)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PADDSB xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xec)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PADDSB m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xec)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PADDSB")
    }
    return p
}

// PADDSW performs "Add Packed Signed Word Integers with Signed Saturation".
//
// Mnemonic        : PADDSW
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PADDSW mm, mm
//    * PADDSW m64, mm
//    * PADDSW xmm, xmm
//    * PADDSW m128, xmm
//
func (self *Program) PADDSW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PADDSW mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xed)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PADDSW m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xed)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PADDSW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xed)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PADDSW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xed)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PADDSW")
    }
    return p
}

// PADDUSB performs "Add Packed Unsigned Byte Integers with Unsigned Saturation".
//
// Mnemonic        : PADDUSB
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PADDUSB mm, mm
//    * PADDUSB m64, mm
//    * PADDUSB xmm, xmm
//    * PADDUSB m128, xmm
//
func (self *Program) PADDUSB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PADDUSB mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xdc)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PADDUSB m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xdc)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PADDUSB xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xdc)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PADDUSB m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xdc)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PADDUSB")
    }
    return p
}

// PADDUSW performs "Add Packed Unsigned Word Integers with Unsigned Saturation".
//
// Mnemonic        : PADDUSW
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PADDUSW mm, mm
//    * PADDUSW m64, mm
//    * PADDUSW xmm, xmm
//    * PADDUSW m128, xmm
//
func (self *Program) PADDUSW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PADDUSW mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xdd)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PADDUSW m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xdd)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PADDUSW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xdd)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PADDUSW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xdd)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PADDUSW")
    }
    return p
}

// PADDW performs "Add Packed Word Integers".
//
// Mnemonic        : PADDW
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PADDW mm, mm
//    * PADDW m64, mm
//    * PADDW xmm, xmm
//    * PADDW m128, xmm
//
func (self *Program) PADDW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PADDW mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xfd)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PADDW m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xfd)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PADDW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xfd)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PADDW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xfd)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PADDW")
    }
    return p
}

// PALIGNR performs "Packed Align Right".
//
// Mnemonic        : PALIGNR
// ISA extensions  : SSSE3
// Supported forms : (4 forms)
//
//    * PALIGNR imm8, mm, mm
//    * PALIGNR imm8, m64, mm
//    * PALIGNR imm8, xmm, xmm
//    * PALIGNR imm8, m128, xmm
//
func (self *Program) PALIGNR(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // PALIGNR imm8, mm, mm
    if isImm8(v0) && isMM(v1) && isMM(v2) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x0f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PALIGNR imm8, m64, mm
    if isImm8(v0) && isM64(v1) && isMM(v2) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x0f)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // PALIGNR imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x0f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PALIGNR imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x0f)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for PALIGNR")
    }
    return p
}

// PAND performs "Packed Bitwise Logical AND".
//
// Mnemonic        : PAND
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PAND mm, mm
//    * PAND m64, mm
//    * PAND xmm, xmm
//    * PAND m128, xmm
//
func (self *Program) PAND(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PAND mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xdb)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PAND m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xdb)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PAND xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xdb)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PAND m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xdb)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PAND")
    }
    return p
}

// PANDN performs "Packed Bitwise Logical AND NOT".
//
// Mnemonic        : PANDN
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PANDN mm, mm
//    * PANDN m64, mm
//    * PANDN xmm, xmm
//    * PANDN m128, xmm
//
func (self *Program) PANDN(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PANDN mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xdf)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PANDN m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xdf)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PANDN xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xdf)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PANDN m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xdf)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PANDN")
    }
    return p
}

// PAUSE performs "Spin Loop Hint".
//
// Mnemonic        : PAUSE
// Supported forms : (1 form)
//
//    * PAUSE
//
func (self *Program) PAUSE() *Instruction {
    p := self.alloc(0, Operands{})
    // PAUSE
    p.add(0, func(m *_Encoding, v []interface{}) {
        m.emit(0xf3)
        m.emit(0x90)
    })
    return p
}

// PAVGB performs "Average Packed Byte Integers".
//
// Mnemonic        : PAVGB
// ISA extensions  : MMX+, SSE2
// Supported forms : (4 forms)
//
//    * PAVGB mm, mm
//    * PAVGB m64, mm
//    * PAVGB xmm, xmm
//    * PAVGB m128, xmm
//
func (self *Program) PAVGB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PAVGB mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xe0)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PAVGB m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xe0)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PAVGB xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xe0)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PAVGB m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xe0)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PAVGB")
    }
    return p
}

// PAVGUSB performs "Average Packed Byte Integers".
//
// Mnemonic        : PAVGUSB
// ISA extensions  : 3dnow!
// Supported forms : (2 forms)
//
//    * PAVGUSB mm, mm
//    * PAVGUSB m64, mm
//
func (self *Program) PAVGUSB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PAVGUSB mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
            m.emit(0xbf)
        })
    }
    // PAVGUSB m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
            m.emit(0xbf)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PAVGUSB")
    }
    return p
}

// PAVGW performs "Average Packed Word Integers".
//
// Mnemonic        : PAVGW
// ISA extensions  : MMX+, SSE2
// Supported forms : (4 forms)
//
//    * PAVGW mm, mm
//    * PAVGW m64, mm
//    * PAVGW xmm, xmm
//    * PAVGW m128, xmm
//
func (self *Program) PAVGW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PAVGW mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xe3)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PAVGW m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xe3)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PAVGW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xe3)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PAVGW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xe3)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PAVGW")
    }
    return p
}

// PBLENDVB performs "Variable Blend Packed Bytes".
//
// Mnemonic        : PBLENDVB
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PBLENDVB xmm0, xmm, xmm
//    * PBLENDVB xmm0, m128, xmm
//
func (self *Program) PBLENDVB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // PBLENDVB xmm0, xmm, xmm
    if v0 == XMM0 && isXMM(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x10)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // PBLENDVB xmm0, m128, xmm
    if v0 == XMM0 && isM128(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x10)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PBLENDVB")
    }
    return p
}

// PBLENDW performs "Blend Packed Words".
//
// Mnemonic        : PBLENDW
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PBLENDW imm8, xmm, xmm
//    * PBLENDW imm8, m128, xmm
//
func (self *Program) PBLENDW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // PBLENDW imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x0e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PBLENDW imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x0e)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for PBLENDW")
    }
    return p
}

// PCLMULQDQ performs "Carry-Less Quadword Multiplication".
//
// Mnemonic        : PCLMULQDQ
// ISA extensions  : PCLMULQDQ
// Supported forms : (2 forms)
//
//    * PCLMULQDQ imm8, xmm, xmm
//    * PCLMULQDQ imm8, m128, xmm
//
func (self *Program) PCLMULQDQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // PCLMULQDQ imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_PCLMULQDQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x44)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PCLMULQDQ imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_PCLMULQDQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x44)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for PCLMULQDQ")
    }
    return p
}

// PCMPEQB performs "Compare Packed Byte Data for Equality".
//
// Mnemonic        : PCMPEQB
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PCMPEQB mm, mm
//    * PCMPEQB m64, mm
//    * PCMPEQB xmm, xmm
//    * PCMPEQB m128, xmm
//
func (self *Program) PCMPEQB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PCMPEQB mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x74)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PCMPEQB m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x74)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PCMPEQB xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x74)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PCMPEQB m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x74)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PCMPEQB")
    }
    return p
}

// PCMPEQD performs "Compare Packed Doubleword Data for Equality".
//
// Mnemonic        : PCMPEQD
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PCMPEQD mm, mm
//    * PCMPEQD m64, mm
//    * PCMPEQD xmm, xmm
//    * PCMPEQD m128, xmm
//
func (self *Program) PCMPEQD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PCMPEQD mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x76)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PCMPEQD m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x76)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PCMPEQD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x76)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PCMPEQD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x76)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PCMPEQD")
    }
    return p
}

// PCMPEQQ performs "Compare Packed Quadword Data for Equality".
//
// Mnemonic        : PCMPEQQ
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PCMPEQQ xmm, xmm
//    * PCMPEQQ m128, xmm
//
func (self *Program) PCMPEQQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PCMPEQQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x29)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PCMPEQQ m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x29)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PCMPEQQ")
    }
    return p
}

// PCMPEQW performs "Compare Packed Word Data for Equality".
//
// Mnemonic        : PCMPEQW
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PCMPEQW mm, mm
//    * PCMPEQW m64, mm
//    * PCMPEQW xmm, xmm
//    * PCMPEQW m128, xmm
//
func (self *Program) PCMPEQW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PCMPEQW mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x75)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PCMPEQW m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x75)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PCMPEQW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x75)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PCMPEQW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x75)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PCMPEQW")
    }
    return p
}

// PCMPESTRI performs "Packed Compare Explicit Length Strings, Return Index".
//
// Mnemonic        : PCMPESTRI
// ISA extensions  : SSE4.2
// Supported forms : (2 forms)
//
//    * PCMPESTRI imm8, xmm, xmm
//    * PCMPESTRI imm8, m128, xmm
//
func (self *Program) PCMPESTRI(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // PCMPESTRI imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_SSE4_2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x61)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PCMPESTRI imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_SSE4_2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x61)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for PCMPESTRI")
    }
    return p
}

// PCMPESTRM performs "Packed Compare Explicit Length Strings, Return Mask".
//
// Mnemonic        : PCMPESTRM
// ISA extensions  : SSE4.2
// Supported forms : (2 forms)
//
//    * PCMPESTRM imm8, xmm, xmm
//    * PCMPESTRM imm8, m128, xmm
//
func (self *Program) PCMPESTRM(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // PCMPESTRM imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_SSE4_2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x60)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PCMPESTRM imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_SSE4_2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x60)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for PCMPESTRM")
    }
    return p
}

// PCMPGTB performs "Compare Packed Signed Byte Integers for Greater Than".
//
// Mnemonic        : PCMPGTB
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PCMPGTB mm, mm
//    * PCMPGTB m64, mm
//    * PCMPGTB xmm, xmm
//    * PCMPGTB m128, xmm
//
func (self *Program) PCMPGTB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PCMPGTB mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x64)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PCMPGTB m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x64)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PCMPGTB xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x64)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PCMPGTB m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x64)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PCMPGTB")
    }
    return p
}

// PCMPGTD performs "Compare Packed Signed Doubleword Integers for Greater Than".
//
// Mnemonic        : PCMPGTD
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PCMPGTD mm, mm
//    * PCMPGTD m64, mm
//    * PCMPGTD xmm, xmm
//    * PCMPGTD m128, xmm
//
func (self *Program) PCMPGTD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PCMPGTD mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x66)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PCMPGTD m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x66)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PCMPGTD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x66)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PCMPGTD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x66)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PCMPGTD")
    }
    return p
}

// PCMPGTQ performs "Compare Packed Data for Greater Than".
//
// Mnemonic        : PCMPGTQ
// ISA extensions  : SSE4.2
// Supported forms : (2 forms)
//
//    * PCMPGTQ xmm, xmm
//    * PCMPGTQ m128, xmm
//
func (self *Program) PCMPGTQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PCMPGTQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE4_2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x37)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PCMPGTQ m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE4_2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x37)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PCMPGTQ")
    }
    return p
}

// PCMPGTW performs "Compare Packed Signed Word Integers for Greater Than".
//
// Mnemonic        : PCMPGTW
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PCMPGTW mm, mm
//    * PCMPGTW m64, mm
//    * PCMPGTW xmm, xmm
//    * PCMPGTW m128, xmm
//
func (self *Program) PCMPGTW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PCMPGTW mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x65)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PCMPGTW m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x65)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PCMPGTW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x65)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PCMPGTW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x65)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PCMPGTW")
    }
    return p
}

// PCMPISTRI performs "Packed Compare Implicit Length Strings, Return Index".
//
// Mnemonic        : PCMPISTRI
// ISA extensions  : SSE4.2
// Supported forms : (2 forms)
//
//    * PCMPISTRI imm8, xmm, xmm
//    * PCMPISTRI imm8, m128, xmm
//
func (self *Program) PCMPISTRI(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // PCMPISTRI imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_SSE4_2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x63)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PCMPISTRI imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_SSE4_2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x63)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for PCMPISTRI")
    }
    return p
}

// PCMPISTRM performs "Packed Compare Implicit Length Strings, Return Mask".
//
// Mnemonic        : PCMPISTRM
// ISA extensions  : SSE4.2
// Supported forms : (2 forms)
//
//    * PCMPISTRM imm8, xmm, xmm
//    * PCMPISTRM imm8, m128, xmm
//
func (self *Program) PCMPISTRM(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // PCMPISTRM imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_SSE4_2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x62)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PCMPISTRM imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_SSE4_2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x62)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for PCMPISTRM")
    }
    return p
}

// PDEP performs "Parallel Bits Deposit".
//
// Mnemonic        : PDEP
// ISA extensions  : BMI2
// Supported forms : (4 forms)
//
//    * PDEP r32, r32, r32
//    * PDEP m32, r32, r32
//    * PDEP r64, r64, r64
//    * PDEP m64, r64, r64
//
func (self *Program) PDEP(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // PDEP r32, r32, r32
    if isReg32(v0) && isReg32(v1) && isReg32(v2) {
        self.require(ISA_BMI2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7b ^ (hlcode(v[1]) << 3))
            m.emit(0xf5)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // PDEP m32, r32, r32
    if isM32(v0) && isReg32(v1) && isReg32(v2) {
        self.require(ISA_BMI2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x03, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xf5)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // PDEP r64, r64, r64
    if isReg64(v0) && isReg64(v1) && isReg64(v2) {
        self.require(ISA_BMI2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfb ^ (hlcode(v[1]) << 3))
            m.emit(0xf5)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // PDEP m64, r64, r64
    if isM64(v0) && isReg64(v1) && isReg64(v2) {
        self.require(ISA_BMI2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x83, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xf5)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PDEP")
    }
    return p
}

// PEXT performs "Parallel Bits Extract".
//
// Mnemonic        : PEXT
// ISA extensions  : BMI2
// Supported forms : (4 forms)
//
//    * PEXT r32, r32, r32
//    * PEXT m32, r32, r32
//    * PEXT r64, r64, r64
//    * PEXT m64, r64, r64
//
func (self *Program) PEXT(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // PEXT r32, r32, r32
    if isReg32(v0) && isReg32(v1) && isReg32(v2) {
        self.require(ISA_BMI2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7a ^ (hlcode(v[1]) << 3))
            m.emit(0xf5)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // PEXT m32, r32, r32
    if isM32(v0) && isReg32(v1) && isReg32(v2) {
        self.require(ISA_BMI2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x02, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xf5)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // PEXT r64, r64, r64
    if isReg64(v0) && isReg64(v1) && isReg64(v2) {
        self.require(ISA_BMI2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfa ^ (hlcode(v[1]) << 3))
            m.emit(0xf5)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // PEXT m64, r64, r64
    if isM64(v0) && isReg64(v1) && isReg64(v2) {
        self.require(ISA_BMI2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x82, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xf5)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PEXT")
    }
    return p
}

// PEXTRB performs "Extract Byte".
//
// Mnemonic        : PEXTRB
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PEXTRB imm8, xmm, r32
//    * PEXTRB imm8, xmm, m8
//
func (self *Program) PEXTRB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // PEXTRB imm8, xmm, r32
    if isImm8(v0) && isXMM(v1) && isReg32(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[2], false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x14)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PEXTRB imm8, xmm, m8
    if isImm8(v0) && isXMM(v1) && isM8(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[2]), false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x14)
            m.mrsd(lcode(v[1]), addr(v[2]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for PEXTRB")
    }
    return p
}

// PEXTRD performs "Extract Doubleword".
//
// Mnemonic        : PEXTRD
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PEXTRD imm8, xmm, r32
//    * PEXTRD imm8, xmm, m32
//
func (self *Program) PEXTRD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // PEXTRD imm8, xmm, r32
    if isImm8(v0) && isXMM(v1) && isReg32(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[2], false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x16)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PEXTRD imm8, xmm, m32
    if isImm8(v0) && isXMM(v1) && isM32(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[2]), false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x16)
            m.mrsd(lcode(v[1]), addr(v[2]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for PEXTRD")
    }
    return p
}

// PEXTRQ performs "Extract Quadword".
//
// Mnemonic        : PEXTRQ
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PEXTRQ imm8, xmm, r64
//    * PEXTRQ imm8, xmm, m64
//
func (self *Program) PEXTRQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // PEXTRQ imm8, xmm, r64
    if isImm8(v0) && isXMM(v1) && isReg64(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[2]))
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x16)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PEXTRQ imm8, xmm, m64
    if isImm8(v0) && isXMM(v1) && isM64(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexm(1, hcode(v[1]), addr(v[2]))
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x16)
            m.mrsd(lcode(v[1]), addr(v[2]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for PEXTRQ")
    }
    return p
}

// PEXTRW performs "Extract Word".
//
// Mnemonic        : PEXTRW
// ISA extensions  : MMX+, SSE4.1
// Supported forms : (3 forms)
//
//    * PEXTRW imm8, mm, r32
//    * PEXTRW imm8, xmm, r32
//    * PEXTRW imm8, xmm, m16
//
func (self *Program) PEXTRW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // PEXTRW imm8, mm, r32
    if isImm8(v0) && isMM(v1) && isReg32(v2) {
        self.require(ISA_MMX_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0xc5)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PEXTRW imm8, xmm, r32
    if isImm8(v0) && isXMM(v1) && isReg32(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[2], false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x15)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0xc5)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PEXTRW imm8, xmm, m16
    if isImm8(v0) && isXMM(v1) && isM16(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[2]), false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x15)
            m.mrsd(lcode(v[1]), addr(v[2]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for PEXTRW")
    }
    return p
}

// PF2ID performs "Packed Floating-Point to Integer Doubleword Converson".
//
// Mnemonic        : PF2ID
// ISA extensions  : 3dnow!
// Supported forms : (2 forms)
//
//    * PF2ID mm, mm
//    * PF2ID m64, mm
//
func (self *Program) PF2ID(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PF2ID mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
            m.emit(0x1d)
        })
    }
    // PF2ID m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
            m.emit(0x1d)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PF2ID")
    }
    return p
}

// PF2IW performs "Packed Floating-Point to Integer Word Conversion".
//
// Mnemonic        : PF2IW
// ISA extensions  : 3dnow!+
// Supported forms : (2 forms)
//
//    * PF2IW mm, mm
//    * PF2IW m64, mm
//
func (self *Program) PF2IW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PF2IW mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_3DNOW_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
            m.emit(0x1c)
        })
    }
    // PF2IW m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_3DNOW_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
            m.emit(0x1c)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PF2IW")
    }
    return p
}

// PFACC performs "Packed Floating-Point Accumulate".
//
// Mnemonic        : PFACC
// ISA extensions  : 3dnow!
// Supported forms : (2 forms)
//
//    * PFACC mm, mm
//    * PFACC m64, mm
//
func (self *Program) PFACC(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PFACC mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
            m.emit(0xae)
        })
    }
    // PFACC m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
            m.emit(0xae)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PFACC")
    }
    return p
}

// PFADD performs "Packed Floating-Point Add".
//
// Mnemonic        : PFADD
// ISA extensions  : 3dnow!
// Supported forms : (2 forms)
//
//    * PFADD mm, mm
//    * PFADD m64, mm
//
func (self *Program) PFADD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PFADD mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
            m.emit(0x9e)
        })
    }
    // PFADD m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
            m.emit(0x9e)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PFADD")
    }
    return p
}

// PFCMPEQ performs "Packed Floating-Point Compare for Equal".
//
// Mnemonic        : PFCMPEQ
// ISA extensions  : 3dnow!
// Supported forms : (2 forms)
//
//    * PFCMPEQ mm, mm
//    * PFCMPEQ m64, mm
//
func (self *Program) PFCMPEQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PFCMPEQ mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
            m.emit(0xb0)
        })
    }
    // PFCMPEQ m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
            m.emit(0xb0)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PFCMPEQ")
    }
    return p
}

// PFCMPGE performs "Packed Floating-Point Compare for Greater or Equal".
//
// Mnemonic        : PFCMPGE
// ISA extensions  : 3dnow!
// Supported forms : (2 forms)
//
//    * PFCMPGE mm, mm
//    * PFCMPGE m64, mm
//
func (self *Program) PFCMPGE(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PFCMPGE mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
            m.emit(0x90)
        })
    }
    // PFCMPGE m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
            m.emit(0x90)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PFCMPGE")
    }
    return p
}

// PFCMPGT performs "Packed Floating-Point Compare for Greater Than".
//
// Mnemonic        : PFCMPGT
// ISA extensions  : 3dnow!
// Supported forms : (2 forms)
//
//    * PFCMPGT mm, mm
//    * PFCMPGT m64, mm
//
func (self *Program) PFCMPGT(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PFCMPGT mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
            m.emit(0xa0)
        })
    }
    // PFCMPGT m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
            m.emit(0xa0)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PFCMPGT")
    }
    return p
}

// PFMAX performs "Packed Floating-Point Maximum".
//
// Mnemonic        : PFMAX
// ISA extensions  : 3dnow!
// Supported forms : (2 forms)
//
//    * PFMAX mm, mm
//    * PFMAX m64, mm
//
func (self *Program) PFMAX(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PFMAX mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
            m.emit(0xa4)
        })
    }
    // PFMAX m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
            m.emit(0xa4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PFMAX")
    }
    return p
}

// PFMIN performs "Packed Floating-Point Minimum".
//
// Mnemonic        : PFMIN
// ISA extensions  : 3dnow!
// Supported forms : (2 forms)
//
//    * PFMIN mm, mm
//    * PFMIN m64, mm
//
func (self *Program) PFMIN(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PFMIN mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
            m.emit(0x94)
        })
    }
    // PFMIN m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
            m.emit(0x94)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PFMIN")
    }
    return p
}

// PFMUL performs "Packed Floating-Point Multiply".
//
// Mnemonic        : PFMUL
// ISA extensions  : 3dnow!
// Supported forms : (2 forms)
//
//    * PFMUL mm, mm
//    * PFMUL m64, mm
//
func (self *Program) PFMUL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PFMUL mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
            m.emit(0xb4)
        })
    }
    // PFMUL m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
            m.emit(0xb4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PFMUL")
    }
    return p
}

// PFNACC performs "Packed Floating-Point Negative Accumulate".
//
// Mnemonic        : PFNACC
// ISA extensions  : 3dnow!+
// Supported forms : (2 forms)
//
//    * PFNACC mm, mm
//    * PFNACC m64, mm
//
func (self *Program) PFNACC(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PFNACC mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_3DNOW_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
            m.emit(0x8a)
        })
    }
    // PFNACC m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_3DNOW_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
            m.emit(0x8a)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PFNACC")
    }
    return p
}

// PFPNACC performs "Packed Floating-Point Positive-Negative Accumulate".
//
// Mnemonic        : PFPNACC
// ISA extensions  : 3dnow!+
// Supported forms : (2 forms)
//
//    * PFPNACC mm, mm
//    * PFPNACC m64, mm
//
func (self *Program) PFPNACC(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PFPNACC mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_3DNOW_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
            m.emit(0x8e)
        })
    }
    // PFPNACC m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_3DNOW_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
            m.emit(0x8e)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PFPNACC")
    }
    return p
}

// PFRCP performs "Packed Floating-Point Reciprocal Approximation".
//
// Mnemonic        : PFRCP
// ISA extensions  : 3dnow!
// Supported forms : (2 forms)
//
//    * PFRCP mm, mm
//    * PFRCP m64, mm
//
func (self *Program) PFRCP(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PFRCP mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
            m.emit(0x96)
        })
    }
    // PFRCP m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
            m.emit(0x96)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PFRCP")
    }
    return p
}

// PFRCPIT1 performs "Packed Floating-Point Reciprocal Iteration 1".
//
// Mnemonic        : PFRCPIT1
// ISA extensions  : 3dnow!
// Supported forms : (2 forms)
//
//    * PFRCPIT1 mm, mm
//    * PFRCPIT1 m64, mm
//
func (self *Program) PFRCPIT1(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PFRCPIT1 mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
            m.emit(0xa6)
        })
    }
    // PFRCPIT1 m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
            m.emit(0xa6)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PFRCPIT1")
    }
    return p
}

// PFRCPIT2 performs "Packed Floating-Point Reciprocal Iteration 2".
//
// Mnemonic        : PFRCPIT2
// ISA extensions  : 3dnow!
// Supported forms : (2 forms)
//
//    * PFRCPIT2 mm, mm
//    * PFRCPIT2 m64, mm
//
func (self *Program) PFRCPIT2(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PFRCPIT2 mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
            m.emit(0xb6)
        })
    }
    // PFRCPIT2 m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
            m.emit(0xb6)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PFRCPIT2")
    }
    return p
}

// PFRSQIT1 performs "Packed Floating-Point Reciprocal Square Root Iteration 1".
//
// Mnemonic        : PFRSQIT1
// ISA extensions  : 3dnow!
// Supported forms : (2 forms)
//
//    * PFRSQIT1 mm, mm
//    * PFRSQIT1 m64, mm
//
func (self *Program) PFRSQIT1(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PFRSQIT1 mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
            m.emit(0xa7)
        })
    }
    // PFRSQIT1 m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
            m.emit(0xa7)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PFRSQIT1")
    }
    return p
}

// PFRSQRT performs "Packed Floating-Point Reciprocal Square Root Approximation".
//
// Mnemonic        : PFRSQRT
// ISA extensions  : 3dnow!
// Supported forms : (2 forms)
//
//    * PFRSQRT mm, mm
//    * PFRSQRT m64, mm
//
func (self *Program) PFRSQRT(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PFRSQRT mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
            m.emit(0x97)
        })
    }
    // PFRSQRT m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
            m.emit(0x97)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PFRSQRT")
    }
    return p
}

// PFSUB performs "Packed Floating-Point Subtract".
//
// Mnemonic        : PFSUB
// ISA extensions  : 3dnow!
// Supported forms : (2 forms)
//
//    * PFSUB mm, mm
//    * PFSUB m64, mm
//
func (self *Program) PFSUB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PFSUB mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
            m.emit(0x9a)
        })
    }
    // PFSUB m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
            m.emit(0x9a)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PFSUB")
    }
    return p
}

// PFSUBR performs "Packed Floating-Point Subtract Reverse".
//
// Mnemonic        : PFSUBR
// ISA extensions  : 3dnow!
// Supported forms : (2 forms)
//
//    * PFSUBR mm, mm
//    * PFSUBR m64, mm
//
func (self *Program) PFSUBR(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PFSUBR mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
            m.emit(0xaa)
        })
    }
    // PFSUBR m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
            m.emit(0xaa)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PFSUBR")
    }
    return p
}

// PHADDD performs "Packed Horizontal Add Doubleword Integer".
//
// Mnemonic        : PHADDD
// ISA extensions  : SSSE3
// Supported forms : (4 forms)
//
//    * PHADDD mm, mm
//    * PHADDD m64, mm
//    * PHADDD xmm, xmm
//    * PHADDD m128, xmm
//
func (self *Program) PHADDD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PHADDD mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x02)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PHADDD m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x02)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PHADDD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x02)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PHADDD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x02)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PHADDD")
    }
    return p
}

// PHADDSW performs "Packed Horizontal Add Signed Word Integers with Signed Saturation".
//
// Mnemonic        : PHADDSW
// ISA extensions  : SSSE3
// Supported forms : (4 forms)
//
//    * PHADDSW mm, mm
//    * PHADDSW m64, mm
//    * PHADDSW xmm, xmm
//    * PHADDSW m128, xmm
//
func (self *Program) PHADDSW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PHADDSW mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x03)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PHADDSW m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x03)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PHADDSW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x03)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PHADDSW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x03)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PHADDSW")
    }
    return p
}

// PHADDW performs "Packed Horizontal Add Word Integers".
//
// Mnemonic        : PHADDW
// ISA extensions  : SSSE3
// Supported forms : (4 forms)
//
//    * PHADDW mm, mm
//    * PHADDW m64, mm
//    * PHADDW xmm, xmm
//    * PHADDW m128, xmm
//
func (self *Program) PHADDW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PHADDW mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x01)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PHADDW m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x01)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PHADDW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x01)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PHADDW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x01)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PHADDW")
    }
    return p
}

// PHMINPOSUW performs "Packed Horizontal Minimum of Unsigned Word Integers".
//
// Mnemonic        : PHMINPOSUW
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PHMINPOSUW xmm, xmm
//    * PHMINPOSUW m128, xmm
//
func (self *Program) PHMINPOSUW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PHMINPOSUW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x41)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PHMINPOSUW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x41)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PHMINPOSUW")
    }
    return p
}

// PHSUBD performs "Packed Horizontal Subtract Doubleword Integers".
//
// Mnemonic        : PHSUBD
// ISA extensions  : SSSE3
// Supported forms : (4 forms)
//
//    * PHSUBD mm, mm
//    * PHSUBD m64, mm
//    * PHSUBD xmm, xmm
//    * PHSUBD m128, xmm
//
func (self *Program) PHSUBD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PHSUBD mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x06)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PHSUBD m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x06)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PHSUBD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x06)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PHSUBD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x06)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PHSUBD")
    }
    return p
}

// PHSUBSW performs "Packed Horizontal Subtract Signed Word Integers with Signed Saturation".
//
// Mnemonic        : PHSUBSW
// ISA extensions  : SSSE3
// Supported forms : (4 forms)
//
//    * PHSUBSW mm, mm
//    * PHSUBSW m64, mm
//    * PHSUBSW xmm, xmm
//    * PHSUBSW m128, xmm
//
func (self *Program) PHSUBSW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PHSUBSW mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x07)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PHSUBSW m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x07)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PHSUBSW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x07)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PHSUBSW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x07)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PHSUBSW")
    }
    return p
}

// PHSUBW performs "Packed Horizontal Subtract Word Integers".
//
// Mnemonic        : PHSUBW
// ISA extensions  : SSSE3
// Supported forms : (4 forms)
//
//    * PHSUBW mm, mm
//    * PHSUBW m64, mm
//    * PHSUBW xmm, xmm
//    * PHSUBW m128, xmm
//
func (self *Program) PHSUBW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PHSUBW mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x05)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PHSUBW m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x05)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PHSUBW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x05)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PHSUBW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x05)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PHSUBW")
    }
    return p
}

// PI2FD performs "Packed Integer to Floating-Point Doubleword Conversion".
//
// Mnemonic        : PI2FD
// ISA extensions  : 3dnow!
// Supported forms : (2 forms)
//
//    * PI2FD mm, mm
//    * PI2FD m64, mm
//
func (self *Program) PI2FD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PI2FD mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
            m.emit(0x0d)
        })
    }
    // PI2FD m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
            m.emit(0x0d)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PI2FD")
    }
    return p
}

// PI2FW performs "Packed Integer to Floating-Point Word Conversion".
//
// Mnemonic        : PI2FW
// ISA extensions  : 3dnow!+
// Supported forms : (2 forms)
//
//    * PI2FW mm, mm
//    * PI2FW m64, mm
//
func (self *Program) PI2FW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PI2FW mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_3DNOW_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
            m.emit(0x0c)
        })
    }
    // PI2FW m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_3DNOW_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
            m.emit(0x0c)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PI2FW")
    }
    return p
}

// PINSRB performs "Insert Byte".
//
// Mnemonic        : PINSRB
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PINSRB imm8, r32, xmm
//    * PINSRB imm8, m8, xmm
//
func (self *Program) PINSRB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // PINSRB imm8, r32, xmm
    if isImm8(v0) && isReg32(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x20)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PINSRB imm8, m8, xmm
    if isImm8(v0) && isM8(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x20)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for PINSRB")
    }
    return p
}

// PINSRD performs "Insert Doubleword".
//
// Mnemonic        : PINSRD
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PINSRD imm8, r32, xmm
//    * PINSRD imm8, m32, xmm
//
func (self *Program) PINSRD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // PINSRD imm8, r32, xmm
    if isImm8(v0) && isReg32(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x22)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PINSRD imm8, m32, xmm
    if isImm8(v0) && isM32(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x22)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for PINSRD")
    }
    return p
}

// PINSRQ performs "Insert Quadword".
//
// Mnemonic        : PINSRQ
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PINSRQ imm8, r64, xmm
//    * PINSRQ imm8, m64, xmm
//
func (self *Program) PINSRQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // PINSRQ imm8, r64, xmm
    if isImm8(v0) && isReg64(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.emit(0x48 | hcode(v[2]) << 2 | hcode(v[1]))
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x22)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PINSRQ imm8, m64, xmm
    if isImm8(v0) && isM64(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexm(1, hcode(v[2]), addr(v[1]))
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x22)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for PINSRQ")
    }
    return p
}

// PINSRW performs "Insert Word".
//
// Mnemonic        : PINSRW
// ISA extensions  : MMX+, SSE2
// Supported forms : (4 forms)
//
//    * PINSRW imm8, r32, mm
//    * PINSRW imm8, m16, mm
//    * PINSRW imm8, r32, xmm
//    * PINSRW imm8, m16, xmm
//
func (self *Program) PINSRW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // PINSRW imm8, r32, mm
    if isImm8(v0) && isReg32(v1) && isMM(v2) {
        self.require(ISA_MMX_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0xc4)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PINSRW imm8, m16, mm
    if isImm8(v0) && isM16(v1) && isMM(v2) {
        self.require(ISA_MMX_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0xc4)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // PINSRW imm8, r32, xmm
    if isImm8(v0) && isReg32(v1) && isXMM(v2) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0xc4)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PINSRW imm8, m16, xmm
    if isImm8(v0) && isM16(v1) && isXMM(v2) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0xc4)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for PINSRW")
    }
    return p
}

// PMADDUBSW performs "Multiply and Add Packed Signed and Unsigned Byte Integers".
//
// Mnemonic        : PMADDUBSW
// ISA extensions  : SSSE3
// Supported forms : (4 forms)
//
//    * PMADDUBSW mm, mm
//    * PMADDUBSW m64, mm
//    * PMADDUBSW xmm, xmm
//    * PMADDUBSW m128, xmm
//
func (self *Program) PMADDUBSW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMADDUBSW mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x04)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMADDUBSW m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x04)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PMADDUBSW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x04)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMADDUBSW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x04)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMADDUBSW")
    }
    return p
}

// PMADDWD performs "Multiply and Add Packed Signed Word Integers".
//
// Mnemonic        : PMADDWD
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PMADDWD mm, mm
//    * PMADDWD m64, mm
//    * PMADDWD xmm, xmm
//    * PMADDWD m128, xmm
//
func (self *Program) PMADDWD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMADDWD mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xf5)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMADDWD m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xf5)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PMADDWD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xf5)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMADDWD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xf5)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMADDWD")
    }
    return p
}

// PMAXSB performs "Maximum of Packed Signed Byte Integers".
//
// Mnemonic        : PMAXSB
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PMAXSB xmm, xmm
//    * PMAXSB m128, xmm
//
func (self *Program) PMAXSB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMAXSB xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x3c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMAXSB m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x3c)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMAXSB")
    }
    return p
}

// PMAXSD performs "Maximum of Packed Signed Doubleword Integers".
//
// Mnemonic        : PMAXSD
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PMAXSD xmm, xmm
//    * PMAXSD m128, xmm
//
func (self *Program) PMAXSD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMAXSD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x3d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMAXSD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x3d)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMAXSD")
    }
    return p
}

// PMAXSW performs "Maximum of Packed Signed Word Integers".
//
// Mnemonic        : PMAXSW
// ISA extensions  : MMX+, SSE2
// Supported forms : (4 forms)
//
//    * PMAXSW mm, mm
//    * PMAXSW m64, mm
//    * PMAXSW xmm, xmm
//    * PMAXSW m128, xmm
//
func (self *Program) PMAXSW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMAXSW mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xee)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMAXSW m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xee)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PMAXSW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xee)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMAXSW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xee)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMAXSW")
    }
    return p
}

// PMAXUB performs "Maximum of Packed Unsigned Byte Integers".
//
// Mnemonic        : PMAXUB
// ISA extensions  : MMX+, SSE2
// Supported forms : (4 forms)
//
//    * PMAXUB mm, mm
//    * PMAXUB m64, mm
//    * PMAXUB xmm, xmm
//    * PMAXUB m128, xmm
//
func (self *Program) PMAXUB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMAXUB mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xde)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMAXUB m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xde)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PMAXUB xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xde)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMAXUB m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xde)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMAXUB")
    }
    return p
}

// PMAXUD performs "Maximum of Packed Unsigned Doubleword Integers".
//
// Mnemonic        : PMAXUD
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PMAXUD xmm, xmm
//    * PMAXUD m128, xmm
//
func (self *Program) PMAXUD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMAXUD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x3f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMAXUD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x3f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMAXUD")
    }
    return p
}

// PMAXUW performs "Maximum of Packed Unsigned Word Integers".
//
// Mnemonic        : PMAXUW
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PMAXUW xmm, xmm
//    * PMAXUW m128, xmm
//
func (self *Program) PMAXUW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMAXUW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x3e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMAXUW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x3e)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMAXUW")
    }
    return p
}

// PMINSB performs "Minimum of Packed Signed Byte Integers".
//
// Mnemonic        : PMINSB
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PMINSB xmm, xmm
//    * PMINSB m128, xmm
//
func (self *Program) PMINSB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMINSB xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x38)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMINSB m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x38)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMINSB")
    }
    return p
}

// PMINSD performs "Minimum of Packed Signed Doubleword Integers".
//
// Mnemonic        : PMINSD
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PMINSD xmm, xmm
//    * PMINSD m128, xmm
//
func (self *Program) PMINSD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMINSD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x39)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMINSD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x39)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMINSD")
    }
    return p
}

// PMINSW performs "Minimum of Packed Signed Word Integers".
//
// Mnemonic        : PMINSW
// ISA extensions  : MMX+, SSE2
// Supported forms : (4 forms)
//
//    * PMINSW mm, mm
//    * PMINSW m64, mm
//    * PMINSW xmm, xmm
//    * PMINSW m128, xmm
//
func (self *Program) PMINSW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMINSW mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xea)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMINSW m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xea)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PMINSW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xea)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMINSW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xea)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMINSW")
    }
    return p
}

// PMINUB performs "Minimum of Packed Unsigned Byte Integers".
//
// Mnemonic        : PMINUB
// ISA extensions  : MMX+, SSE2
// Supported forms : (4 forms)
//
//    * PMINUB mm, mm
//    * PMINUB m64, mm
//    * PMINUB xmm, xmm
//    * PMINUB m128, xmm
//
func (self *Program) PMINUB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMINUB mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xda)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMINUB m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xda)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PMINUB xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xda)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMINUB m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xda)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMINUB")
    }
    return p
}

// PMINUD performs "Minimum of Packed Unsigned Doubleword Integers".
//
// Mnemonic        : PMINUD
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PMINUD xmm, xmm
//    * PMINUD m128, xmm
//
func (self *Program) PMINUD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMINUD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x3b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMINUD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x3b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMINUD")
    }
    return p
}

// PMINUW performs "Minimum of Packed Unsigned Word Integers".
//
// Mnemonic        : PMINUW
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PMINUW xmm, xmm
//    * PMINUW m128, xmm
//
func (self *Program) PMINUW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMINUW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x3a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMINUW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x3a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMINUW")
    }
    return p
}

// PMOVMSKB performs "Move Byte Mask".
//
// Mnemonic        : PMOVMSKB
// ISA extensions  : MMX+, SSE2
// Supported forms : (2 forms)
//
//    * PMOVMSKB mm, r32
//    * PMOVMSKB xmm, r32
//
func (self *Program) PMOVMSKB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMOVMSKB mm, r32
    if isMM(v0) && isReg32(v1) {
        self.require(ISA_MMX_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xd7)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMOVMSKB xmm, r32
    if isXMM(v0) && isReg32(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xd7)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMOVMSKB")
    }
    return p
}

// PMOVSXBD performs "Move Packed Byte Integers to Doubleword Integers with Sign Extension".
//
// Mnemonic        : PMOVSXBD
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PMOVSXBD xmm, xmm
//    * PMOVSXBD m32, xmm
//
func (self *Program) PMOVSXBD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMOVSXBD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x21)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMOVSXBD m32, xmm
    if isM32(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x21)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMOVSXBD")
    }
    return p
}

// PMOVSXBQ performs "Move Packed Byte Integers to Quadword Integers with Sign Extension".
//
// Mnemonic        : PMOVSXBQ
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PMOVSXBQ xmm, xmm
//    * PMOVSXBQ m16, xmm
//
func (self *Program) PMOVSXBQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMOVSXBQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x22)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMOVSXBQ m16, xmm
    if isM16(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x22)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMOVSXBQ")
    }
    return p
}

// PMOVSXBW performs "Move Packed Byte Integers to Word Integers with Sign Extension".
//
// Mnemonic        : PMOVSXBW
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PMOVSXBW xmm, xmm
//    * PMOVSXBW m64, xmm
//
func (self *Program) PMOVSXBW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMOVSXBW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x20)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMOVSXBW m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x20)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMOVSXBW")
    }
    return p
}

// PMOVSXDQ performs "Move Packed Doubleword Integers to Quadword Integers with Sign Extension".
//
// Mnemonic        : PMOVSXDQ
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PMOVSXDQ xmm, xmm
//    * PMOVSXDQ m64, xmm
//
func (self *Program) PMOVSXDQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMOVSXDQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x25)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMOVSXDQ m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x25)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMOVSXDQ")
    }
    return p
}

// PMOVSXWD performs "Move Packed Word Integers to Doubleword Integers with Sign Extension".
//
// Mnemonic        : PMOVSXWD
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PMOVSXWD xmm, xmm
//    * PMOVSXWD m64, xmm
//
func (self *Program) PMOVSXWD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMOVSXWD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x23)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMOVSXWD m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x23)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMOVSXWD")
    }
    return p
}

// PMOVSXWQ performs "Move Packed Word Integers to Quadword Integers with Sign Extension".
//
// Mnemonic        : PMOVSXWQ
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PMOVSXWQ xmm, xmm
//    * PMOVSXWQ m32, xmm
//
func (self *Program) PMOVSXWQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMOVSXWQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x24)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMOVSXWQ m32, xmm
    if isM32(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x24)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMOVSXWQ")
    }
    return p
}

// PMOVZXBD performs "Move Packed Byte Integers to Doubleword Integers with Zero Extension".
//
// Mnemonic        : PMOVZXBD
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PMOVZXBD xmm, xmm
//    * PMOVZXBD m32, xmm
//
func (self *Program) PMOVZXBD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMOVZXBD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x31)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMOVZXBD m32, xmm
    if isM32(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x31)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMOVZXBD")
    }
    return p
}

// PMOVZXBQ performs "Move Packed Byte Integers to Quadword Integers with Zero Extension".
//
// Mnemonic        : PMOVZXBQ
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PMOVZXBQ xmm, xmm
//    * PMOVZXBQ m16, xmm
//
func (self *Program) PMOVZXBQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMOVZXBQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x32)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMOVZXBQ m16, xmm
    if isM16(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x32)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMOVZXBQ")
    }
    return p
}

// PMOVZXBW performs "Move Packed Byte Integers to Word Integers with Zero Extension".
//
// Mnemonic        : PMOVZXBW
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PMOVZXBW xmm, xmm
//    * PMOVZXBW m64, xmm
//
func (self *Program) PMOVZXBW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMOVZXBW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x30)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMOVZXBW m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x30)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMOVZXBW")
    }
    return p
}

// PMOVZXDQ performs "Move Packed Doubleword Integers to Quadword Integers with Zero Extension".
//
// Mnemonic        : PMOVZXDQ
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PMOVZXDQ xmm, xmm
//    * PMOVZXDQ m64, xmm
//
func (self *Program) PMOVZXDQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMOVZXDQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x35)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMOVZXDQ m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x35)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMOVZXDQ")
    }
    return p
}

// PMOVZXWD performs "Move Packed Word Integers to Doubleword Integers with Zero Extension".
//
// Mnemonic        : PMOVZXWD
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PMOVZXWD xmm, xmm
//    * PMOVZXWD m64, xmm
//
func (self *Program) PMOVZXWD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMOVZXWD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x33)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMOVZXWD m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x33)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMOVZXWD")
    }
    return p
}

// PMOVZXWQ performs "Move Packed Word Integers to Quadword Integers with Zero Extension".
//
// Mnemonic        : PMOVZXWQ
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PMOVZXWQ xmm, xmm
//    * PMOVZXWQ m32, xmm
//
func (self *Program) PMOVZXWQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMOVZXWQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x34)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMOVZXWQ m32, xmm
    if isM32(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x34)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMOVZXWQ")
    }
    return p
}

// PMULDQ performs "Multiply Packed Signed Doubleword Integers and Store Quadword Result".
//
// Mnemonic        : PMULDQ
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PMULDQ xmm, xmm
//    * PMULDQ m128, xmm
//
func (self *Program) PMULDQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMULDQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x28)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMULDQ m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x28)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMULDQ")
    }
    return p
}

// PMULHRSW performs "Packed Multiply Signed Word Integers and Store High Result with Round and Scale".
//
// Mnemonic        : PMULHRSW
// ISA extensions  : SSSE3
// Supported forms : (4 forms)
//
//    * PMULHRSW mm, mm
//    * PMULHRSW m64, mm
//    * PMULHRSW xmm, xmm
//    * PMULHRSW m128, xmm
//
func (self *Program) PMULHRSW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMULHRSW mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x0b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMULHRSW m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x0b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PMULHRSW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x0b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMULHRSW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x0b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMULHRSW")
    }
    return p
}

// PMULHRW performs "Packed Multiply High Rounded Word".
//
// Mnemonic        : PMULHRW
// ISA extensions  : 3dnow!
// Supported forms : (2 forms)
//
//    * PMULHRW mm, mm
//    * PMULHRW m64, mm
//
func (self *Program) PMULHRW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMULHRW mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
            m.emit(0xb7)
        })
    }
    // PMULHRW m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_3DNOW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
            m.emit(0xb7)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMULHRW")
    }
    return p
}

// PMULHUW performs "Multiply Packed Unsigned Word Integers and Store High Result".
//
// Mnemonic        : PMULHUW
// ISA extensions  : MMX+, SSE2
// Supported forms : (4 forms)
//
//    * PMULHUW mm, mm
//    * PMULHUW m64, mm
//    * PMULHUW xmm, xmm
//    * PMULHUW m128, xmm
//
func (self *Program) PMULHUW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMULHUW mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xe4)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMULHUW m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xe4)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PMULHUW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xe4)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMULHUW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xe4)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMULHUW")
    }
    return p
}

// PMULHW performs "Multiply Packed Signed Word Integers and Store High Result".
//
// Mnemonic        : PMULHW
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PMULHW mm, mm
//    * PMULHW m64, mm
//    * PMULHW xmm, xmm
//    * PMULHW m128, xmm
//
func (self *Program) PMULHW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMULHW mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xe5)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMULHW m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xe5)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PMULHW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xe5)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMULHW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xe5)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMULHW")
    }
    return p
}

// PMULLD performs "Multiply Packed Signed Doubleword Integers and Store Low Result".
//
// Mnemonic        : PMULLD
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PMULLD xmm, xmm
//    * PMULLD m128, xmm
//
func (self *Program) PMULLD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMULLD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x40)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMULLD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x40)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMULLD")
    }
    return p
}

// PMULLW performs "Multiply Packed Signed Word Integers and Store Low Result".
//
// Mnemonic        : PMULLW
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PMULLW mm, mm
//    * PMULLW m64, mm
//    * PMULLW xmm, xmm
//    * PMULLW m128, xmm
//
func (self *Program) PMULLW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMULLW mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xd5)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMULLW m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xd5)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PMULLW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xd5)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMULLW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xd5)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMULLW")
    }
    return p
}

// PMULUDQ performs "Multiply Packed Unsigned Doubleword Integers".
//
// Mnemonic        : PMULUDQ
// ISA extensions  : SSE2
// Supported forms : (4 forms)
//
//    * PMULUDQ mm, mm
//    * PMULUDQ m64, mm
//    * PMULUDQ xmm, xmm
//    * PMULUDQ m128, xmm
//
func (self *Program) PMULUDQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PMULUDQ mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xf4)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMULUDQ m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xf4)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PMULUDQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xf4)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PMULUDQ m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xf4)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PMULUDQ")
    }
    return p
}

// POPCNTL performs "Count of Number of Bits Set to 1".
//
// Mnemonic        : POPCNT
// ISA extensions  : POPCNT
// Supported forms : (2 forms)
//
//    * POPCNTL r32, r32
//    * POPCNTL m32, r32
//
func (self *Program) POPCNTL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // POPCNTL r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_POPCNT)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xb8)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // POPCNTL m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_POPCNT)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xb8)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for POPCNTL")
    }
    return p
}

// POPCNTQ performs "Count of Number of Bits Set to 1".
//
// Mnemonic        : POPCNT
// ISA extensions  : POPCNT
// Supported forms : (2 forms)
//
//    * POPCNTQ r64, r64
//    * POPCNTQ m64, r64
//
func (self *Program) POPCNTQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // POPCNTQ r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_POPCNT)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0xb8)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // POPCNTQ m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_POPCNT)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0xb8)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for POPCNTQ")
    }
    return p
}

// POPCNTW performs "Count of Number of Bits Set to 1".
//
// Mnemonic        : POPCNT
// ISA extensions  : POPCNT
// Supported forms : (2 forms)
//
//    * POPCNTW r16, r16
//    * POPCNTW m16, r16
//
func (self *Program) POPCNTW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // POPCNTW r16, r16
    if isReg16(v0) && isReg16(v1) {
        self.require(ISA_POPCNT)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.emit(0xf3)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xb8)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // POPCNTW m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_POPCNT)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.emit(0xf3)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xb8)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for POPCNTW")
    }
    return p
}

// POPQ performs "Pop a Value from the Stack".
//
// Mnemonic        : POP
// Supported forms : (2 forms)
//
//    * POPQ r64
//    * POPQ m64
//
func (self *Program) POPQ(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // POPQ r64
    if isReg64(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], false)
            m.emit(0x58 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], false)
            m.emit(0x8f)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // POPQ m64
    if isM64(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x8f)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for POPQ")
    }
    return p
}

// POPW performs "Pop a Value from the Stack".
//
// Mnemonic        : POP
// Supported forms : (2 forms)
//
//    * POPW r16
//    * POPW m16
//
func (self *Program) POPW(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // POPW r16
    if isReg16(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[0], false)
            m.emit(0x58 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[0], false)
            m.emit(0x8f)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // POPW m16
    if isM16(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[0]), false)
            m.emit(0x8f)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for POPW")
    }
    return p
}

// POR performs "Packed Bitwise Logical OR".
//
// Mnemonic        : POR
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * POR mm, mm
//    * POR m64, mm
//    * POR xmm, xmm
//    * POR m128, xmm
//
func (self *Program) POR(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // POR mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xeb)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // POR m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xeb)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // POR xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xeb)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // POR m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xeb)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for POR")
    }
    return p
}

// PREFETCH performs "Prefetch Data into Caches".
//
// Mnemonic        : PREFETCH
// ISA extensions  : PREFETCH
// Supported forms : (1 form)
//
//    * PREFETCH m8
//
func (self *Program) PREFETCH(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // PREFETCH m8
    if isM8(v0) {
        self.require(ISA_PREFETCH)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x0d)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PREFETCH")
    }
    return p
}

// PREFETCHNTA performs "Prefetch Data Into Caches using NTA Hint".
//
// Mnemonic        : PREFETCHNTA
// ISA extensions  : MMX+
// Supported forms : (1 form)
//
//    * PREFETCHNTA m8
//
func (self *Program) PREFETCHNTA(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // PREFETCHNTA m8
    if isM8(v0) {
        self.require(ISA_MMX_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x18)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PREFETCHNTA")
    }
    return p
}

// PREFETCHT0 performs "Prefetch Data Into Caches using T0 Hint".
//
// Mnemonic        : PREFETCHT0
// ISA extensions  : MMX+
// Supported forms : (1 form)
//
//    * PREFETCHT0 m8
//
func (self *Program) PREFETCHT0(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // PREFETCHT0 m8
    if isM8(v0) {
        self.require(ISA_MMX_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x18)
            m.mrsd(1, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PREFETCHT0")
    }
    return p
}

// PREFETCHT1 performs "Prefetch Data Into Caches using T1 Hint".
//
// Mnemonic        : PREFETCHT1
// ISA extensions  : MMX+
// Supported forms : (1 form)
//
//    * PREFETCHT1 m8
//
func (self *Program) PREFETCHT1(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // PREFETCHT1 m8
    if isM8(v0) {
        self.require(ISA_MMX_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x18)
            m.mrsd(2, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PREFETCHT1")
    }
    return p
}

// PREFETCHT2 performs "Prefetch Data Into Caches using T2 Hint".
//
// Mnemonic        : PREFETCHT2
// ISA extensions  : MMX+
// Supported forms : (1 form)
//
//    * PREFETCHT2 m8
//
func (self *Program) PREFETCHT2(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // PREFETCHT2 m8
    if isM8(v0) {
        self.require(ISA_MMX_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x18)
            m.mrsd(3, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PREFETCHT2")
    }
    return p
}

// PREFETCHW performs "Prefetch Data into Caches in Anticipation of a Write".
//
// Mnemonic        : PREFETCHW
// ISA extensions  : PREFETCHW
// Supported forms : (1 form)
//
//    * PREFETCHW m8
//
func (self *Program) PREFETCHW(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // PREFETCHW m8
    if isM8(v0) {
        self.require(ISA_PREFETCHW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x0d)
            m.mrsd(1, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PREFETCHW")
    }
    return p
}

// PREFETCHWT1 performs "Prefetch Vector Data Into Caches with Intent to Write and T1 Hint".
//
// Mnemonic        : PREFETCHWT1
// ISA extensions  : PREFETCHWT1
// Supported forms : (1 form)
//
//    * PREFETCHWT1 m8
//
func (self *Program) PREFETCHWT1(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // PREFETCHWT1 m8
    if isM8(v0) {
        self.require(ISA_PREFETCHWT1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x0d)
            m.mrsd(2, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PREFETCHWT1")
    }
    return p
}

// PSADBW performs "Compute Sum of Absolute Differences".
//
// Mnemonic        : PSADBW
// ISA extensions  : MMX+, SSE2
// Supported forms : (4 forms)
//
//    * PSADBW mm, mm
//    * PSADBW m64, mm
//    * PSADBW xmm, xmm
//    * PSADBW m128, xmm
//
func (self *Program) PSADBW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PSADBW mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xf6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSADBW m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xf6)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PSADBW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xf6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSADBW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xf6)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PSADBW")
    }
    return p
}

// PSHUFB performs "Packed Shuffle Bytes".
//
// Mnemonic        : PSHUFB
// ISA extensions  : SSSE3
// Supported forms : (4 forms)
//
//    * PSHUFB mm, mm
//    * PSHUFB m64, mm
//    * PSHUFB xmm, xmm
//    * PSHUFB m128, xmm
//
func (self *Program) PSHUFB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PSHUFB mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x00)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSHUFB m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x00)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PSHUFB xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x00)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSHUFB m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x00)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PSHUFB")
    }
    return p
}

// PSHUFD performs "Shuffle Packed Doublewords".
//
// Mnemonic        : PSHUFD
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * PSHUFD imm8, xmm, xmm
//    * PSHUFD imm8, m128, xmm
//
func (self *Program) PSHUFD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // PSHUFD imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0x70)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PSHUFD imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x70)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for PSHUFD")
    }
    return p
}

// PSHUFHW performs "Shuffle Packed High Words".
//
// Mnemonic        : PSHUFHW
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * PSHUFHW imm8, xmm, xmm
//    * PSHUFHW imm8, m128, xmm
//
func (self *Program) PSHUFHW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // PSHUFHW imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0x70)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PSHUFHW imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x70)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for PSHUFHW")
    }
    return p
}

// PSHUFLW performs "Shuffle Packed Low Words".
//
// Mnemonic        : PSHUFLW
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * PSHUFLW imm8, xmm, xmm
//    * PSHUFLW imm8, m128, xmm
//
func (self *Program) PSHUFLW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // PSHUFLW imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0x70)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PSHUFLW imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x70)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for PSHUFLW")
    }
    return p
}

// PSHUFW performs "Shuffle Packed Words".
//
// Mnemonic        : PSHUFW
// ISA extensions  : MMX+
// Supported forms : (2 forms)
//
//    * PSHUFW imm8, mm, mm
//    * PSHUFW imm8, m64, mm
//
func (self *Program) PSHUFW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // PSHUFW imm8, mm, mm
    if isImm8(v0) && isMM(v1) && isMM(v2) {
        self.require(ISA_MMX_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0x70)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PSHUFW imm8, m64, mm
    if isImm8(v0) && isM64(v1) && isMM(v2) {
        self.require(ISA_MMX_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x70)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for PSHUFW")
    }
    return p
}

// PSIGNB performs "Packed Sign of Byte Integers".
//
// Mnemonic        : PSIGNB
// ISA extensions  : SSSE3
// Supported forms : (4 forms)
//
//    * PSIGNB mm, mm
//    * PSIGNB m64, mm
//    * PSIGNB xmm, xmm
//    * PSIGNB m128, xmm
//
func (self *Program) PSIGNB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PSIGNB mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x08)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSIGNB m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x08)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PSIGNB xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x08)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSIGNB m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x08)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PSIGNB")
    }
    return p
}

// PSIGND performs "Packed Sign of Doubleword Integers".
//
// Mnemonic        : PSIGND
// ISA extensions  : SSSE3
// Supported forms : (4 forms)
//
//    * PSIGND mm, mm
//    * PSIGND m64, mm
//    * PSIGND xmm, xmm
//    * PSIGND m128, xmm
//
func (self *Program) PSIGND(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PSIGND mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x0a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSIGND m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x0a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PSIGND xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x0a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSIGND m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x0a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PSIGND")
    }
    return p
}

// PSIGNW performs "Packed Sign of Word Integers".
//
// Mnemonic        : PSIGNW
// ISA extensions  : SSSE3
// Supported forms : (4 forms)
//
//    * PSIGNW mm, mm
//    * PSIGNW m64, mm
//    * PSIGNW xmm, xmm
//    * PSIGNW m128, xmm
//
func (self *Program) PSIGNW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PSIGNW mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x09)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSIGNW m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x09)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PSIGNW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x09)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSIGNW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSSE3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x09)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PSIGNW")
    }
    return p
}

// PSLLD performs "Shift Packed Doubleword Data Left Logical".
//
// Mnemonic        : PSLLD
// ISA extensions  : MMX, SSE2
// Supported forms : (6 forms)
//
//    * PSLLD imm8, mm
//    * PSLLD mm, mm
//    * PSLLD m64, mm
//    * PSLLD imm8, xmm
//    * PSLLD xmm, xmm
//    * PSLLD m128, xmm
//
func (self *Program) PSLLD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PSLLD imm8, mm
    if isImm8(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0x0f)
            m.emit(0x72)
            m.emit(0xf0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PSLLD mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xf2)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSLLD m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xf2)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PSLLD imm8, xmm
    if isImm8(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0x0f)
            m.emit(0x72)
            m.emit(0xf0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PSLLD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xf2)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSLLD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xf2)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PSLLD")
    }
    return p
}

// PSLLDQ performs "Shift Packed Double Quadword Left Logical".
//
// Mnemonic        : PSLLDQ
// ISA extensions  : SSE2
// Supported forms : (1 form)
//
//    * PSLLDQ imm8, xmm
//
func (self *Program) PSLLDQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PSLLDQ imm8, xmm
    if isImm8(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0x0f)
            m.emit(0x73)
            m.emit(0xf8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for PSLLDQ")
    }
    return p
}

// PSLLQ performs "Shift Packed Quadword Data Left Logical".
//
// Mnemonic        : PSLLQ
// ISA extensions  : MMX, SSE2
// Supported forms : (6 forms)
//
//    * PSLLQ imm8, mm
//    * PSLLQ mm, mm
//    * PSLLQ m64, mm
//    * PSLLQ imm8, xmm
//    * PSLLQ xmm, xmm
//    * PSLLQ m128, xmm
//
func (self *Program) PSLLQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PSLLQ imm8, mm
    if isImm8(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0x0f)
            m.emit(0x73)
            m.emit(0xf0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PSLLQ mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xf3)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSLLQ m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xf3)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PSLLQ imm8, xmm
    if isImm8(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0x0f)
            m.emit(0x73)
            m.emit(0xf0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PSLLQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xf3)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSLLQ m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xf3)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PSLLQ")
    }
    return p
}

// PSLLW performs "Shift Packed Word Data Left Logical".
//
// Mnemonic        : PSLLW
// ISA extensions  : MMX, SSE2
// Supported forms : (6 forms)
//
//    * PSLLW imm8, mm
//    * PSLLW mm, mm
//    * PSLLW m64, mm
//    * PSLLW imm8, xmm
//    * PSLLW xmm, xmm
//    * PSLLW m128, xmm
//
func (self *Program) PSLLW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PSLLW imm8, mm
    if isImm8(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0x0f)
            m.emit(0x71)
            m.emit(0xf0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PSLLW mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xf1)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSLLW m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xf1)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PSLLW imm8, xmm
    if isImm8(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0x0f)
            m.emit(0x71)
            m.emit(0xf0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PSLLW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xf1)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSLLW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xf1)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PSLLW")
    }
    return p
}

// PSRAD performs "Shift Packed Doubleword Data Right Arithmetic".
//
// Mnemonic        : PSRAD
// ISA extensions  : MMX, SSE2
// Supported forms : (6 forms)
//
//    * PSRAD imm8, mm
//    * PSRAD mm, mm
//    * PSRAD m64, mm
//    * PSRAD imm8, xmm
//    * PSRAD xmm, xmm
//    * PSRAD m128, xmm
//
func (self *Program) PSRAD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PSRAD imm8, mm
    if isImm8(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0x0f)
            m.emit(0x72)
            m.emit(0xe0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PSRAD mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xe2)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSRAD m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xe2)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PSRAD imm8, xmm
    if isImm8(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0x0f)
            m.emit(0x72)
            m.emit(0xe0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PSRAD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xe2)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSRAD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xe2)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PSRAD")
    }
    return p
}

// PSRAW performs "Shift Packed Word Data Right Arithmetic".
//
// Mnemonic        : PSRAW
// ISA extensions  : MMX, SSE2
// Supported forms : (6 forms)
//
//    * PSRAW imm8, mm
//    * PSRAW mm, mm
//    * PSRAW m64, mm
//    * PSRAW imm8, xmm
//    * PSRAW xmm, xmm
//    * PSRAW m128, xmm
//
func (self *Program) PSRAW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PSRAW imm8, mm
    if isImm8(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0x0f)
            m.emit(0x71)
            m.emit(0xe0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PSRAW mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xe1)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSRAW m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xe1)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PSRAW imm8, xmm
    if isImm8(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0x0f)
            m.emit(0x71)
            m.emit(0xe0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PSRAW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xe1)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSRAW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xe1)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PSRAW")
    }
    return p
}

// PSRLD performs "Shift Packed Doubleword Data Right Logical".
//
// Mnemonic        : PSRLD
// ISA extensions  : MMX, SSE2
// Supported forms : (6 forms)
//
//    * PSRLD imm8, mm
//    * PSRLD mm, mm
//    * PSRLD m64, mm
//    * PSRLD imm8, xmm
//    * PSRLD xmm, xmm
//    * PSRLD m128, xmm
//
func (self *Program) PSRLD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PSRLD imm8, mm
    if isImm8(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0x0f)
            m.emit(0x72)
            m.emit(0xd0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PSRLD mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xd2)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSRLD m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xd2)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PSRLD imm8, xmm
    if isImm8(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0x0f)
            m.emit(0x72)
            m.emit(0xd0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PSRLD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xd2)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSRLD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xd2)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PSRLD")
    }
    return p
}

// PSRLDQ performs "Shift Packed Double Quadword Right Logical".
//
// Mnemonic        : PSRLDQ
// ISA extensions  : SSE2
// Supported forms : (1 form)
//
//    * PSRLDQ imm8, xmm
//
func (self *Program) PSRLDQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PSRLDQ imm8, xmm
    if isImm8(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0x0f)
            m.emit(0x73)
            m.emit(0xd8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for PSRLDQ")
    }
    return p
}

// PSRLQ performs "Shift Packed Quadword Data Right Logical".
//
// Mnemonic        : PSRLQ
// ISA extensions  : MMX, SSE2
// Supported forms : (6 forms)
//
//    * PSRLQ imm8, mm
//    * PSRLQ mm, mm
//    * PSRLQ m64, mm
//    * PSRLQ imm8, xmm
//    * PSRLQ xmm, xmm
//    * PSRLQ m128, xmm
//
func (self *Program) PSRLQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PSRLQ imm8, mm
    if isImm8(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0x0f)
            m.emit(0x73)
            m.emit(0xd0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PSRLQ mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xd3)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSRLQ m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xd3)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PSRLQ imm8, xmm
    if isImm8(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0x0f)
            m.emit(0x73)
            m.emit(0xd0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PSRLQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xd3)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSRLQ m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xd3)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PSRLQ")
    }
    return p
}

// PSRLW performs "Shift Packed Word Data Right Logical".
//
// Mnemonic        : PSRLW
// ISA extensions  : MMX, SSE2
// Supported forms : (6 forms)
//
//    * PSRLW imm8, mm
//    * PSRLW mm, mm
//    * PSRLW m64, mm
//    * PSRLW imm8, xmm
//    * PSRLW xmm, xmm
//    * PSRLW m128, xmm
//
func (self *Program) PSRLW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PSRLW imm8, mm
    if isImm8(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0x0f)
            m.emit(0x71)
            m.emit(0xd0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PSRLW mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xd1)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSRLW m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xd1)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PSRLW imm8, xmm
    if isImm8(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0x0f)
            m.emit(0x71)
            m.emit(0xd0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // PSRLW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xd1)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSRLW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xd1)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PSRLW")
    }
    return p
}

// PSUBB performs "Subtract Packed Byte Integers".
//
// Mnemonic        : PSUBB
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PSUBB mm, mm
//    * PSUBB m64, mm
//    * PSUBB xmm, xmm
//    * PSUBB m128, xmm
//
func (self *Program) PSUBB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PSUBB mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xf8)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSUBB m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xf8)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PSUBB xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xf8)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSUBB m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xf8)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PSUBB")
    }
    return p
}

// PSUBD performs "Subtract Packed Doubleword Integers".
//
// Mnemonic        : PSUBD
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PSUBD mm, mm
//    * PSUBD m64, mm
//    * PSUBD xmm, xmm
//    * PSUBD m128, xmm
//
func (self *Program) PSUBD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PSUBD mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xfa)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSUBD m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xfa)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PSUBD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xfa)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSUBD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xfa)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PSUBD")
    }
    return p
}

// PSUBQ performs "Subtract Packed Quadword Integers".
//
// Mnemonic        : PSUBQ
// ISA extensions  : SSE2
// Supported forms : (4 forms)
//
//    * PSUBQ mm, mm
//    * PSUBQ m64, mm
//    * PSUBQ xmm, xmm
//    * PSUBQ m128, xmm
//
func (self *Program) PSUBQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PSUBQ mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xfb)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSUBQ m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xfb)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PSUBQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xfb)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSUBQ m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xfb)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PSUBQ")
    }
    return p
}

// PSUBSB performs "Subtract Packed Signed Byte Integers with Signed Saturation".
//
// Mnemonic        : PSUBSB
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PSUBSB mm, mm
//    * PSUBSB m64, mm
//    * PSUBSB xmm, xmm
//    * PSUBSB m128, xmm
//
func (self *Program) PSUBSB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PSUBSB mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xe8)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSUBSB m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xe8)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PSUBSB xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xe8)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSUBSB m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xe8)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PSUBSB")
    }
    return p
}

// PSUBSW performs "Subtract Packed Signed Word Integers with Signed Saturation".
//
// Mnemonic        : PSUBSW
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PSUBSW mm, mm
//    * PSUBSW m64, mm
//    * PSUBSW xmm, xmm
//    * PSUBSW m128, xmm
//
func (self *Program) PSUBSW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PSUBSW mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xe9)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSUBSW m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xe9)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PSUBSW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xe9)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSUBSW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xe9)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PSUBSW")
    }
    return p
}

// PSUBUSB performs "Subtract Packed Unsigned Byte Integers with Unsigned Saturation".
//
// Mnemonic        : PSUBUSB
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PSUBUSB mm, mm
//    * PSUBUSB m64, mm
//    * PSUBUSB xmm, xmm
//    * PSUBUSB m128, xmm
//
func (self *Program) PSUBUSB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PSUBUSB mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xd8)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSUBUSB m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xd8)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PSUBUSB xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xd8)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSUBUSB m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xd8)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PSUBUSB")
    }
    return p
}

// PSUBUSW performs "Subtract Packed Unsigned Word Integers with Unsigned Saturation".
//
// Mnemonic        : PSUBUSW
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PSUBUSW mm, mm
//    * PSUBUSW m64, mm
//    * PSUBUSW xmm, xmm
//    * PSUBUSW m128, xmm
//
func (self *Program) PSUBUSW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PSUBUSW mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xd9)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSUBUSW m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xd9)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PSUBUSW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xd9)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSUBUSW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xd9)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PSUBUSW")
    }
    return p
}

// PSUBW performs "Subtract Packed Word Integers".
//
// Mnemonic        : PSUBW
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PSUBW mm, mm
//    * PSUBW m64, mm
//    * PSUBW xmm, xmm
//    * PSUBW m128, xmm
//
func (self *Program) PSUBW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PSUBW mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xf9)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSUBW m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xf9)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PSUBW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xf9)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PSUBW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xf9)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PSUBW")
    }
    return p
}

// PSWAPD performs "Packed Swap Doubleword".
//
// Mnemonic        : PSWAPD
// ISA extensions  : 3dnow!+
// Supported forms : (2 forms)
//
//    * PSWAPD mm, mm
//    * PSWAPD m64, mm
//
func (self *Program) PSWAPD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PSWAPD mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_3DNOW_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
            m.emit(0xbb)
        })
    }
    // PSWAPD m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_3DNOW_PLUS)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x0f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
            m.emit(0xbb)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PSWAPD")
    }
    return p
}

// PTEST performs "Packed Logical Compare".
//
// Mnemonic        : PTEST
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * PTEST xmm, xmm
//    * PTEST m128, xmm
//
func (self *Program) PTEST(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PTEST xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x17)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PTEST m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0x17)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PTEST")
    }
    return p
}

// PUNPCKHBW performs "Unpack and Interleave High-Order Bytes into Words".
//
// Mnemonic        : PUNPCKHBW
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PUNPCKHBW mm, mm
//    * PUNPCKHBW m64, mm
//    * PUNPCKHBW xmm, xmm
//    * PUNPCKHBW m128, xmm
//
func (self *Program) PUNPCKHBW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PUNPCKHBW mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x68)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PUNPCKHBW m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x68)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PUNPCKHBW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x68)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PUNPCKHBW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x68)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PUNPCKHBW")
    }
    return p
}

// PUNPCKHDQ performs "Unpack and Interleave High-Order Doublewords into Quadwords".
//
// Mnemonic        : PUNPCKHDQ
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PUNPCKHDQ mm, mm
//    * PUNPCKHDQ m64, mm
//    * PUNPCKHDQ xmm, xmm
//    * PUNPCKHDQ m128, xmm
//
func (self *Program) PUNPCKHDQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PUNPCKHDQ mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x6a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PUNPCKHDQ m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x6a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PUNPCKHDQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x6a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PUNPCKHDQ m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x6a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PUNPCKHDQ")
    }
    return p
}

// PUNPCKHQDQ performs "Unpack and Interleave High-Order Quadwords into Double Quadwords".
//
// Mnemonic        : PUNPCKHQDQ
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * PUNPCKHQDQ xmm, xmm
//    * PUNPCKHQDQ m128, xmm
//
func (self *Program) PUNPCKHQDQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PUNPCKHQDQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x6d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PUNPCKHQDQ m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x6d)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PUNPCKHQDQ")
    }
    return p
}

// PUNPCKHWD performs "Unpack and Interleave High-Order Words into Doublewords".
//
// Mnemonic        : PUNPCKHWD
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PUNPCKHWD mm, mm
//    * PUNPCKHWD m64, mm
//    * PUNPCKHWD xmm, xmm
//    * PUNPCKHWD m128, xmm
//
func (self *Program) PUNPCKHWD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PUNPCKHWD mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x69)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PUNPCKHWD m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x69)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PUNPCKHWD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x69)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PUNPCKHWD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x69)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PUNPCKHWD")
    }
    return p
}

// PUNPCKLBW performs "Unpack and Interleave Low-Order Bytes into Words".
//
// Mnemonic        : PUNPCKLBW
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PUNPCKLBW mm, mm
//    * PUNPCKLBW m32, mm
//    * PUNPCKLBW xmm, xmm
//    * PUNPCKLBW m128, xmm
//
func (self *Program) PUNPCKLBW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PUNPCKLBW mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x60)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PUNPCKLBW m32, mm
    if isM32(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x60)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PUNPCKLBW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x60)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PUNPCKLBW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x60)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PUNPCKLBW")
    }
    return p
}

// PUNPCKLDQ performs "Unpack and Interleave Low-Order Doublewords into Quadwords".
//
// Mnemonic        : PUNPCKLDQ
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PUNPCKLDQ mm, mm
//    * PUNPCKLDQ m32, mm
//    * PUNPCKLDQ xmm, xmm
//    * PUNPCKLDQ m128, xmm
//
func (self *Program) PUNPCKLDQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PUNPCKLDQ mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x62)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PUNPCKLDQ m32, mm
    if isM32(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x62)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PUNPCKLDQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x62)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PUNPCKLDQ m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x62)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PUNPCKLDQ")
    }
    return p
}

// PUNPCKLQDQ performs "Unpack and Interleave Low-Order Quadwords into Double Quadwords".
//
// Mnemonic        : PUNPCKLQDQ
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * PUNPCKLQDQ xmm, xmm
//    * PUNPCKLQDQ m128, xmm
//
func (self *Program) PUNPCKLQDQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PUNPCKLQDQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x6c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PUNPCKLQDQ m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x6c)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PUNPCKLQDQ")
    }
    return p
}

// PUNPCKLWD performs "Unpack and Interleave Low-Order Words into Doublewords".
//
// Mnemonic        : PUNPCKLWD
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PUNPCKLWD mm, mm
//    * PUNPCKLWD m32, mm
//    * PUNPCKLWD xmm, xmm
//    * PUNPCKLWD m128, xmm
//
func (self *Program) PUNPCKLWD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PUNPCKLWD mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x61)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PUNPCKLWD m32, mm
    if isM32(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x61)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PUNPCKLWD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x61)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PUNPCKLWD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x61)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PUNPCKLWD")
    }
    return p
}

// PUSHQ performs "Push Value Onto the Stack".
//
// Mnemonic        : PUSH
// Supported forms : (4 forms)
//
//    * PUSHQ imm8
//    * PUSHQ imm32
//    * PUSHQ r64
//    * PUSHQ m64
//
func (self *Program) PUSHQ(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // PUSHQ imm8
    if isImm8Ext(v0, 8) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x6a)
            m.imm1(toImmAny(v[0]))
        })
    }
    // PUSHQ imm32
    if isImm32Ext(v0, 8) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x68)
            m.imm4(toImmAny(v[0]))
        })
    }
    // PUSHQ r64
    if isReg64(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], false)
            m.emit(0x50 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], false)
            m.emit(0xff)
            m.emit(0xf0 | lcode(v[0]))
        })
    }
    // PUSHQ m64
    if isM64(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0xff)
            m.mrsd(6, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PUSHQ")
    }
    return p
}

// PUSHW performs "Push Value Onto the Stack".
//
// Mnemonic        : PUSH
// Supported forms : (2 forms)
//
//    * PUSHW r16
//    * PUSHW m16
//
func (self *Program) PUSHW(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // PUSHW r16
    if isReg16(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[0], false)
            m.emit(0x50 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[0], false)
            m.emit(0xff)
            m.emit(0xf0 | lcode(v[0]))
        })
    }
    // PUSHW m16
    if isM16(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[0]), false)
            m.emit(0xff)
            m.mrsd(6, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PUSHW")
    }
    return p
}

// PXOR performs "Packed Bitwise Logical Exclusive OR".
//
// Mnemonic        : PXOR
// ISA extensions  : MMX, SSE2
// Supported forms : (4 forms)
//
//    * PXOR mm, mm
//    * PXOR m64, mm
//    * PXOR xmm, xmm
//    * PXOR m128, xmm
//
func (self *Program) PXOR(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // PXOR mm, mm
    if isMM(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xef)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PXOR m64, mm
    if isM64(v0) && isMM(v1) {
        self.require(ISA_MMX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xef)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // PXOR xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xef)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // PXOR m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xef)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for PXOR")
    }
    return p
}

// RCLB performs "Rotate Left through Carry Flag".
//
// Mnemonic        : RCL
// Supported forms : (6 forms)
//
//    * RCLB 1, r8
//    * RCLB imm8, r8
//    * RCLB cl, r8
//    * RCLB 1, m8
//    * RCLB imm8, m8
//    * RCLB cl, m8
//
func (self *Program) RCLB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // RCLB 1, r8
    if isConst1(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0xd0)
            m.emit(0xd0 | lcode(v[1]))
        })
    }
    // RCLB imm8, r8
    if isImm8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0xc0)
            m.emit(0xd0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // RCLB cl, r8
    if v0 == CL && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0xd2)
            m.emit(0xd0 | lcode(v[1]))
        })
    }
    // RCLB 1, m8
    if isConst1(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd0)
            m.mrsd(2, addr(v[1]), 1)
        })
    }
    // RCLB imm8, m8
    if isImm8(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xc0)
            m.mrsd(2, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // RCLB cl, m8
    if v0 == CL && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd2)
            m.mrsd(2, addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for RCLB")
    }
    return p
}

// RCLL performs "Rotate Left through Carry Flag".
//
// Mnemonic        : RCL
// Supported forms : (6 forms)
//
//    * RCLL 1, r32
//    * RCLL imm8, r32
//    * RCLL cl, r32
//    * RCLL 1, m32
//    * RCLL imm8, m32
//    * RCLL cl, m32
//
func (self *Program) RCLL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // RCLL 1, r32
    if isConst1(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0xd1)
            m.emit(0xd0 | lcode(v[1]))
        })
    }
    // RCLL imm8, r32
    if isImm8(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0xc1)
            m.emit(0xd0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // RCLL cl, r32
    if v0 == CL && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0xd3)
            m.emit(0xd0 | lcode(v[1]))
        })
    }
    // RCLL 1, m32
    if isConst1(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd1)
            m.mrsd(2, addr(v[1]), 1)
        })
    }
    // RCLL imm8, m32
    if isImm8(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xc1)
            m.mrsd(2, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // RCLL cl, m32
    if v0 == CL && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd3)
            m.mrsd(2, addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for RCLL")
    }
    return p
}

// RCLQ performs "Rotate Left through Carry Flag".
//
// Mnemonic        : RCL
// Supported forms : (6 forms)
//
//    * RCLQ 1, r64
//    * RCLQ imm8, r64
//    * RCLQ cl, r64
//    * RCLQ 1, m64
//    * RCLQ imm8, m64
//    * RCLQ cl, m64
//
func (self *Program) RCLQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // RCLQ 1, r64
    if isConst1(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0xd1)
            m.emit(0xd0 | lcode(v[1]))
        })
    }
    // RCLQ imm8, r64
    if isImm8(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0xc1)
            m.emit(0xd0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // RCLQ cl, r64
    if v0 == CL && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0xd3)
            m.emit(0xd0 | lcode(v[1]))
        })
    }
    // RCLQ 1, m64
    if isConst1(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0xd1)
            m.mrsd(2, addr(v[1]), 1)
        })
    }
    // RCLQ imm8, m64
    if isImm8(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0xc1)
            m.mrsd(2, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // RCLQ cl, m64
    if v0 == CL && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0xd3)
            m.mrsd(2, addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for RCLQ")
    }
    return p
}

// RCLW performs "Rotate Left through Carry Flag".
//
// Mnemonic        : RCL
// Supported forms : (6 forms)
//
//    * RCLW 1, r16
//    * RCLW imm8, r16
//    * RCLW cl, r16
//    * RCLW 1, m16
//    * RCLW imm8, m16
//    * RCLW cl, m16
//
func (self *Program) RCLW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // RCLW 1, r16
    if isConst1(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0xd1)
            m.emit(0xd0 | lcode(v[1]))
        })
    }
    // RCLW imm8, r16
    if isImm8(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0xc1)
            m.emit(0xd0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // RCLW cl, r16
    if v0 == CL && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0xd3)
            m.emit(0xd0 | lcode(v[1]))
        })
    }
    // RCLW 1, m16
    if isConst1(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd1)
            m.mrsd(2, addr(v[1]), 1)
        })
    }
    // RCLW imm8, m16
    if isImm8(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0xc1)
            m.mrsd(2, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // RCLW cl, m16
    if v0 == CL && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd3)
            m.mrsd(2, addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for RCLW")
    }
    return p
}

// RCPPS performs "Compute Approximate Reciprocals of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : RCPPS
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * RCPPS xmm, xmm
//    * RCPPS m128, xmm
//
func (self *Program) RCPPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // RCPPS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x53)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // RCPPS m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x53)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for RCPPS")
    }
    return p
}

// RCPSS performs "Compute Approximate Reciprocal of Scalar Single-Precision Floating-Point Values".
//
// Mnemonic        : RCPSS
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * RCPSS xmm, xmm
//    * RCPSS m32, xmm
//
func (self *Program) RCPSS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // RCPSS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x53)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // RCPSS m32, xmm
    if isM32(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x53)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for RCPSS")
    }
    return p
}

// RCRB performs "Rotate Right through Carry Flag".
//
// Mnemonic        : RCR
// Supported forms : (6 forms)
//
//    * RCRB 1, r8
//    * RCRB imm8, r8
//    * RCRB cl, r8
//    * RCRB 1, m8
//    * RCRB imm8, m8
//    * RCRB cl, m8
//
func (self *Program) RCRB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // RCRB 1, r8
    if isConst1(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0xd0)
            m.emit(0xd8 | lcode(v[1]))
        })
    }
    // RCRB imm8, r8
    if isImm8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0xc0)
            m.emit(0xd8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // RCRB cl, r8
    if v0 == CL && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0xd2)
            m.emit(0xd8 | lcode(v[1]))
        })
    }
    // RCRB 1, m8
    if isConst1(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd0)
            m.mrsd(3, addr(v[1]), 1)
        })
    }
    // RCRB imm8, m8
    if isImm8(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xc0)
            m.mrsd(3, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // RCRB cl, m8
    if v0 == CL && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd2)
            m.mrsd(3, addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for RCRB")
    }
    return p
}

// RCRL performs "Rotate Right through Carry Flag".
//
// Mnemonic        : RCR
// Supported forms : (6 forms)
//
//    * RCRL 1, r32
//    * RCRL imm8, r32
//    * RCRL cl, r32
//    * RCRL 1, m32
//    * RCRL imm8, m32
//    * RCRL cl, m32
//
func (self *Program) RCRL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // RCRL 1, r32
    if isConst1(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0xd1)
            m.emit(0xd8 | lcode(v[1]))
        })
    }
    // RCRL imm8, r32
    if isImm8(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0xc1)
            m.emit(0xd8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // RCRL cl, r32
    if v0 == CL && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0xd3)
            m.emit(0xd8 | lcode(v[1]))
        })
    }
    // RCRL 1, m32
    if isConst1(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd1)
            m.mrsd(3, addr(v[1]), 1)
        })
    }
    // RCRL imm8, m32
    if isImm8(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xc1)
            m.mrsd(3, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // RCRL cl, m32
    if v0 == CL && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd3)
            m.mrsd(3, addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for RCRL")
    }
    return p
}

// RCRQ performs "Rotate Right through Carry Flag".
//
// Mnemonic        : RCR
// Supported forms : (6 forms)
//
//    * RCRQ 1, r64
//    * RCRQ imm8, r64
//    * RCRQ cl, r64
//    * RCRQ 1, m64
//    * RCRQ imm8, m64
//    * RCRQ cl, m64
//
func (self *Program) RCRQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // RCRQ 1, r64
    if isConst1(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0xd1)
            m.emit(0xd8 | lcode(v[1]))
        })
    }
    // RCRQ imm8, r64
    if isImm8(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0xc1)
            m.emit(0xd8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // RCRQ cl, r64
    if v0 == CL && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0xd3)
            m.emit(0xd8 | lcode(v[1]))
        })
    }
    // RCRQ 1, m64
    if isConst1(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0xd1)
            m.mrsd(3, addr(v[1]), 1)
        })
    }
    // RCRQ imm8, m64
    if isImm8(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0xc1)
            m.mrsd(3, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // RCRQ cl, m64
    if v0 == CL && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0xd3)
            m.mrsd(3, addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for RCRQ")
    }
    return p
}

// RCRW performs "Rotate Right through Carry Flag".
//
// Mnemonic        : RCR
// Supported forms : (6 forms)
//
//    * RCRW 1, r16
//    * RCRW imm8, r16
//    * RCRW cl, r16
//    * RCRW 1, m16
//    * RCRW imm8, m16
//    * RCRW cl, m16
//
func (self *Program) RCRW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // RCRW 1, r16
    if isConst1(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0xd1)
            m.emit(0xd8 | lcode(v[1]))
        })
    }
    // RCRW imm8, r16
    if isImm8(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0xc1)
            m.emit(0xd8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // RCRW cl, r16
    if v0 == CL && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0xd3)
            m.emit(0xd8 | lcode(v[1]))
        })
    }
    // RCRW 1, m16
    if isConst1(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd1)
            m.mrsd(3, addr(v[1]), 1)
        })
    }
    // RCRW imm8, m16
    if isImm8(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0xc1)
            m.mrsd(3, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // RCRW cl, m16
    if v0 == CL && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd3)
            m.mrsd(3, addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for RCRW")
    }
    return p
}

// RDRAND performs "Read Random Number".
//
// Mnemonic        : RDRAND
// ISA extensions  : RDRAND
// Supported forms : (3 forms)
//
//    * RDRAND r16
//    * RDRAND r32
//    * RDRAND r64
//
func (self *Program) RDRAND(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // RDRAND r16
    if isReg16(v0) {
        self.require(ISA_RDRAND)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[0], false)
            m.emit(0x0f)
            m.emit(0xc7)
            m.emit(0xf0 | lcode(v[0]))
        })
    }
    // RDRAND r32
    if isReg32(v0) {
        self.require(ISA_RDRAND)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], false)
            m.emit(0x0f)
            m.emit(0xc7)
            m.emit(0xf0 | lcode(v[0]))
        })
    }
    // RDRAND r64
    if isReg64(v0) {
        self.require(ISA_RDRAND)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0xc7)
            m.emit(0xf0 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for RDRAND")
    }
    return p
}

// RDSEED performs "Read Random SEED".
//
// Mnemonic        : RDSEED
// ISA extensions  : RDSEED
// Supported forms : (3 forms)
//
//    * RDSEED r16
//    * RDSEED r32
//    * RDSEED r64
//
func (self *Program) RDSEED(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // RDSEED r16
    if isReg16(v0) {
        self.require(ISA_RDSEED)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[0], false)
            m.emit(0x0f)
            m.emit(0xc7)
            m.emit(0xf8 | lcode(v[0]))
        })
    }
    // RDSEED r32
    if isReg32(v0) {
        self.require(ISA_RDSEED)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], false)
            m.emit(0x0f)
            m.emit(0xc7)
            m.emit(0xf8 | lcode(v[0]))
        })
    }
    // RDSEED r64
    if isReg64(v0) {
        self.require(ISA_RDSEED)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0xc7)
            m.emit(0xf8 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for RDSEED")
    }
    return p
}

// RDTSC performs "Read Time-Stamp Counter".
//
// Mnemonic        : RDTSC
// ISA extensions  : RDTSC
// Supported forms : (1 form)
//
//    * RDTSC
//
func (self *Program) RDTSC() *Instruction {
    p := self.alloc(0, Operands{})
    // RDTSC
    self.require(ISA_RDTSC)
    p.add(0, func(m *_Encoding, v []interface{}) {
        m.emit(0x0f)
        m.emit(0x31)
    })
    return p
}

// RDTSCP performs "Read Time-Stamp Counter and Processor ID".
//
// Mnemonic        : RDTSCP
// ISA extensions  : RDTSCP
// Supported forms : (1 form)
//
//    * RDTSCP
//
func (self *Program) RDTSCP() *Instruction {
    p := self.alloc(0, Operands{})
    // RDTSCP
    self.require(ISA_RDTSCP)
    p.add(0, func(m *_Encoding, v []interface{}) {
        m.emit(0x0f)
        m.emit(0x01)
        m.emit(0xf9)
    })
    return p
}

// RET performs "Return from Procedure".
//
// Mnemonic        : RET
// Supported forms : (2 forms)
//
//    * RET
//    * RET imm16
//
func (self *Program) RET(vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(0, Operands{})
        case 1  : p = self.alloc(1, Operands{vv[0]})
        default : panic("instruction RET takes 0 or 1 operands")
    }
    // RET
    if len(vv) == 0 {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc3)
        })
    }
    // RET imm16
    if len(vv) == 1 && isImm16(vv[0]) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc2)
            m.imm2(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for RET")
    }
    return p
}

// ROLB performs "Rotate Left".
//
// Mnemonic        : ROL
// Supported forms : (6 forms)
//
//    * ROLB 1, r8
//    * ROLB imm8, r8
//    * ROLB cl, r8
//    * ROLB 1, m8
//    * ROLB imm8, m8
//    * ROLB cl, m8
//
func (self *Program) ROLB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ROLB 1, r8
    if isConst1(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0xd0)
            m.emit(0xc0 | lcode(v[1]))
        })
    }
    // ROLB imm8, r8
    if isImm8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0xc0)
            m.emit(0xc0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // ROLB cl, r8
    if v0 == CL && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0xd2)
            m.emit(0xc0 | lcode(v[1]))
        })
    }
    // ROLB 1, m8
    if isConst1(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd0)
            m.mrsd(0, addr(v[1]), 1)
        })
    }
    // ROLB imm8, m8
    if isImm8(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xc0)
            m.mrsd(0, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // ROLB cl, m8
    if v0 == CL && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd2)
            m.mrsd(0, addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ROLB")
    }
    return p
}

// ROLL performs "Rotate Left".
//
// Mnemonic        : ROL
// Supported forms : (6 forms)
//
//    * ROLL 1, r32
//    * ROLL imm8, r32
//    * ROLL cl, r32
//    * ROLL 1, m32
//    * ROLL imm8, m32
//    * ROLL cl, m32
//
func (self *Program) ROLL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ROLL 1, r32
    if isConst1(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0xd1)
            m.emit(0xc0 | lcode(v[1]))
        })
    }
    // ROLL imm8, r32
    if isImm8(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0xc1)
            m.emit(0xc0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // ROLL cl, r32
    if v0 == CL && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0xd3)
            m.emit(0xc0 | lcode(v[1]))
        })
    }
    // ROLL 1, m32
    if isConst1(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd1)
            m.mrsd(0, addr(v[1]), 1)
        })
    }
    // ROLL imm8, m32
    if isImm8(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xc1)
            m.mrsd(0, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // ROLL cl, m32
    if v0 == CL && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd3)
            m.mrsd(0, addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ROLL")
    }
    return p
}

// ROLQ performs "Rotate Left".
//
// Mnemonic        : ROL
// Supported forms : (6 forms)
//
//    * ROLQ 1, r64
//    * ROLQ imm8, r64
//    * ROLQ cl, r64
//    * ROLQ 1, m64
//    * ROLQ imm8, m64
//    * ROLQ cl, m64
//
func (self *Program) ROLQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ROLQ 1, r64
    if isConst1(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0xd1)
            m.emit(0xc0 | lcode(v[1]))
        })
    }
    // ROLQ imm8, r64
    if isImm8(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0xc1)
            m.emit(0xc0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // ROLQ cl, r64
    if v0 == CL && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0xd3)
            m.emit(0xc0 | lcode(v[1]))
        })
    }
    // ROLQ 1, m64
    if isConst1(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0xd1)
            m.mrsd(0, addr(v[1]), 1)
        })
    }
    // ROLQ imm8, m64
    if isImm8(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0xc1)
            m.mrsd(0, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // ROLQ cl, m64
    if v0 == CL && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0xd3)
            m.mrsd(0, addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ROLQ")
    }
    return p
}

// ROLW performs "Rotate Left".
//
// Mnemonic        : ROL
// Supported forms : (6 forms)
//
//    * ROLW 1, r16
//    * ROLW imm8, r16
//    * ROLW cl, r16
//    * ROLW 1, m16
//    * ROLW imm8, m16
//    * ROLW cl, m16
//
func (self *Program) ROLW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // ROLW 1, r16
    if isConst1(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0xd1)
            m.emit(0xc0 | lcode(v[1]))
        })
    }
    // ROLW imm8, r16
    if isImm8(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0xc1)
            m.emit(0xc0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // ROLW cl, r16
    if v0 == CL && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0xd3)
            m.emit(0xc0 | lcode(v[1]))
        })
    }
    // ROLW 1, m16
    if isConst1(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd1)
            m.mrsd(0, addr(v[1]), 1)
        })
    }
    // ROLW imm8, m16
    if isImm8(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0xc1)
            m.mrsd(0, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // ROLW cl, m16
    if v0 == CL && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd3)
            m.mrsd(0, addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for ROLW")
    }
    return p
}

// RORB performs "Rotate Right".
//
// Mnemonic        : ROR
// Supported forms : (6 forms)
//
//    * RORB 1, r8
//    * RORB imm8, r8
//    * RORB cl, r8
//    * RORB 1, m8
//    * RORB imm8, m8
//    * RORB cl, m8
//
func (self *Program) RORB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // RORB 1, r8
    if isConst1(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0xd0)
            m.emit(0xc8 | lcode(v[1]))
        })
    }
    // RORB imm8, r8
    if isImm8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0xc0)
            m.emit(0xc8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // RORB cl, r8
    if v0 == CL && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0xd2)
            m.emit(0xc8 | lcode(v[1]))
        })
    }
    // RORB 1, m8
    if isConst1(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd0)
            m.mrsd(1, addr(v[1]), 1)
        })
    }
    // RORB imm8, m8
    if isImm8(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xc0)
            m.mrsd(1, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // RORB cl, m8
    if v0 == CL && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd2)
            m.mrsd(1, addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for RORB")
    }
    return p
}

// RORL performs "Rotate Right".
//
// Mnemonic        : ROR
// Supported forms : (6 forms)
//
//    * RORL 1, r32
//    * RORL imm8, r32
//    * RORL cl, r32
//    * RORL 1, m32
//    * RORL imm8, m32
//    * RORL cl, m32
//
func (self *Program) RORL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // RORL 1, r32
    if isConst1(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0xd1)
            m.emit(0xc8 | lcode(v[1]))
        })
    }
    // RORL imm8, r32
    if isImm8(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0xc1)
            m.emit(0xc8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // RORL cl, r32
    if v0 == CL && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0xd3)
            m.emit(0xc8 | lcode(v[1]))
        })
    }
    // RORL 1, m32
    if isConst1(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd1)
            m.mrsd(1, addr(v[1]), 1)
        })
    }
    // RORL imm8, m32
    if isImm8(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xc1)
            m.mrsd(1, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // RORL cl, m32
    if v0 == CL && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd3)
            m.mrsd(1, addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for RORL")
    }
    return p
}

// RORQ performs "Rotate Right".
//
// Mnemonic        : ROR
// Supported forms : (6 forms)
//
//    * RORQ 1, r64
//    * RORQ imm8, r64
//    * RORQ cl, r64
//    * RORQ 1, m64
//    * RORQ imm8, m64
//    * RORQ cl, m64
//
func (self *Program) RORQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // RORQ 1, r64
    if isConst1(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0xd1)
            m.emit(0xc8 | lcode(v[1]))
        })
    }
    // RORQ imm8, r64
    if isImm8(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0xc1)
            m.emit(0xc8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // RORQ cl, r64
    if v0 == CL && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0xd3)
            m.emit(0xc8 | lcode(v[1]))
        })
    }
    // RORQ 1, m64
    if isConst1(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0xd1)
            m.mrsd(1, addr(v[1]), 1)
        })
    }
    // RORQ imm8, m64
    if isImm8(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0xc1)
            m.mrsd(1, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // RORQ cl, m64
    if v0 == CL && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0xd3)
            m.mrsd(1, addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for RORQ")
    }
    return p
}

// RORW performs "Rotate Right".
//
// Mnemonic        : ROR
// Supported forms : (6 forms)
//
//    * RORW 1, r16
//    * RORW imm8, r16
//    * RORW cl, r16
//    * RORW 1, m16
//    * RORW imm8, m16
//    * RORW cl, m16
//
func (self *Program) RORW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // RORW 1, r16
    if isConst1(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0xd1)
            m.emit(0xc8 | lcode(v[1]))
        })
    }
    // RORW imm8, r16
    if isImm8(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0xc1)
            m.emit(0xc8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // RORW cl, r16
    if v0 == CL && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0xd3)
            m.emit(0xc8 | lcode(v[1]))
        })
    }
    // RORW 1, m16
    if isConst1(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd1)
            m.mrsd(1, addr(v[1]), 1)
        })
    }
    // RORW imm8, m16
    if isImm8(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0xc1)
            m.mrsd(1, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // RORW cl, m16
    if v0 == CL && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd3)
            m.mrsd(1, addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for RORW")
    }
    return p
}

// RORXL performs "Rotate Right Logical Without Affecting Flags".
//
// Mnemonic        : RORX
// ISA extensions  : BMI2
// Supported forms : (2 forms)
//
//    * RORXL imm8, r32, r32
//    * RORXL imm8, m32, r32
//
func (self *Program) RORXL(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // RORXL imm8, r32, r32
    if isImm8(v0) && isReg32(v1) && isReg32(v2) {
        self.require(ISA_BMI2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7b)
            m.emit(0xf0)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // RORXL imm8, m32, r32
    if isImm8(v0) && isM32(v1) && isReg32(v2) {
        self.require(ISA_BMI2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x03, hcode(v[2]), addr(v[1]), 0)
            m.emit(0xf0)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for RORXL")
    }
    return p
}

// RORXQ performs "Rotate Right Logical Without Affecting Flags".
//
// Mnemonic        : RORX
// ISA extensions  : BMI2
// Supported forms : (2 forms)
//
//    * RORXQ imm8, r64, r64
//    * RORXQ imm8, m64, r64
//
func (self *Program) RORXQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // RORXQ imm8, r64, r64
    if isImm8(v0) && isReg64(v1) && isReg64(v2) {
        self.require(ISA_BMI2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0xfb)
            m.emit(0xf0)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // RORXQ imm8, m64, r64
    if isImm8(v0) && isM64(v1) && isReg64(v2) {
        self.require(ISA_BMI2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x83, hcode(v[2]), addr(v[1]), 0)
            m.emit(0xf0)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for RORXQ")
    }
    return p
}

// ROUNDPD performs "Round Packed Double Precision Floating-Point Values".
//
// Mnemonic        : ROUNDPD
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * ROUNDPD imm8, xmm, xmm
//    * ROUNDPD imm8, m128, xmm
//
func (self *Program) ROUNDPD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // ROUNDPD imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x09)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // ROUNDPD imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x09)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for ROUNDPD")
    }
    return p
}

// ROUNDPS performs "Round Packed Single Precision Floating-Point Values".
//
// Mnemonic        : ROUNDPS
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * ROUNDPS imm8, xmm, xmm
//    * ROUNDPS imm8, m128, xmm
//
func (self *Program) ROUNDPS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // ROUNDPS imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x08)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // ROUNDPS imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x08)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for ROUNDPS")
    }
    return p
}

// ROUNDSD performs "Round Scalar Double Precision Floating-Point Values".
//
// Mnemonic        : ROUNDSD
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * ROUNDSD imm8, xmm, xmm
//    * ROUNDSD imm8, m64, xmm
//
func (self *Program) ROUNDSD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // ROUNDSD imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x0b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // ROUNDSD imm8, m64, xmm
    if isImm8(v0) && isM64(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x0b)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for ROUNDSD")
    }
    return p
}

// ROUNDSS performs "Round Scalar Single Precision Floating-Point Values".
//
// Mnemonic        : ROUNDSS
// ISA extensions  : SSE4.1
// Supported forms : (2 forms)
//
//    * ROUNDSS imm8, xmm, xmm
//    * ROUNDSS imm8, m32, xmm
//
func (self *Program) ROUNDSS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // ROUNDSS imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x0a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // ROUNDSS imm8, m32, xmm
    if isImm8(v0) && isM32(v1) && isXMM(v2) {
        self.require(ISA_SSE4_1)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0x0a)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for ROUNDSS")
    }
    return p
}

// RSQRTPS performs "Compute Reciprocals of Square Roots of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : RSQRTPS
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * RSQRTPS xmm, xmm
//    * RSQRTPS m128, xmm
//
func (self *Program) RSQRTPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // RSQRTPS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x52)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // RSQRTPS m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x52)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for RSQRTPS")
    }
    return p
}

// RSQRTSS performs "Compute Reciprocal of Square Root of Scalar Single-Precision Floating-Point Value".
//
// Mnemonic        : RSQRTSS
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * RSQRTSS xmm, xmm
//    * RSQRTSS m32, xmm
//
func (self *Program) RSQRTSS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // RSQRTSS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x52)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // RSQRTSS m32, xmm
    if isM32(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x52)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for RSQRTSS")
    }
    return p
}

// SALB performs "Arithmetic Shift Left".
//
// Mnemonic        : SAL
// Supported forms : (6 forms)
//
//    * SALB 1, r8
//    * SALB imm8, r8
//    * SALB cl, r8
//    * SALB 1, m8
//    * SALB imm8, m8
//    * SALB cl, m8
//
func (self *Program) SALB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SALB 1, r8
    if isConst1(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0xd0)
            m.emit(0xe0 | lcode(v[1]))
        })
    }
    // SALB imm8, r8
    if isImm8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0xc0)
            m.emit(0xe0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // SALB cl, r8
    if v0 == CL && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0xd2)
            m.emit(0xe0 | lcode(v[1]))
        })
    }
    // SALB 1, m8
    if isConst1(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd0)
            m.mrsd(4, addr(v[1]), 1)
        })
    }
    // SALB imm8, m8
    if isImm8(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xc0)
            m.mrsd(4, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // SALB cl, m8
    if v0 == CL && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd2)
            m.mrsd(4, addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SALB")
    }
    return p
}

// SALL performs "Arithmetic Shift Left".
//
// Mnemonic        : SAL
// Supported forms : (6 forms)
//
//    * SALL 1, r32
//    * SALL imm8, r32
//    * SALL cl, r32
//    * SALL 1, m32
//    * SALL imm8, m32
//    * SALL cl, m32
//
func (self *Program) SALL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SALL 1, r32
    if isConst1(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0xd1)
            m.emit(0xe0 | lcode(v[1]))
        })
    }
    // SALL imm8, r32
    if isImm8(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0xc1)
            m.emit(0xe0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // SALL cl, r32
    if v0 == CL && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0xd3)
            m.emit(0xe0 | lcode(v[1]))
        })
    }
    // SALL 1, m32
    if isConst1(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd1)
            m.mrsd(4, addr(v[1]), 1)
        })
    }
    // SALL imm8, m32
    if isImm8(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xc1)
            m.mrsd(4, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // SALL cl, m32
    if v0 == CL && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd3)
            m.mrsd(4, addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SALL")
    }
    return p
}

// SALQ performs "Arithmetic Shift Left".
//
// Mnemonic        : SAL
// Supported forms : (6 forms)
//
//    * SALQ 1, r64
//    * SALQ imm8, r64
//    * SALQ cl, r64
//    * SALQ 1, m64
//    * SALQ imm8, m64
//    * SALQ cl, m64
//
func (self *Program) SALQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SALQ 1, r64
    if isConst1(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0xd1)
            m.emit(0xe0 | lcode(v[1]))
        })
    }
    // SALQ imm8, r64
    if isImm8(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0xc1)
            m.emit(0xe0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // SALQ cl, r64
    if v0 == CL && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0xd3)
            m.emit(0xe0 | lcode(v[1]))
        })
    }
    // SALQ 1, m64
    if isConst1(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0xd1)
            m.mrsd(4, addr(v[1]), 1)
        })
    }
    // SALQ imm8, m64
    if isImm8(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0xc1)
            m.mrsd(4, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // SALQ cl, m64
    if v0 == CL && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0xd3)
            m.mrsd(4, addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SALQ")
    }
    return p
}

// SALW performs "Arithmetic Shift Left".
//
// Mnemonic        : SAL
// Supported forms : (6 forms)
//
//    * SALW 1, r16
//    * SALW imm8, r16
//    * SALW cl, r16
//    * SALW 1, m16
//    * SALW imm8, m16
//    * SALW cl, m16
//
func (self *Program) SALW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SALW 1, r16
    if isConst1(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0xd1)
            m.emit(0xe0 | lcode(v[1]))
        })
    }
    // SALW imm8, r16
    if isImm8(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0xc1)
            m.emit(0xe0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // SALW cl, r16
    if v0 == CL && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0xd3)
            m.emit(0xe0 | lcode(v[1]))
        })
    }
    // SALW 1, m16
    if isConst1(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd1)
            m.mrsd(4, addr(v[1]), 1)
        })
    }
    // SALW imm8, m16
    if isImm8(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0xc1)
            m.mrsd(4, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // SALW cl, m16
    if v0 == CL && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd3)
            m.mrsd(4, addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SALW")
    }
    return p
}

// SARB performs "Arithmetic Shift Right".
//
// Mnemonic        : SAR
// Supported forms : (6 forms)
//
//    * SARB 1, r8
//    * SARB imm8, r8
//    * SARB cl, r8
//    * SARB 1, m8
//    * SARB imm8, m8
//    * SARB cl, m8
//
func (self *Program) SARB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SARB 1, r8
    if isConst1(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0xd0)
            m.emit(0xf8 | lcode(v[1]))
        })
    }
    // SARB imm8, r8
    if isImm8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0xc0)
            m.emit(0xf8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // SARB cl, r8
    if v0 == CL && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0xd2)
            m.emit(0xf8 | lcode(v[1]))
        })
    }
    // SARB 1, m8
    if isConst1(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd0)
            m.mrsd(7, addr(v[1]), 1)
        })
    }
    // SARB imm8, m8
    if isImm8(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xc0)
            m.mrsd(7, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // SARB cl, m8
    if v0 == CL && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd2)
            m.mrsd(7, addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SARB")
    }
    return p
}

// SARL performs "Arithmetic Shift Right".
//
// Mnemonic        : SAR
// Supported forms : (6 forms)
//
//    * SARL 1, r32
//    * SARL imm8, r32
//    * SARL cl, r32
//    * SARL 1, m32
//    * SARL imm8, m32
//    * SARL cl, m32
//
func (self *Program) SARL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SARL 1, r32
    if isConst1(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0xd1)
            m.emit(0xf8 | lcode(v[1]))
        })
    }
    // SARL imm8, r32
    if isImm8(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0xc1)
            m.emit(0xf8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // SARL cl, r32
    if v0 == CL && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0xd3)
            m.emit(0xf8 | lcode(v[1]))
        })
    }
    // SARL 1, m32
    if isConst1(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd1)
            m.mrsd(7, addr(v[1]), 1)
        })
    }
    // SARL imm8, m32
    if isImm8(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xc1)
            m.mrsd(7, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // SARL cl, m32
    if v0 == CL && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd3)
            m.mrsd(7, addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SARL")
    }
    return p
}

// SARQ performs "Arithmetic Shift Right".
//
// Mnemonic        : SAR
// Supported forms : (6 forms)
//
//    * SARQ 1, r64
//    * SARQ imm8, r64
//    * SARQ cl, r64
//    * SARQ 1, m64
//    * SARQ imm8, m64
//    * SARQ cl, m64
//
func (self *Program) SARQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SARQ 1, r64
    if isConst1(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0xd1)
            m.emit(0xf8 | lcode(v[1]))
        })
    }
    // SARQ imm8, r64
    if isImm8(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0xc1)
            m.emit(0xf8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // SARQ cl, r64
    if v0 == CL && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0xd3)
            m.emit(0xf8 | lcode(v[1]))
        })
    }
    // SARQ 1, m64
    if isConst1(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0xd1)
            m.mrsd(7, addr(v[1]), 1)
        })
    }
    // SARQ imm8, m64
    if isImm8(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0xc1)
            m.mrsd(7, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // SARQ cl, m64
    if v0 == CL && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0xd3)
            m.mrsd(7, addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SARQ")
    }
    return p
}

// SARW performs "Arithmetic Shift Right".
//
// Mnemonic        : SAR
// Supported forms : (6 forms)
//
//    * SARW 1, r16
//    * SARW imm8, r16
//    * SARW cl, r16
//    * SARW 1, m16
//    * SARW imm8, m16
//    * SARW cl, m16
//
func (self *Program) SARW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SARW 1, r16
    if isConst1(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0xd1)
            m.emit(0xf8 | lcode(v[1]))
        })
    }
    // SARW imm8, r16
    if isImm8(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0xc1)
            m.emit(0xf8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // SARW cl, r16
    if v0 == CL && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0xd3)
            m.emit(0xf8 | lcode(v[1]))
        })
    }
    // SARW 1, m16
    if isConst1(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd1)
            m.mrsd(7, addr(v[1]), 1)
        })
    }
    // SARW imm8, m16
    if isImm8(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0xc1)
            m.mrsd(7, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // SARW cl, m16
    if v0 == CL && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd3)
            m.mrsd(7, addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SARW")
    }
    return p
}

// SARXL performs "Arithmetic Shift Right Without Affecting Flags".
//
// Mnemonic        : SARX
// ISA extensions  : BMI2
// Supported forms : (2 forms)
//
//    * SARXL r32, r32, r32
//    * SARXL r32, m32, r32
//
func (self *Program) SARXL(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // SARXL r32, r32, r32
    if isReg32(v0) && isReg32(v1) && isReg32(v2) {
        self.require(ISA_BMI2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7a ^ (hlcode(v[0]) << 3))
            m.emit(0xf7)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // SARXL r32, m32, r32
    if isReg32(v0) && isM32(v1) && isReg32(v2) {
        self.require(ISA_BMI2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x02, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0xf7)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SARXL")
    }
    return p
}

// SARXQ performs "Arithmetic Shift Right Without Affecting Flags".
//
// Mnemonic        : SARX
// ISA extensions  : BMI2
// Supported forms : (2 forms)
//
//    * SARXQ r64, r64, r64
//    * SARXQ r64, m64, r64
//
func (self *Program) SARXQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // SARXQ r64, r64, r64
    if isReg64(v0) && isReg64(v1) && isReg64(v2) {
        self.require(ISA_BMI2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0xfa ^ (hlcode(v[0]) << 3))
            m.emit(0xf7)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // SARXQ r64, m64, r64
    if isReg64(v0) && isM64(v1) && isReg64(v2) {
        self.require(ISA_BMI2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x82, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0xf7)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SARXQ")
    }
    return p
}

// SBBB performs "Subtract with Borrow".
//
// Mnemonic        : SBB
// Supported forms : (6 forms)
//
//    * SBBB imm8, al
//    * SBBB imm8, r8
//    * SBBB r8, r8
//    * SBBB m8, r8
//    * SBBB imm8, m8
//    * SBBB r8, m8
//
func (self *Program) SBBB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SBBB imm8, al
    if isImm8(v0) && v1 == AL {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x1c)
            m.imm1(toImmAny(v[0]))
        })
    }
    // SBBB imm8, r8
    if isImm8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0x80)
            m.emit(0xd8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // SBBB r8, r8
    if isReg8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], isReg8REX(v[0]) || isReg8REX(v[1]))
            m.emit(0x18)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], isReg8REX(v[0]) || isReg8REX(v[1]))
            m.emit(0x1a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // SBBB m8, r8
    if isM8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), isReg8REX(v[1]))
            m.emit(0x1a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // SBBB imm8, m8
    if isImm8(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0x80)
            m.mrsd(3, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // SBBB r8, m8
    if isReg8(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), isReg8REX(v[0]))
            m.emit(0x18)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SBBB")
    }
    return p
}

// SBBL performs "Subtract with Borrow".
//
// Mnemonic        : SBB
// Supported forms : (8 forms)
//
//    * SBBL imm32, eax
//    * SBBL imm8, r32
//    * SBBL imm32, r32
//    * SBBL r32, r32
//    * SBBL m32, r32
//    * SBBL imm8, m32
//    * SBBL imm32, m32
//    * SBBL r32, m32
//
func (self *Program) SBBL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SBBL imm32, eax
    if isImm32(v0) && v1 == EAX {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x1d)
            m.imm4(toImmAny(v[0]))
        })
    }
    // SBBL imm8, r32
    if isImm8Ext(v0, 4) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0x83)
            m.emit(0xd8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // SBBL imm32, r32
    if isImm32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0x81)
            m.emit(0xd8 | lcode(v[1]))
            m.imm4(toImmAny(v[0]))
        })
    }
    // SBBL r32, r32
    if isReg32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x19)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x1b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // SBBL m32, r32
    if isM32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x1b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // SBBL imm8, m32
    if isImm8Ext(v0, 4) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0x83)
            m.mrsd(3, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // SBBL imm32, m32
    if isImm32(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0x81)
            m.mrsd(3, addr(v[1]), 1)
            m.imm4(toImmAny(v[0]))
        })
    }
    // SBBL r32, m32
    if isReg32(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x19)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SBBL")
    }
    return p
}

// SBBQ performs "Subtract with Borrow".
//
// Mnemonic        : SBB
// Supported forms : (8 forms)
//
//    * SBBQ imm32, rax
//    * SBBQ imm8, r64
//    * SBBQ imm32, r64
//    * SBBQ r64, r64
//    * SBBQ m64, r64
//    * SBBQ imm8, m64
//    * SBBQ imm32, m64
//    * SBBQ r64, m64
//
func (self *Program) SBBQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SBBQ imm32, rax
    if isImm32(v0) && v1 == RAX {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48)
            m.emit(0x1d)
            m.imm4(toImmAny(v[0]))
        })
    }
    // SBBQ imm8, r64
    if isImm8Ext(v0, 8) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0x83)
            m.emit(0xd8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // SBBQ imm32, r64
    if isImm32Ext(v0, 8) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0x81)
            m.emit(0xd8 | lcode(v[1]))
            m.imm4(toImmAny(v[0]))
        })
    }
    // SBBQ r64, r64
    if isReg64(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[0]) << 2 | hcode(v[1]))
            m.emit(0x19)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x1b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // SBBQ m64, r64
    if isM64(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x1b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // SBBQ imm8, m64
    if isImm8Ext(v0, 8) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0x83)
            m.mrsd(3, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // SBBQ imm32, m64
    if isImm32Ext(v0, 8) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0x81)
            m.mrsd(3, addr(v[1]), 1)
            m.imm4(toImmAny(v[0]))
        })
    }
    // SBBQ r64, m64
    if isReg64(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[0]), addr(v[1]))
            m.emit(0x19)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SBBQ")
    }
    return p
}

// SBBW performs "Subtract with Borrow".
//
// Mnemonic        : SBB
// Supported forms : (8 forms)
//
//    * SBBW imm16, ax
//    * SBBW imm8, r16
//    * SBBW imm16, r16
//    * SBBW r16, r16
//    * SBBW m16, r16
//    * SBBW imm8, m16
//    * SBBW imm16, m16
//    * SBBW r16, m16
//
func (self *Program) SBBW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SBBW imm16, ax
    if isImm16(v0) && v1 == AX {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.emit(0x1d)
            m.imm2(toImmAny(v[0]))
        })
    }
    // SBBW imm8, r16
    if isImm8Ext(v0, 2) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0x83)
            m.emit(0xd8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // SBBW imm16, r16
    if isImm16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0x81)
            m.emit(0xd8 | lcode(v[1]))
            m.imm2(toImmAny(v[0]))
        })
    }
    // SBBW r16, r16
    if isReg16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x19)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x1b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // SBBW m16, r16
    if isM16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x1b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // SBBW imm8, m16
    if isImm8Ext(v0, 2) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0x83)
            m.mrsd(3, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // SBBW imm16, m16
    if isImm16(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0x81)
            m.mrsd(3, addr(v[1]), 1)
            m.imm2(toImmAny(v[0]))
        })
    }
    // SBBW r16, m16
    if isReg16(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x19)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SBBW")
    }
    return p
}

// SETA performs "Set byte if above (CF == 0 and ZF == 0)".
//
// Mnemonic        : SETA
// Supported forms : (2 forms)
//
//    * SETA r8
//    * SETA m8
//
func (self *Program) SETA(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // SETA r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0x97)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // SETA m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x97)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SETA")
    }
    return p
}

// SETAE performs "Set byte if above or equal (CF == 0)".
//
// Mnemonic        : SETAE
// Supported forms : (2 forms)
//
//    * SETAE r8
//    * SETAE m8
//
func (self *Program) SETAE(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // SETAE r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0x93)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // SETAE m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x93)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SETAE")
    }
    return p
}

// SETB performs "Set byte if below (CF == 1)".
//
// Mnemonic        : SETB
// Supported forms : (2 forms)
//
//    * SETB r8
//    * SETB m8
//
func (self *Program) SETB(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // SETB r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0x92)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // SETB m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x92)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SETB")
    }
    return p
}

// SETBE performs "Set byte if below or equal (CF == 1 or ZF == 1)".
//
// Mnemonic        : SETBE
// Supported forms : (2 forms)
//
//    * SETBE r8
//    * SETBE m8
//
func (self *Program) SETBE(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // SETBE r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0x96)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // SETBE m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x96)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SETBE")
    }
    return p
}

// SETC performs "Set byte if carry (CF == 1)".
//
// Mnemonic        : SETC
// Supported forms : (2 forms)
//
//    * SETC r8
//    * SETC m8
//
func (self *Program) SETC(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // SETC r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0x92)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // SETC m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x92)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SETC")
    }
    return p
}

// SETE performs "Set byte if equal (ZF == 1)".
//
// Mnemonic        : SETE
// Supported forms : (2 forms)
//
//    * SETE r8
//    * SETE m8
//
func (self *Program) SETE(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // SETE r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0x94)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // SETE m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x94)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SETE")
    }
    return p
}

// SETG performs "Set byte if greater (ZF == 0 and SF == OF)".
//
// Mnemonic        : SETG
// Supported forms : (2 forms)
//
//    * SETG r8
//    * SETG m8
//
func (self *Program) SETG(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // SETG r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0x9f)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // SETG m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x9f)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SETG")
    }
    return p
}

// SETGE performs "Set byte if greater or equal (SF == OF)".
//
// Mnemonic        : SETGE
// Supported forms : (2 forms)
//
//    * SETGE r8
//    * SETGE m8
//
func (self *Program) SETGE(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // SETGE r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0x9d)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // SETGE m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x9d)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SETGE")
    }
    return p
}

// SETL performs "Set byte if less (SF != OF)".
//
// Mnemonic        : SETL
// Supported forms : (2 forms)
//
//    * SETL r8
//    * SETL m8
//
func (self *Program) SETL(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // SETL r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0x9c)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // SETL m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x9c)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SETL")
    }
    return p
}

// SETLE performs "Set byte if less or equal (ZF == 1 or SF != OF)".
//
// Mnemonic        : SETLE
// Supported forms : (2 forms)
//
//    * SETLE r8
//    * SETLE m8
//
func (self *Program) SETLE(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // SETLE r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0x9e)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // SETLE m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x9e)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SETLE")
    }
    return p
}

// SETNA performs "Set byte if not above (CF == 1 or ZF == 1)".
//
// Mnemonic        : SETNA
// Supported forms : (2 forms)
//
//    * SETNA r8
//    * SETNA m8
//
func (self *Program) SETNA(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // SETNA r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0x96)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // SETNA m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x96)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SETNA")
    }
    return p
}

// SETNAE performs "Set byte if not above or equal (CF == 1)".
//
// Mnemonic        : SETNAE
// Supported forms : (2 forms)
//
//    * SETNAE r8
//    * SETNAE m8
//
func (self *Program) SETNAE(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // SETNAE r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0x92)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // SETNAE m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x92)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SETNAE")
    }
    return p
}

// SETNB performs "Set byte if not below (CF == 0)".
//
// Mnemonic        : SETNB
// Supported forms : (2 forms)
//
//    * SETNB r8
//    * SETNB m8
//
func (self *Program) SETNB(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // SETNB r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0x93)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // SETNB m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x93)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SETNB")
    }
    return p
}

// SETNBE performs "Set byte if not below or equal (CF == 0 and ZF == 0)".
//
// Mnemonic        : SETNBE
// Supported forms : (2 forms)
//
//    * SETNBE r8
//    * SETNBE m8
//
func (self *Program) SETNBE(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // SETNBE r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0x97)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // SETNBE m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x97)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SETNBE")
    }
    return p
}

// SETNC performs "Set byte if not carry (CF == 0)".
//
// Mnemonic        : SETNC
// Supported forms : (2 forms)
//
//    * SETNC r8
//    * SETNC m8
//
func (self *Program) SETNC(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // SETNC r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0x93)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // SETNC m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x93)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SETNC")
    }
    return p
}

// SETNE performs "Set byte if not equal (ZF == 0)".
//
// Mnemonic        : SETNE
// Supported forms : (2 forms)
//
//    * SETNE r8
//    * SETNE m8
//
func (self *Program) SETNE(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // SETNE r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0x95)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // SETNE m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x95)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SETNE")
    }
    return p
}

// SETNG performs "Set byte if not greater (ZF == 1 or SF != OF)".
//
// Mnemonic        : SETNG
// Supported forms : (2 forms)
//
//    * SETNG r8
//    * SETNG m8
//
func (self *Program) SETNG(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // SETNG r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0x9e)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // SETNG m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x9e)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SETNG")
    }
    return p
}

// SETNGE performs "Set byte if not greater or equal (SF != OF)".
//
// Mnemonic        : SETNGE
// Supported forms : (2 forms)
//
//    * SETNGE r8
//    * SETNGE m8
//
func (self *Program) SETNGE(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // SETNGE r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0x9c)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // SETNGE m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x9c)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SETNGE")
    }
    return p
}

// SETNL performs "Set byte if not less (SF == OF)".
//
// Mnemonic        : SETNL
// Supported forms : (2 forms)
//
//    * SETNL r8
//    * SETNL m8
//
func (self *Program) SETNL(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // SETNL r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0x9d)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // SETNL m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x9d)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SETNL")
    }
    return p
}

// SETNLE performs "Set byte if not less or equal (ZF == 0 and SF == OF)".
//
// Mnemonic        : SETNLE
// Supported forms : (2 forms)
//
//    * SETNLE r8
//    * SETNLE m8
//
func (self *Program) SETNLE(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // SETNLE r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0x9f)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // SETNLE m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x9f)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SETNLE")
    }
    return p
}

// SETNO performs "Set byte if not overflow (OF == 0)".
//
// Mnemonic        : SETNO
// Supported forms : (2 forms)
//
//    * SETNO r8
//    * SETNO m8
//
func (self *Program) SETNO(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // SETNO r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0x91)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // SETNO m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x91)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SETNO")
    }
    return p
}

// SETNP performs "Set byte if not parity (PF == 0)".
//
// Mnemonic        : SETNP
// Supported forms : (2 forms)
//
//    * SETNP r8
//    * SETNP m8
//
func (self *Program) SETNP(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // SETNP r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0x9b)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // SETNP m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x9b)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SETNP")
    }
    return p
}

// SETNS performs "Set byte if not sign (SF == 0)".
//
// Mnemonic        : SETNS
// Supported forms : (2 forms)
//
//    * SETNS r8
//    * SETNS m8
//
func (self *Program) SETNS(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // SETNS r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0x99)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // SETNS m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x99)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SETNS")
    }
    return p
}

// SETNZ performs "Set byte if not zero (ZF == 0)".
//
// Mnemonic        : SETNZ
// Supported forms : (2 forms)
//
//    * SETNZ r8
//    * SETNZ m8
//
func (self *Program) SETNZ(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // SETNZ r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0x95)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // SETNZ m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x95)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SETNZ")
    }
    return p
}

// SETO performs "Set byte if overflow (OF == 1)".
//
// Mnemonic        : SETO
// Supported forms : (2 forms)
//
//    * SETO r8
//    * SETO m8
//
func (self *Program) SETO(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // SETO r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0x90)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // SETO m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x90)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SETO")
    }
    return p
}

// SETP performs "Set byte if parity (PF == 1)".
//
// Mnemonic        : SETP
// Supported forms : (2 forms)
//
//    * SETP r8
//    * SETP m8
//
func (self *Program) SETP(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // SETP r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0x9a)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // SETP m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x9a)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SETP")
    }
    return p
}

// SETPE performs "Set byte if parity even (PF == 1)".
//
// Mnemonic        : SETPE
// Supported forms : (2 forms)
//
//    * SETPE r8
//    * SETPE m8
//
func (self *Program) SETPE(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // SETPE r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0x9a)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // SETPE m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x9a)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SETPE")
    }
    return p
}

// SETPO performs "Set byte if parity odd (PF == 0)".
//
// Mnemonic        : SETPO
// Supported forms : (2 forms)
//
//    * SETPO r8
//    * SETPO m8
//
func (self *Program) SETPO(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // SETPO r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0x9b)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // SETPO m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x9b)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SETPO")
    }
    return p
}

// SETS performs "Set byte if sign (SF == 1)".
//
// Mnemonic        : SETS
// Supported forms : (2 forms)
//
//    * SETS r8
//    * SETS m8
//
func (self *Program) SETS(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // SETS r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0x98)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // SETS m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x98)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SETS")
    }
    return p
}

// SETZ performs "Set byte if zero (ZF == 1)".
//
// Mnemonic        : SETZ
// Supported forms : (2 forms)
//
//    * SETZ r8
//    * SETZ m8
//
func (self *Program) SETZ(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // SETZ r8
    if isReg8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0x94)
            m.emit(0xc0 | lcode(v[0]))
        })
    }
    // SETZ m8
    if isM8(v0) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x94)
            m.mrsd(0, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SETZ")
    }
    return p
}

// SFENCE performs "Store Fence".
//
// Mnemonic        : SFENCE
// ISA extensions  : MMX+
// Supported forms : (1 form)
//
//    * SFENCE
//
func (self *Program) SFENCE() *Instruction {
    p := self.alloc(0, Operands{})
    // SFENCE
    self.require(ISA_MMX_PLUS)
    p.add(0, func(m *_Encoding, v []interface{}) {
        m.emit(0x0f)
        m.emit(0xae)
        m.emit(0xf8)
    })
    return p
}

// SHA1MSG1 performs "Perform an Intermediate Calculation for the Next Four SHA1 Message Doublewords".
//
// Mnemonic        : SHA1MSG1
// ISA extensions  : SHA
// Supported forms : (2 forms)
//
//    * SHA1MSG1 xmm, xmm
//    * SHA1MSG1 m128, xmm
//
func (self *Program) SHA1MSG1(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SHA1MSG1 xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SHA)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xc9)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // SHA1MSG1 m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SHA)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xc9)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SHA1MSG1")
    }
    return p
}

// SHA1MSG2 performs "Perform a Final Calculation for the Next Four SHA1 Message Doublewords".
//
// Mnemonic        : SHA1MSG2
// ISA extensions  : SHA
// Supported forms : (2 forms)
//
//    * SHA1MSG2 xmm, xmm
//    * SHA1MSG2 m128, xmm
//
func (self *Program) SHA1MSG2(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SHA1MSG2 xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SHA)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xca)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // SHA1MSG2 m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SHA)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xca)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SHA1MSG2")
    }
    return p
}

// SHA1NEXTE performs "Calculate SHA1 State Variable E after Four Rounds".
//
// Mnemonic        : SHA1NEXTE
// ISA extensions  : SHA
// Supported forms : (2 forms)
//
//    * SHA1NEXTE xmm, xmm
//    * SHA1NEXTE m128, xmm
//
func (self *Program) SHA1NEXTE(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SHA1NEXTE xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SHA)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xc8)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // SHA1NEXTE m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SHA)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xc8)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SHA1NEXTE")
    }
    return p
}

// SHA1RNDS4 performs "Perform Four Rounds of SHA1 Operation".
//
// Mnemonic        : SHA1RNDS4
// ISA extensions  : SHA
// Supported forms : (2 forms)
//
//    * SHA1RNDS4 imm8, xmm, xmm
//    * SHA1RNDS4 imm8, m128, xmm
//
func (self *Program) SHA1RNDS4(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // SHA1RNDS4 imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_SHA)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0xcc)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // SHA1RNDS4 imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_SHA)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x3a)
            m.emit(0xcc)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for SHA1RNDS4")
    }
    return p
}

// SHA256MSG1 performs "Perform an Intermediate Calculation for the Next Four SHA256 Message Doublewords".
//
// Mnemonic        : SHA256MSG1
// ISA extensions  : SHA
// Supported forms : (2 forms)
//
//    * SHA256MSG1 xmm, xmm
//    * SHA256MSG1 m128, xmm
//
func (self *Program) SHA256MSG1(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SHA256MSG1 xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SHA)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xcc)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // SHA256MSG1 m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SHA)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xcc)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SHA256MSG1")
    }
    return p
}

// SHA256MSG2 performs "Perform a Final Calculation for the Next Four SHA256 Message Doublewords".
//
// Mnemonic        : SHA256MSG2
// ISA extensions  : SHA
// Supported forms : (2 forms)
//
//    * SHA256MSG2 xmm, xmm
//    * SHA256MSG2 m128, xmm
//
func (self *Program) SHA256MSG2(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SHA256MSG2 xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SHA)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xcd)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // SHA256MSG2 m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SHA)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xcd)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SHA256MSG2")
    }
    return p
}

// SHA256RNDS2 performs "Perform Two Rounds of SHA256 Operation".
//
// Mnemonic        : SHA256RNDS2
// ISA extensions  : SHA
// Supported forms : (2 forms)
//
//    * SHA256RNDS2 xmm0, xmm, xmm
//    * SHA256RNDS2 xmm0, m128, xmm
//
func (self *Program) SHA256RNDS2(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // SHA256RNDS2 xmm0, xmm, xmm
    if v0 == XMM0 && isXMM(v1) && isXMM(v2) {
        self.require(ISA_SHA)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xcb)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // SHA256RNDS2 xmm0, m128, xmm
    if v0 == XMM0 && isM128(v1) && isXMM(v2) {
        self.require(ISA_SHA)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0x38)
            m.emit(0xcb)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SHA256RNDS2")
    }
    return p
}

// SHLB performs "Logical Shift Left".
//
// Mnemonic        : SHL
// Supported forms : (6 forms)
//
//    * SHLB 1, r8
//    * SHLB imm8, r8
//    * SHLB cl, r8
//    * SHLB 1, m8
//    * SHLB imm8, m8
//    * SHLB cl, m8
//
func (self *Program) SHLB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SHLB 1, r8
    if isConst1(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0xd0)
            m.emit(0xe0 | lcode(v[1]))
        })
    }
    // SHLB imm8, r8
    if isImm8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0xc0)
            m.emit(0xe0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // SHLB cl, r8
    if v0 == CL && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0xd2)
            m.emit(0xe0 | lcode(v[1]))
        })
    }
    // SHLB 1, m8
    if isConst1(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd0)
            m.mrsd(4, addr(v[1]), 1)
        })
    }
    // SHLB imm8, m8
    if isImm8(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xc0)
            m.mrsd(4, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // SHLB cl, m8
    if v0 == CL && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd2)
            m.mrsd(4, addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SHLB")
    }
    return p
}

// SHLDL performs "Integer Double Precision Shift Left".
//
// Mnemonic        : SHLD
// Supported forms : (4 forms)
//
//    * SHLDL imm8, r32, r32
//    * SHLDL cl, r32, r32
//    * SHLDL imm8, r32, m32
//    * SHLDL cl, r32, m32
//
func (self *Program) SHLDL(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // SHLDL imm8, r32, r32
    if isImm8(v0) && isReg32(v1) && isReg32(v2) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[2], false)
            m.emit(0x0f)
            m.emit(0xa4)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // SHLDL cl, r32, r32
    if v0 == CL && isReg32(v1) && isReg32(v2) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[2], false)
            m.emit(0x0f)
            m.emit(0xa5)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
        })
    }
    // SHLDL imm8, r32, m32
    if isImm8(v0) && isReg32(v1) && isM32(v2) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[2]), false)
            m.emit(0x0f)
            m.emit(0xa4)
            m.mrsd(lcode(v[1]), addr(v[2]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // SHLDL cl, r32, m32
    if v0 == CL && isReg32(v1) && isM32(v2) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[2]), false)
            m.emit(0x0f)
            m.emit(0xa5)
            m.mrsd(lcode(v[1]), addr(v[2]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SHLDL")
    }
    return p
}

// SHLDQ performs "Integer Double Precision Shift Left".
//
// Mnemonic        : SHLD
// Supported forms : (4 forms)
//
//    * SHLDQ imm8, r64, r64
//    * SHLDQ cl, r64, r64
//    * SHLDQ imm8, r64, m64
//    * SHLDQ cl, r64, m64
//
func (self *Program) SHLDQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // SHLDQ imm8, r64, r64
    if isImm8(v0) && isReg64(v1) && isReg64(v2) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[2]))
            m.emit(0x0f)
            m.emit(0xa4)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // SHLDQ cl, r64, r64
    if v0 == CL && isReg64(v1) && isReg64(v2) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[2]))
            m.emit(0x0f)
            m.emit(0xa5)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
        })
    }
    // SHLDQ imm8, r64, m64
    if isImm8(v0) && isReg64(v1) && isM64(v2) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[2]))
            m.emit(0x0f)
            m.emit(0xa4)
            m.mrsd(lcode(v[1]), addr(v[2]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // SHLDQ cl, r64, m64
    if v0 == CL && isReg64(v1) && isM64(v2) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[2]))
            m.emit(0x0f)
            m.emit(0xa5)
            m.mrsd(lcode(v[1]), addr(v[2]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SHLDQ")
    }
    return p
}

// SHLDW performs "Integer Double Precision Shift Left".
//
// Mnemonic        : SHLD
// Supported forms : (4 forms)
//
//    * SHLDW imm8, r16, r16
//    * SHLDW cl, r16, r16
//    * SHLDW imm8, r16, m16
//    * SHLDW cl, r16, m16
//
func (self *Program) SHLDW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // SHLDW imm8, r16, r16
    if isImm8(v0) && isReg16(v1) && isReg16(v2) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[2], false)
            m.emit(0x0f)
            m.emit(0xa4)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // SHLDW cl, r16, r16
    if v0 == CL && isReg16(v1) && isReg16(v2) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[2], false)
            m.emit(0x0f)
            m.emit(0xa5)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
        })
    }
    // SHLDW imm8, r16, m16
    if isImm8(v0) && isReg16(v1) && isM16(v2) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[2]), false)
            m.emit(0x0f)
            m.emit(0xa4)
            m.mrsd(lcode(v[1]), addr(v[2]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // SHLDW cl, r16, m16
    if v0 == CL && isReg16(v1) && isM16(v2) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[2]), false)
            m.emit(0x0f)
            m.emit(0xa5)
            m.mrsd(lcode(v[1]), addr(v[2]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SHLDW")
    }
    return p
}

// SHLL performs "Logical Shift Left".
//
// Mnemonic        : SHL
// Supported forms : (6 forms)
//
//    * SHLL 1, r32
//    * SHLL imm8, r32
//    * SHLL cl, r32
//    * SHLL 1, m32
//    * SHLL imm8, m32
//    * SHLL cl, m32
//
func (self *Program) SHLL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SHLL 1, r32
    if isConst1(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0xd1)
            m.emit(0xe0 | lcode(v[1]))
        })
    }
    // SHLL imm8, r32
    if isImm8(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0xc1)
            m.emit(0xe0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // SHLL cl, r32
    if v0 == CL && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0xd3)
            m.emit(0xe0 | lcode(v[1]))
        })
    }
    // SHLL 1, m32
    if isConst1(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd1)
            m.mrsd(4, addr(v[1]), 1)
        })
    }
    // SHLL imm8, m32
    if isImm8(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xc1)
            m.mrsd(4, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // SHLL cl, m32
    if v0 == CL && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd3)
            m.mrsd(4, addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SHLL")
    }
    return p
}

// SHLQ performs "Logical Shift Left".
//
// Mnemonic        : SHL
// Supported forms : (6 forms)
//
//    * SHLQ 1, r64
//    * SHLQ imm8, r64
//    * SHLQ cl, r64
//    * SHLQ 1, m64
//    * SHLQ imm8, m64
//    * SHLQ cl, m64
//
func (self *Program) SHLQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SHLQ 1, r64
    if isConst1(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0xd1)
            m.emit(0xe0 | lcode(v[1]))
        })
    }
    // SHLQ imm8, r64
    if isImm8(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0xc1)
            m.emit(0xe0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // SHLQ cl, r64
    if v0 == CL && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0xd3)
            m.emit(0xe0 | lcode(v[1]))
        })
    }
    // SHLQ 1, m64
    if isConst1(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0xd1)
            m.mrsd(4, addr(v[1]), 1)
        })
    }
    // SHLQ imm8, m64
    if isImm8(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0xc1)
            m.mrsd(4, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // SHLQ cl, m64
    if v0 == CL && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0xd3)
            m.mrsd(4, addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SHLQ")
    }
    return p
}

// SHLW performs "Logical Shift Left".
//
// Mnemonic        : SHL
// Supported forms : (6 forms)
//
//    * SHLW 1, r16
//    * SHLW imm8, r16
//    * SHLW cl, r16
//    * SHLW 1, m16
//    * SHLW imm8, m16
//    * SHLW cl, m16
//
func (self *Program) SHLW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SHLW 1, r16
    if isConst1(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0xd1)
            m.emit(0xe0 | lcode(v[1]))
        })
    }
    // SHLW imm8, r16
    if isImm8(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0xc1)
            m.emit(0xe0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // SHLW cl, r16
    if v0 == CL && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0xd3)
            m.emit(0xe0 | lcode(v[1]))
        })
    }
    // SHLW 1, m16
    if isConst1(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd1)
            m.mrsd(4, addr(v[1]), 1)
        })
    }
    // SHLW imm8, m16
    if isImm8(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0xc1)
            m.mrsd(4, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // SHLW cl, m16
    if v0 == CL && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd3)
            m.mrsd(4, addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SHLW")
    }
    return p
}

// SHLXL performs "Logical Shift Left Without Affecting Flags".
//
// Mnemonic        : SHLX
// ISA extensions  : BMI2
// Supported forms : (2 forms)
//
//    * SHLXL r32, r32, r32
//    * SHLXL r32, m32, r32
//
func (self *Program) SHLXL(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // SHLXL r32, r32, r32
    if isReg32(v0) && isReg32(v1) && isReg32(v2) {
        self.require(ISA_BMI2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[0]) << 3))
            m.emit(0xf7)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // SHLXL r32, m32, r32
    if isReg32(v0) && isM32(v1) && isReg32(v2) {
        self.require(ISA_BMI2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0xf7)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SHLXL")
    }
    return p
}

// SHLXQ performs "Logical Shift Left Without Affecting Flags".
//
// Mnemonic        : SHLX
// ISA extensions  : BMI2
// Supported forms : (2 forms)
//
//    * SHLXQ r64, r64, r64
//    * SHLXQ r64, m64, r64
//
func (self *Program) SHLXQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // SHLXQ r64, r64, r64
    if isReg64(v0) && isReg64(v1) && isReg64(v2) {
        self.require(ISA_BMI2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0xf9 ^ (hlcode(v[0]) << 3))
            m.emit(0xf7)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // SHLXQ r64, m64, r64
    if isReg64(v0) && isM64(v1) && isReg64(v2) {
        self.require(ISA_BMI2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0xf7)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SHLXQ")
    }
    return p
}

// SHRB performs "Logical Shift Right".
//
// Mnemonic        : SHR
// Supported forms : (6 forms)
//
//    * SHRB 1, r8
//    * SHRB imm8, r8
//    * SHRB cl, r8
//    * SHRB 1, m8
//    * SHRB imm8, m8
//    * SHRB cl, m8
//
func (self *Program) SHRB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SHRB 1, r8
    if isConst1(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0xd0)
            m.emit(0xe8 | lcode(v[1]))
        })
    }
    // SHRB imm8, r8
    if isImm8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0xc0)
            m.emit(0xe8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // SHRB cl, r8
    if v0 == CL && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0xd2)
            m.emit(0xe8 | lcode(v[1]))
        })
    }
    // SHRB 1, m8
    if isConst1(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd0)
            m.mrsd(5, addr(v[1]), 1)
        })
    }
    // SHRB imm8, m8
    if isImm8(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xc0)
            m.mrsd(5, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // SHRB cl, m8
    if v0 == CL && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd2)
            m.mrsd(5, addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SHRB")
    }
    return p
}

// SHRDL performs "Integer Double Precision Shift Right".
//
// Mnemonic        : SHRD
// Supported forms : (4 forms)
//
//    * SHRDL imm8, r32, r32
//    * SHRDL cl, r32, r32
//    * SHRDL imm8, r32, m32
//    * SHRDL cl, r32, m32
//
func (self *Program) SHRDL(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // SHRDL imm8, r32, r32
    if isImm8(v0) && isReg32(v1) && isReg32(v2) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[2], false)
            m.emit(0x0f)
            m.emit(0xac)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // SHRDL cl, r32, r32
    if v0 == CL && isReg32(v1) && isReg32(v2) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[2], false)
            m.emit(0x0f)
            m.emit(0xad)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
        })
    }
    // SHRDL imm8, r32, m32
    if isImm8(v0) && isReg32(v1) && isM32(v2) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[2]), false)
            m.emit(0x0f)
            m.emit(0xac)
            m.mrsd(lcode(v[1]), addr(v[2]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // SHRDL cl, r32, m32
    if v0 == CL && isReg32(v1) && isM32(v2) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[2]), false)
            m.emit(0x0f)
            m.emit(0xad)
            m.mrsd(lcode(v[1]), addr(v[2]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SHRDL")
    }
    return p
}

// SHRDQ performs "Integer Double Precision Shift Right".
//
// Mnemonic        : SHRD
// Supported forms : (4 forms)
//
//    * SHRDQ imm8, r64, r64
//    * SHRDQ cl, r64, r64
//    * SHRDQ imm8, r64, m64
//    * SHRDQ cl, r64, m64
//
func (self *Program) SHRDQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // SHRDQ imm8, r64, r64
    if isImm8(v0) && isReg64(v1) && isReg64(v2) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[2]))
            m.emit(0x0f)
            m.emit(0xac)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // SHRDQ cl, r64, r64
    if v0 == CL && isReg64(v1) && isReg64(v2) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[2]))
            m.emit(0x0f)
            m.emit(0xad)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
        })
    }
    // SHRDQ imm8, r64, m64
    if isImm8(v0) && isReg64(v1) && isM64(v2) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[2]))
            m.emit(0x0f)
            m.emit(0xac)
            m.mrsd(lcode(v[1]), addr(v[2]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // SHRDQ cl, r64, m64
    if v0 == CL && isReg64(v1) && isM64(v2) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[2]))
            m.emit(0x0f)
            m.emit(0xad)
            m.mrsd(lcode(v[1]), addr(v[2]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SHRDQ")
    }
    return p
}

// SHRDW performs "Integer Double Precision Shift Right".
//
// Mnemonic        : SHRD
// Supported forms : (4 forms)
//
//    * SHRDW imm8, r16, r16
//    * SHRDW cl, r16, r16
//    * SHRDW imm8, r16, m16
//    * SHRDW cl, r16, m16
//
func (self *Program) SHRDW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // SHRDW imm8, r16, r16
    if isImm8(v0) && isReg16(v1) && isReg16(v2) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[2], false)
            m.emit(0x0f)
            m.emit(0xac)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // SHRDW cl, r16, r16
    if v0 == CL && isReg16(v1) && isReg16(v2) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[2], false)
            m.emit(0x0f)
            m.emit(0xad)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
        })
    }
    // SHRDW imm8, r16, m16
    if isImm8(v0) && isReg16(v1) && isM16(v2) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[2]), false)
            m.emit(0x0f)
            m.emit(0xac)
            m.mrsd(lcode(v[1]), addr(v[2]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // SHRDW cl, r16, m16
    if v0 == CL && isReg16(v1) && isM16(v2) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[2]), false)
            m.emit(0x0f)
            m.emit(0xad)
            m.mrsd(lcode(v[1]), addr(v[2]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SHRDW")
    }
    return p
}

// SHRL performs "Logical Shift Right".
//
// Mnemonic        : SHR
// Supported forms : (6 forms)
//
//    * SHRL 1, r32
//    * SHRL imm8, r32
//    * SHRL cl, r32
//    * SHRL 1, m32
//    * SHRL imm8, m32
//    * SHRL cl, m32
//
func (self *Program) SHRL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SHRL 1, r32
    if isConst1(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0xd1)
            m.emit(0xe8 | lcode(v[1]))
        })
    }
    // SHRL imm8, r32
    if isImm8(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0xc1)
            m.emit(0xe8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // SHRL cl, r32
    if v0 == CL && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0xd3)
            m.emit(0xe8 | lcode(v[1]))
        })
    }
    // SHRL 1, m32
    if isConst1(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd1)
            m.mrsd(5, addr(v[1]), 1)
        })
    }
    // SHRL imm8, m32
    if isImm8(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xc1)
            m.mrsd(5, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // SHRL cl, m32
    if v0 == CL && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd3)
            m.mrsd(5, addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SHRL")
    }
    return p
}

// SHRQ performs "Logical Shift Right".
//
// Mnemonic        : SHR
// Supported forms : (6 forms)
//
//    * SHRQ 1, r64
//    * SHRQ imm8, r64
//    * SHRQ cl, r64
//    * SHRQ 1, m64
//    * SHRQ imm8, m64
//    * SHRQ cl, m64
//
func (self *Program) SHRQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SHRQ 1, r64
    if isConst1(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0xd1)
            m.emit(0xe8 | lcode(v[1]))
        })
    }
    // SHRQ imm8, r64
    if isImm8(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0xc1)
            m.emit(0xe8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // SHRQ cl, r64
    if v0 == CL && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0xd3)
            m.emit(0xe8 | lcode(v[1]))
        })
    }
    // SHRQ 1, m64
    if isConst1(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0xd1)
            m.mrsd(5, addr(v[1]), 1)
        })
    }
    // SHRQ imm8, m64
    if isImm8(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0xc1)
            m.mrsd(5, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // SHRQ cl, m64
    if v0 == CL && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0xd3)
            m.mrsd(5, addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SHRQ")
    }
    return p
}

// SHRW performs "Logical Shift Right".
//
// Mnemonic        : SHR
// Supported forms : (6 forms)
//
//    * SHRW 1, r16
//    * SHRW imm8, r16
//    * SHRW cl, r16
//    * SHRW 1, m16
//    * SHRW imm8, m16
//    * SHRW cl, m16
//
func (self *Program) SHRW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SHRW 1, r16
    if isConst1(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0xd1)
            m.emit(0xe8 | lcode(v[1]))
        })
    }
    // SHRW imm8, r16
    if isImm8(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0xc1)
            m.emit(0xe8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // SHRW cl, r16
    if v0 == CL && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0xd3)
            m.emit(0xe8 | lcode(v[1]))
        })
    }
    // SHRW 1, m16
    if isConst1(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd1)
            m.mrsd(5, addr(v[1]), 1)
        })
    }
    // SHRW imm8, m16
    if isImm8(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0xc1)
            m.mrsd(5, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // SHRW cl, m16
    if v0 == CL && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0xd3)
            m.mrsd(5, addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SHRW")
    }
    return p
}

// SHRXL performs "Logical Shift Right Without Affecting Flags".
//
// Mnemonic        : SHRX
// ISA extensions  : BMI2
// Supported forms : (2 forms)
//
//    * SHRXL r32, r32, r32
//    * SHRXL r32, m32, r32
//
func (self *Program) SHRXL(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // SHRXL r32, r32, r32
    if isReg32(v0) && isReg32(v1) && isReg32(v2) {
        self.require(ISA_BMI2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7b ^ (hlcode(v[0]) << 3))
            m.emit(0xf7)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // SHRXL r32, m32, r32
    if isReg32(v0) && isM32(v1) && isReg32(v2) {
        self.require(ISA_BMI2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x03, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0xf7)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SHRXL")
    }
    return p
}

// SHRXQ performs "Logical Shift Right Without Affecting Flags".
//
// Mnemonic        : SHRX
// ISA extensions  : BMI2
// Supported forms : (2 forms)
//
//    * SHRXQ r64, r64, r64
//    * SHRXQ r64, m64, r64
//
func (self *Program) SHRXQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // SHRXQ r64, r64, r64
    if isReg64(v0) && isReg64(v1) && isReg64(v2) {
        self.require(ISA_BMI2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0xfb ^ (hlcode(v[0]) << 3))
            m.emit(0xf7)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // SHRXQ r64, m64, r64
    if isReg64(v0) && isM64(v1) && isReg64(v2) {
        self.require(ISA_BMI2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x83, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0xf7)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SHRXQ")
    }
    return p
}

// SHUFPD performs "Shuffle Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : SHUFPD
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * SHUFPD imm8, xmm, xmm
//    * SHUFPD imm8, m128, xmm
//
func (self *Program) SHUFPD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // SHUFPD imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0xc6)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // SHUFPD imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0xc6)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for SHUFPD")
    }
    return p
}

// SHUFPS performs "Shuffle Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : SHUFPS
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * SHUFPS imm8, xmm, xmm
//    * SHUFPS imm8, m128, xmm
//
func (self *Program) SHUFPS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // SHUFPS imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[2]), v[1], false)
            m.emit(0x0f)
            m.emit(0xc6)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // SHUFPS imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[2]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0xc6)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for SHUFPS")
    }
    return p
}

// SQRTPD performs "Compute Square Roots of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : SQRTPD
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * SQRTPD xmm, xmm
//    * SQRTPD m128, xmm
//
func (self *Program) SQRTPD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SQRTPD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x51)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // SQRTPD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x51)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SQRTPD")
    }
    return p
}

// SQRTPS performs "Compute Square Roots of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : SQRTPS
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * SQRTPS xmm, xmm
//    * SQRTPS m128, xmm
//
func (self *Program) SQRTPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SQRTPS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x51)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // SQRTPS m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x51)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SQRTPS")
    }
    return p
}

// SQRTSD performs "Compute Square Root of Scalar Double-Precision Floating-Point Value".
//
// Mnemonic        : SQRTSD
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * SQRTSD xmm, xmm
//    * SQRTSD m64, xmm
//
func (self *Program) SQRTSD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SQRTSD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x51)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // SQRTSD m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x51)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SQRTSD")
    }
    return p
}

// SQRTSS performs "Compute Square Root of Scalar Single-Precision Floating-Point Value".
//
// Mnemonic        : SQRTSS
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * SQRTSS xmm, xmm
//    * SQRTSS m32, xmm
//
func (self *Program) SQRTSS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SQRTSS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x51)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // SQRTSS m32, xmm
    if isM32(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x51)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SQRTSS")
    }
    return p
}

// STC performs "Set Carry Flag".
//
// Mnemonic        : STC
// Supported forms : (1 form)
//
//    * STC
//
func (self *Program) STC() *Instruction {
    p := self.alloc(0, Operands{})
    // STC
    p.add(0, func(m *_Encoding, v []interface{}) {
        m.emit(0xf9)
    })
    return p
}

// STD performs "Set Direction Flag".
//
// Mnemonic        : STD
// Supported forms : (1 form)
//
//    * STD
//
func (self *Program) STD() *Instruction {
    p := self.alloc(0, Operands{})
    // STD
    p.add(0, func(m *_Encoding, v []interface{}) {
        m.emit(0xfd)
    })
    return p
}

// STMXCSR performs "Store MXCSR Register State".
//
// Mnemonic        : STMXCSR
// ISA extensions  : SSE
// Supported forms : (1 form)
//
//    * STMXCSR m32
//
func (self *Program) STMXCSR(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // STMXCSR m32
    if isM32(v0) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xae)
            m.mrsd(3, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for STMXCSR")
    }
    return p
}

// SUBB performs "Subtract".
//
// Mnemonic        : SUB
// Supported forms : (6 forms)
//
//    * SUBB imm8, al
//    * SUBB imm8, r8
//    * SUBB r8, r8
//    * SUBB m8, r8
//    * SUBB imm8, m8
//    * SUBB r8, m8
//
func (self *Program) SUBB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SUBB imm8, al
    if isImm8(v0) && v1 == AL {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x2c)
            m.imm1(toImmAny(v[0]))
        })
    }
    // SUBB imm8, r8
    if isImm8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0x80)
            m.emit(0xe8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // SUBB r8, r8
    if isReg8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], isReg8REX(v[0]) || isReg8REX(v[1]))
            m.emit(0x28)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], isReg8REX(v[0]) || isReg8REX(v[1]))
            m.emit(0x2a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // SUBB m8, r8
    if isM8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), isReg8REX(v[1]))
            m.emit(0x2a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // SUBB imm8, m8
    if isImm8(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0x80)
            m.mrsd(5, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // SUBB r8, m8
    if isReg8(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), isReg8REX(v[0]))
            m.emit(0x28)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SUBB")
    }
    return p
}

// SUBL performs "Subtract".
//
// Mnemonic        : SUB
// Supported forms : (8 forms)
//
//    * SUBL imm32, eax
//    * SUBL imm8, r32
//    * SUBL imm32, r32
//    * SUBL r32, r32
//    * SUBL m32, r32
//    * SUBL imm8, m32
//    * SUBL imm32, m32
//    * SUBL r32, m32
//
func (self *Program) SUBL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SUBL imm32, eax
    if isImm32(v0) && v1 == EAX {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x2d)
            m.imm4(toImmAny(v[0]))
        })
    }
    // SUBL imm8, r32
    if isImm8Ext(v0, 4) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0x83)
            m.emit(0xe8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // SUBL imm32, r32
    if isImm32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0x81)
            m.emit(0xe8 | lcode(v[1]))
            m.imm4(toImmAny(v[0]))
        })
    }
    // SUBL r32, r32
    if isReg32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x29)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x2b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // SUBL m32, r32
    if isM32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x2b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // SUBL imm8, m32
    if isImm8Ext(v0, 4) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0x83)
            m.mrsd(5, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // SUBL imm32, m32
    if isImm32(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0x81)
            m.mrsd(5, addr(v[1]), 1)
            m.imm4(toImmAny(v[0]))
        })
    }
    // SUBL r32, m32
    if isReg32(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x29)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SUBL")
    }
    return p
}

// SUBPD performs "Subtract Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : SUBPD
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * SUBPD xmm, xmm
//    * SUBPD m128, xmm
//
func (self *Program) SUBPD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SUBPD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x5c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // SUBPD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x5c)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SUBPD")
    }
    return p
}

// SUBPS performs "Subtract Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : SUBPS
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * SUBPS xmm, xmm
//    * SUBPS m128, xmm
//
func (self *Program) SUBPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SUBPS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x5c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // SUBPS m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x5c)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SUBPS")
    }
    return p
}

// SUBQ performs "Subtract".
//
// Mnemonic        : SUB
// Supported forms : (8 forms)
//
//    * SUBQ imm32, rax
//    * SUBQ imm8, r64
//    * SUBQ imm32, r64
//    * SUBQ r64, r64
//    * SUBQ m64, r64
//    * SUBQ imm8, m64
//    * SUBQ imm32, m64
//    * SUBQ r64, m64
//
func (self *Program) SUBQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SUBQ imm32, rax
    if isImm32(v0) && v1 == RAX {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48)
            m.emit(0x2d)
            m.imm4(toImmAny(v[0]))
        })
    }
    // SUBQ imm8, r64
    if isImm8Ext(v0, 8) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0x83)
            m.emit(0xe8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // SUBQ imm32, r64
    if isImm32Ext(v0, 8) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0x81)
            m.emit(0xe8 | lcode(v[1]))
            m.imm4(toImmAny(v[0]))
        })
    }
    // SUBQ r64, r64
    if isReg64(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[0]) << 2 | hcode(v[1]))
            m.emit(0x29)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x2b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // SUBQ m64, r64
    if isM64(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x2b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // SUBQ imm8, m64
    if isImm8Ext(v0, 8) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0x83)
            m.mrsd(5, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // SUBQ imm32, m64
    if isImm32Ext(v0, 8) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0x81)
            m.mrsd(5, addr(v[1]), 1)
            m.imm4(toImmAny(v[0]))
        })
    }
    // SUBQ r64, m64
    if isReg64(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[0]), addr(v[1]))
            m.emit(0x29)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SUBQ")
    }
    return p
}

// SUBSD performs "Subtract Scalar Double-Precision Floating-Point Values".
//
// Mnemonic        : SUBSD
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * SUBSD xmm, xmm
//    * SUBSD m64, xmm
//
func (self *Program) SUBSD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SUBSD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x5c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // SUBSD m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf2)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x5c)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SUBSD")
    }
    return p
}

// SUBSS performs "Subtract Scalar Single-Precision Floating-Point Values".
//
// Mnemonic        : SUBSS
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * SUBSS xmm, xmm
//    * SUBSS m32, xmm
//
func (self *Program) SUBSS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SUBSS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x5c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // SUBSS m32, xmm
    if isM32(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x5c)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SUBSS")
    }
    return p
}

// SUBW performs "Subtract".
//
// Mnemonic        : SUB
// Supported forms : (8 forms)
//
//    * SUBW imm16, ax
//    * SUBW imm8, r16
//    * SUBW imm16, r16
//    * SUBW r16, r16
//    * SUBW m16, r16
//    * SUBW imm8, m16
//    * SUBW imm16, m16
//    * SUBW r16, m16
//
func (self *Program) SUBW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // SUBW imm16, ax
    if isImm16(v0) && v1 == AX {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.emit(0x2d)
            m.imm2(toImmAny(v[0]))
        })
    }
    // SUBW imm8, r16
    if isImm8Ext(v0, 2) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0x83)
            m.emit(0xe8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // SUBW imm16, r16
    if isImm16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0x81)
            m.emit(0xe8 | lcode(v[1]))
            m.imm2(toImmAny(v[0]))
        })
    }
    // SUBW r16, r16
    if isReg16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x29)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x2b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // SUBW m16, r16
    if isM16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x2b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // SUBW imm8, m16
    if isImm8Ext(v0, 2) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0x83)
            m.mrsd(5, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // SUBW imm16, m16
    if isImm16(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0x81)
            m.mrsd(5, addr(v[1]), 1)
            m.imm2(toImmAny(v[0]))
        })
    }
    // SUBW r16, m16
    if isReg16(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x29)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for SUBW")
    }
    return p
}

// SYSCALL performs "Fast System Call".
//
// Mnemonic        : SYSCALL
// Supported forms : (1 form)
//
//    * SYSCALL
//
func (self *Program) SYSCALL() *Instruction {
    p := self.alloc(0, Operands{})
    // SYSCALL
    p.add(0, func(m *_Encoding, v []interface{}) {
        m.emit(0x0f)
        m.emit(0x05)
    })
    return p
}

// T1MSKC performs "Inverse Mask From Trailing Ones".
//
// Mnemonic        : T1MSKC
// ISA extensions  : TBM
// Supported forms : (4 forms)
//
//    * T1MSKC r32, r32
//    * T1MSKC m32, r32
//    * T1MSKC r64, r64
//    * T1MSKC m64, r64
//
func (self *Program) T1MSKC(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // T1MSKC r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[0]) << 5))
            m.emit(0x78 ^ (hlcode(v[1]) << 3))
            m.emit(0x01)
            m.emit(0xf8 | lcode(v[0]))
        })
    }
    // T1MSKC m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, 0, addr(v[0]), hlcode(v[1]))
            m.emit(0x01)
            m.mrsd(7, addr(v[0]), 1)
        })
    }
    // T1MSKC r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[0]) << 5))
            m.emit(0xf8 ^ (hlcode(v[1]) << 3))
            m.emit(0x01)
            m.emit(0xf8 | lcode(v[0]))
        })
    }
    // T1MSKC m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x80, 0, addr(v[0]), hlcode(v[1]))
            m.emit(0x01)
            m.mrsd(7, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for T1MSKC")
    }
    return p
}

// TESTB performs "Logical Compare".
//
// Mnemonic        : TEST
// Supported forms : (5 forms)
//
//    * TESTB imm8, al
//    * TESTB imm8, r8
//    * TESTB r8, r8
//    * TESTB imm8, m8
//    * TESTB r8, m8
//
func (self *Program) TESTB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // TESTB imm8, al
    if isImm8(v0) && v1 == AL {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xa8)
            m.imm1(toImmAny(v[0]))
        })
    }
    // TESTB imm8, r8
    if isImm8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0xf6)
            m.emit(0xc0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // TESTB r8, r8
    if isReg8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], isReg8REX(v[0]) || isReg8REX(v[1]))
            m.emit(0x84)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // TESTB imm8, m8
    if isImm8(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xf6)
            m.mrsd(0, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // TESTB r8, m8
    if isReg8(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), isReg8REX(v[0]))
            m.emit(0x84)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for TESTB")
    }
    return p
}

// TESTL performs "Logical Compare".
//
// Mnemonic        : TEST
// Supported forms : (5 forms)
//
//    * TESTL imm32, eax
//    * TESTL imm32, r32
//    * TESTL r32, r32
//    * TESTL imm32, m32
//    * TESTL r32, m32
//
func (self *Program) TESTL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // TESTL imm32, eax
    if isImm32(v0) && v1 == EAX {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xa9)
            m.imm4(toImmAny(v[0]))
        })
    }
    // TESTL imm32, r32
    if isImm32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0xf7)
            m.emit(0xc0 | lcode(v[1]))
            m.imm4(toImmAny(v[0]))
        })
    }
    // TESTL r32, r32
    if isReg32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x85)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // TESTL imm32, m32
    if isImm32(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0xf7)
            m.mrsd(0, addr(v[1]), 1)
            m.imm4(toImmAny(v[0]))
        })
    }
    // TESTL r32, m32
    if isReg32(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x85)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for TESTL")
    }
    return p
}

// TESTQ performs "Logical Compare".
//
// Mnemonic        : TEST
// Supported forms : (5 forms)
//
//    * TESTQ imm32, rax
//    * TESTQ imm32, r64
//    * TESTQ r64, r64
//    * TESTQ imm32, m64
//    * TESTQ r64, m64
//
func (self *Program) TESTQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // TESTQ imm32, rax
    if isImm32(v0) && v1 == RAX {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48)
            m.emit(0xa9)
            m.imm4(toImmAny(v[0]))
        })
    }
    // TESTQ imm32, r64
    if isImm32Ext(v0, 8) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0xf7)
            m.emit(0xc0 | lcode(v[1]))
            m.imm4(toImmAny(v[0]))
        })
    }
    // TESTQ r64, r64
    if isReg64(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[0]) << 2 | hcode(v[1]))
            m.emit(0x85)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // TESTQ imm32, m64
    if isImm32Ext(v0, 8) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0xf7)
            m.mrsd(0, addr(v[1]), 1)
            m.imm4(toImmAny(v[0]))
        })
    }
    // TESTQ r64, m64
    if isReg64(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[0]), addr(v[1]))
            m.emit(0x85)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for TESTQ")
    }
    return p
}

// TESTW performs "Logical Compare".
//
// Mnemonic        : TEST
// Supported forms : (5 forms)
//
//    * TESTW imm16, ax
//    * TESTW imm16, r16
//    * TESTW r16, r16
//    * TESTW imm16, m16
//    * TESTW r16, m16
//
func (self *Program) TESTW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // TESTW imm16, ax
    if isImm16(v0) && v1 == AX {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.emit(0xa9)
            m.imm2(toImmAny(v[0]))
        })
    }
    // TESTW imm16, r16
    if isImm16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0xf7)
            m.emit(0xc0 | lcode(v[1]))
            m.imm2(toImmAny(v[0]))
        })
    }
    // TESTW r16, r16
    if isReg16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x85)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // TESTW imm16, m16
    if isImm16(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0xf7)
            m.mrsd(0, addr(v[1]), 1)
            m.imm2(toImmAny(v[0]))
        })
    }
    // TESTW r16, m16
    if isReg16(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x85)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for TESTW")
    }
    return p
}

// TZCNTL performs "Count the Number of Trailing Zero Bits".
//
// Mnemonic        : TZCNT
// ISA extensions  : BMI
// Supported forms : (2 forms)
//
//    * TZCNTL r32, r32
//    * TZCNTL m32, r32
//
func (self *Program) TZCNTL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // TZCNTL r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_BMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xbc)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // TZCNTL m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_BMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xbc)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for TZCNTL")
    }
    return p
}

// TZCNTQ performs "Count the Number of Trailing Zero Bits".
//
// Mnemonic        : TZCNT
// ISA extensions  : BMI
// Supported forms : (2 forms)
//
//    * TZCNTQ r64, r64
//    * TZCNTQ m64, r64
//
func (self *Program) TZCNTQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // TZCNTQ r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_BMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x0f)
            m.emit(0xbc)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // TZCNTQ m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_BMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xf3)
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x0f)
            m.emit(0xbc)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for TZCNTQ")
    }
    return p
}

// TZCNTW performs "Count the Number of Trailing Zero Bits".
//
// Mnemonic        : TZCNT
// ISA extensions  : BMI
// Supported forms : (2 forms)
//
//    * TZCNTW r16, r16
//    * TZCNTW m16, r16
//
func (self *Program) TZCNTW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // TZCNTW r16, r16
    if isReg16(v0) && isReg16(v1) {
        self.require(ISA_BMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.emit(0xf3)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0xbc)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // TZCNTW m16, r16
    if isM16(v0) && isReg16(v1) {
        self.require(ISA_BMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.emit(0xf3)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0xbc)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for TZCNTW")
    }
    return p
}

// TZMSK performs "Mask From Trailing Zeros".
//
// Mnemonic        : TZMSK
// ISA extensions  : TBM
// Supported forms : (4 forms)
//
//    * TZMSK r32, r32
//    * TZMSK m32, r32
//    * TZMSK r64, r64
//    * TZMSK m64, r64
//
func (self *Program) TZMSK(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // TZMSK r32, r32
    if isReg32(v0) && isReg32(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[0]) << 5))
            m.emit(0x78 ^ (hlcode(v[1]) << 3))
            m.emit(0x01)
            m.emit(0xe0 | lcode(v[0]))
        })
    }
    // TZMSK m32, r32
    if isM32(v0) && isReg32(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, 0, addr(v[0]), hlcode(v[1]))
            m.emit(0x01)
            m.mrsd(4, addr(v[0]), 1)
        })
    }
    // TZMSK r64, r64
    if isReg64(v0) && isReg64(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[0]) << 5))
            m.emit(0xf8 ^ (hlcode(v[1]) << 3))
            m.emit(0x01)
            m.emit(0xe0 | lcode(v[0]))
        })
    }
    // TZMSK m64, r64
    if isM64(v0) && isReg64(v1) {
        self.require(ISA_TBM)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x80, 0, addr(v[0]), hlcode(v[1]))
            m.emit(0x01)
            m.mrsd(4, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for TZMSK")
    }
    return p
}

// UCOMISD performs "Unordered Compare Scalar Double-Precision Floating-Point Values and Set EFLAGS".
//
// Mnemonic        : UCOMISD
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * UCOMISD xmm, xmm
//    * UCOMISD m64, xmm
//
func (self *Program) UCOMISD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // UCOMISD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x2e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // UCOMISD m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x2e)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for UCOMISD")
    }
    return p
}

// UCOMISS performs "Unordered Compare Scalar Single-Precision Floating-Point Values and Set EFLAGS".
//
// Mnemonic        : UCOMISS
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * UCOMISS xmm, xmm
//    * UCOMISS m32, xmm
//
func (self *Program) UCOMISS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // UCOMISS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x2e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // UCOMISS m32, xmm
    if isM32(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x2e)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for UCOMISS")
    }
    return p
}

// UD2 performs "Undefined Instruction".
//
// Mnemonic        : UD2
// Supported forms : (1 form)
//
//    * UD2
//
func (self *Program) UD2() *Instruction {
    p := self.alloc(0, Operands{})
    // UD2
    p.add(0, func(m *_Encoding, v []interface{}) {
        m.emit(0x0f)
        m.emit(0x0b)
    })
    return p
}

// UNPCKHPD performs "Unpack and Interleave High Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : UNPCKHPD
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * UNPCKHPD xmm, xmm
//    * UNPCKHPD m128, xmm
//
func (self *Program) UNPCKHPD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // UNPCKHPD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x15)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // UNPCKHPD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x15)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for UNPCKHPD")
    }
    return p
}

// UNPCKHPS performs "Unpack and Interleave High Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : UNPCKHPS
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * UNPCKHPS xmm, xmm
//    * UNPCKHPS m128, xmm
//
func (self *Program) UNPCKHPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // UNPCKHPS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x15)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // UNPCKHPS m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x15)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for UNPCKHPS")
    }
    return p
}

// UNPCKLPD performs "Unpack and Interleave Low Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : UNPCKLPD
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * UNPCKLPD xmm, xmm
//    * UNPCKLPD m128, xmm
//
func (self *Program) UNPCKLPD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // UNPCKLPD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x14)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // UNPCKLPD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x14)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for UNPCKLPD")
    }
    return p
}

// UNPCKLPS performs "Unpack and Interleave Low Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : UNPCKLPS
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * UNPCKLPS xmm, xmm
//    * UNPCKLPS m128, xmm
//
func (self *Program) UNPCKLPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // UNPCKLPS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x14)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // UNPCKLPS m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x14)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for UNPCKLPS")
    }
    return p
}

// VADDPD performs "Add Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VADDPD
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (11 forms)
//
//    * VADDPD m128/m64bcst, xmm, xmm{k}{z}
//    * VADDPD xmm, xmm, xmm{k}{z}
//    * VADDPD m256/m64bcst, ymm, ymm{k}{z}
//    * VADDPD ymm, ymm, ymm{k}{z}
//    * VADDPD m512/m64bcst, zmm, zmm{k}{z}
//    * VADDPD xmm, xmm, xmm
//    * VADDPD m128, xmm, xmm
//    * VADDPD ymm, ymm, ymm
//    * VADDPD m256, ymm, ymm
//    * VADDPD {er}, zmm, zmm, zmm{k}{z}
//    * VADDPD zmm, zmm, zmm{k}{z}
//
func (self *Program) VADDPD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VADDPD takes 3 or 4 operands")
    }
    // VADDPD m128/m64bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x58)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VADDPD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x58)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VADDPD m256/m64bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x58)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VADDPD ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x58)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VADDPD m512/m64bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x58)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VADDPD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x58)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VADDPD m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x58)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VADDPD ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x58)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VADDPD m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x58)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VADDPD {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x58)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VADDPD zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x58)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VADDPD")
    }
    return p
}

// VADDPS performs "Add Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VADDPS
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (11 forms)
//
//    * VADDPS m128/m32bcst, xmm, xmm{k}{z}
//    * VADDPS xmm, xmm, xmm{k}{z}
//    * VADDPS m256/m32bcst, ymm, ymm{k}{z}
//    * VADDPS ymm, ymm, ymm{k}{z}
//    * VADDPS m512/m32bcst, zmm, zmm{k}{z}
//    * VADDPS xmm, xmm, xmm
//    * VADDPS m128, xmm, xmm
//    * VADDPS ymm, ymm, ymm
//    * VADDPS m256, ymm, ymm
//    * VADDPS {er}, zmm, zmm, zmm{k}{z}
//    * VADDPS zmm, zmm, zmm{k}{z}
//
func (self *Program) VADDPS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VADDPS takes 3 or 4 operands")
    }
    // VADDPS m128/m32bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x58)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VADDPS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x58)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VADDPS m256/m32bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x58)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VADDPS ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x58)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VADDPS m512/m32bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x58)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VADDPS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x58)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VADDPS m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x58)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VADDPS ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x58)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VADDPS m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x58)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VADDPS {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7c ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x58)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VADDPS zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x58)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VADDPS")
    }
    return p
}

// VADDSD performs "Add Scalar Double-Precision Floating-Point Values".
//
// Mnemonic        : VADDSD
// ISA extensions  : AVX, AVX512F
// Supported forms : (5 forms)
//
//    * VADDSD m64, xmm, xmm{k}{z}
//    * VADDSD xmm, xmm, xmm
//    * VADDSD m64, xmm, xmm
//    * VADDSD {er}, xmm, xmm, xmm{k}{z}
//    * VADDSD xmm, xmm, xmm{k}{z}
//
func (self *Program) VADDSD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VADDSD takes 3 or 4 operands")
    }
    // VADDSD m64, xmm, xmm{k}{z}
    if len(vv) == 0 && isM64(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x87, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x58)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    // VADDSD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x58)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VADDSD m64, xmm, xmm
    if len(vv) == 0 && isM64(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x58)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VADDSD {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xff ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x58)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VADDSD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xff ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x58)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VADDSD")
    }
    return p
}

// VADDSS performs "Add Scalar Single-Precision Floating-Point Values".
//
// Mnemonic        : VADDSS
// ISA extensions  : AVX, AVX512F
// Supported forms : (5 forms)
//
//    * VADDSS m32, xmm, xmm{k}{z}
//    * VADDSS xmm, xmm, xmm
//    * VADDSS m32, xmm, xmm
//    * VADDSS {er}, xmm, xmm, xmm{k}{z}
//    * VADDSS xmm, xmm, xmm{k}{z}
//
func (self *Program) VADDSS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VADDSS takes 3 or 4 operands")
    }
    // VADDSS m32, xmm, xmm{k}{z}
    if len(vv) == 0 && isM32(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x58)
            m.mrsd(lcode(v[2]), addr(v[0]), 4)
        })
    }
    // VADDSS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x58)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VADDSS m32, xmm, xmm
    if len(vv) == 0 && isM32(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x58)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VADDSS {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7e ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x58)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VADDSS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7e ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x58)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VADDSS")
    }
    return p
}

// VADDSUBPD performs "Packed Double-FP Add/Subtract".
//
// Mnemonic        : VADDSUBPD
// ISA extensions  : AVX
// Supported forms : (4 forms)
//
//    * VADDSUBPD xmm, xmm, xmm
//    * VADDSUBPD m128, xmm, xmm
//    * VADDSUBPD ymm, ymm, ymm
//    * VADDSUBPD m256, ymm, ymm
//
func (self *Program) VADDSUBPD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VADDSUBPD xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xd0)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VADDSUBPD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xd0)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VADDSUBPD ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xd0)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VADDSUBPD m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xd0)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VADDSUBPD")
    }
    return p
}

// VADDSUBPS performs "Packed Single-FP Add/Subtract".
//
// Mnemonic        : VADDSUBPS
// ISA extensions  : AVX
// Supported forms : (4 forms)
//
//    * VADDSUBPS xmm, xmm, xmm
//    * VADDSUBPS m128, xmm, xmm
//    * VADDSUBPS ymm, ymm, ymm
//    * VADDSUBPS m256, ymm, ymm
//
func (self *Program) VADDSUBPS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VADDSUBPS xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xd0)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VADDSUBPS m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xd0)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VADDSUBPS ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(7, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xd0)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VADDSUBPS m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(7, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xd0)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VADDSUBPS")
    }
    return p
}

// VAESDEC performs "Perform One Round of an AES Decryption Flow".
//
// Mnemonic        : VAESDEC
// ISA extensions  : AES, AVX
// Supported forms : (2 forms)
//
//    * VAESDEC xmm, xmm, xmm
//    * VAESDEC m128, xmm, xmm
//
func (self *Program) VAESDEC(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VAESDEC xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX | ISA_AES)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0xde)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VAESDEC m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX | ISA_AES)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xde)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VAESDEC")
    }
    return p
}

// VAESDECLAST performs "Perform Last Round of an AES Decryption Flow".
//
// Mnemonic        : VAESDECLAST
// ISA extensions  : AES, AVX
// Supported forms : (2 forms)
//
//    * VAESDECLAST xmm, xmm, xmm
//    * VAESDECLAST m128, xmm, xmm
//
func (self *Program) VAESDECLAST(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VAESDECLAST xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX | ISA_AES)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0xdf)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VAESDECLAST m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX | ISA_AES)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xdf)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VAESDECLAST")
    }
    return p
}

// VAESENC performs "Perform One Round of an AES Encryption Flow".
//
// Mnemonic        : VAESENC
// ISA extensions  : AES, AVX
// Supported forms : (2 forms)
//
//    * VAESENC xmm, xmm, xmm
//    * VAESENC m128, xmm, xmm
//
func (self *Program) VAESENC(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VAESENC xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX | ISA_AES)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0xdc)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VAESENC m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX | ISA_AES)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xdc)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VAESENC")
    }
    return p
}

// VAESENCLAST performs "Perform Last Round of an AES Encryption Flow".
//
// Mnemonic        : VAESENCLAST
// ISA extensions  : AES, AVX
// Supported forms : (2 forms)
//
//    * VAESENCLAST xmm, xmm, xmm
//    * VAESENCLAST m128, xmm, xmm
//
func (self *Program) VAESENCLAST(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VAESENCLAST xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX | ISA_AES)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0xdd)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VAESENCLAST m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX | ISA_AES)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xdd)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VAESENCLAST")
    }
    return p
}

// VAESIMC performs "Perform the AES InvMixColumn Transformation".
//
// Mnemonic        : VAESIMC
// ISA extensions  : AES, AVX
// Supported forms : (2 forms)
//
//    * VAESIMC xmm, xmm
//    * VAESIMC m128, xmm
//
func (self *Program) VAESIMC(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VAESIMC xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX | ISA_AES)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79)
            m.emit(0xdb)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VAESIMC m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_AVX | ISA_AES)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[1]), addr(v[0]), 0)
            m.emit(0xdb)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VAESIMC")
    }
    return p
}

// VAESKEYGENASSIST performs "AES Round Key Generation Assist".
//
// Mnemonic        : VAESKEYGENASSIST
// ISA extensions  : AES, AVX
// Supported forms : (2 forms)
//
//    * VAESKEYGENASSIST imm8, xmm, xmm
//    * VAESKEYGENASSIST imm8, m128, xmm
//
func (self *Program) VAESKEYGENASSIST(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VAESKEYGENASSIST imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX | ISA_AES)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79)
            m.emit(0xdf)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VAESKEYGENASSIST imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_AVX | ISA_AES)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[2]), addr(v[1]), 0)
            m.emit(0xdf)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VAESKEYGENASSIST")
    }
    return p
}

// VALIGND performs "Align Doubleword Vectors".
//
// Mnemonic        : VALIGND
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VALIGND imm8, m128/m32bcst, xmm, xmm{k}{z}
//    * VALIGND imm8, xmm, xmm, xmm{k}{z}
//    * VALIGND imm8, m256/m32bcst, ymm, ymm{k}{z}
//    * VALIGND imm8, ymm, ymm, ymm{k}{z}
//    * VALIGND imm8, m512/m32bcst, zmm, zmm{k}{z}
//    * VALIGND imm8, zmm, zmm, zmm{k}{z}
//
func (self *Program) VALIGND(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VALIGND imm8, m128/m32bcst, xmm, xmm{k}{z}
    if isImm8(v0) && isM128M32bcst(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0x03)
            m.mrsd(lcode(v[3]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VALIGND imm8, xmm, xmm, xmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x00)
            m.emit(0x03)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VALIGND imm8, m256/m32bcst, ymm, ymm{k}{z}
    if isImm8(v0) && isM256M32bcst(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b01, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0x03)
            m.mrsd(lcode(v[3]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VALIGND imm8, ymm, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x20)
            m.emit(0x03)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VALIGND imm8, m512/m32bcst, zmm, zmm{k}{z}
    if isImm8(v0) && isM512M32bcst(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0x03)
            m.mrsd(lcode(v[3]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VALIGND imm8, zmm, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x03)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VALIGND")
    }
    return p
}

// VALIGNQ performs "Align Quadword Vectors".
//
// Mnemonic        : VALIGNQ
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VALIGNQ imm8, m128/m64bcst, xmm, xmm{k}{z}
//    * VALIGNQ imm8, xmm, xmm, xmm{k}{z}
//    * VALIGNQ imm8, m256/m64bcst, ymm, ymm{k}{z}
//    * VALIGNQ imm8, ymm, ymm, ymm{k}{z}
//    * VALIGNQ imm8, m512/m64bcst, zmm, zmm{k}{z}
//    * VALIGNQ imm8, zmm, zmm, zmm{k}{z}
//
func (self *Program) VALIGNQ(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VALIGNQ imm8, m128/m64bcst, xmm, xmm{k}{z}
    if isImm8(v0) && isM128M64bcst(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0x03)
            m.mrsd(lcode(v[3]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VALIGNQ imm8, xmm, xmm, xmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x00)
            m.emit(0x03)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VALIGNQ imm8, m256/m64bcst, ymm, ymm{k}{z}
    if isImm8(v0) && isM256M64bcst(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b01, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0x03)
            m.mrsd(lcode(v[3]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VALIGNQ imm8, ymm, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x20)
            m.emit(0x03)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VALIGNQ imm8, m512/m64bcst, zmm, zmm{k}{z}
    if isImm8(v0) && isM512M64bcst(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0x03)
            m.mrsd(lcode(v[3]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VALIGNQ imm8, zmm, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x03)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VALIGNQ")
    }
    return p
}

// VANDNPD performs "Bitwise Logical AND NOT of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VANDNPD
// ISA extensions  : AVX, AVX512DQ, AVX512VL
// Supported forms : (10 forms)
//
//    * VANDNPD m128/m64bcst, xmm, xmm{k}{z}
//    * VANDNPD xmm, xmm, xmm{k}{z}
//    * VANDNPD m256/m64bcst, ymm, ymm{k}{z}
//    * VANDNPD ymm, ymm, ymm{k}{z}
//    * VANDNPD m512/m64bcst, zmm, zmm{k}{z}
//    * VANDNPD zmm, zmm, zmm{k}{z}
//    * VANDNPD xmm, xmm, xmm
//    * VANDNPD m128, xmm, xmm
//    * VANDNPD ymm, ymm, ymm
//    * VANDNPD m256, ymm, ymm
//
func (self *Program) VANDNPD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VANDNPD m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x55)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VANDNPD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x55)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VANDNPD m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x55)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VANDNPD ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x55)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VANDNPD m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x55)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VANDNPD zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x55)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VANDNPD xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x55)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VANDNPD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x55)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VANDNPD ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x55)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VANDNPD m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x55)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VANDNPD")
    }
    return p
}

// VANDNPS performs "Bitwise Logical AND NOT of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VANDNPS
// ISA extensions  : AVX, AVX512DQ, AVX512VL
// Supported forms : (10 forms)
//
//    * VANDNPS m128/m32bcst, xmm, xmm{k}{z}
//    * VANDNPS xmm, xmm, xmm{k}{z}
//    * VANDNPS m256/m32bcst, ymm, ymm{k}{z}
//    * VANDNPS ymm, ymm, ymm{k}{z}
//    * VANDNPS m512/m32bcst, zmm, zmm{k}{z}
//    * VANDNPS zmm, zmm, zmm{k}{z}
//    * VANDNPS xmm, xmm, xmm
//    * VANDNPS m128, xmm, xmm
//    * VANDNPS ymm, ymm, ymm
//    * VANDNPS m256, ymm, ymm
//
func (self *Program) VANDNPS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VANDNPS m128/m32bcst, xmm, xmm{k}{z}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x55)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VANDNPS xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x55)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VANDNPS m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x55)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VANDNPS ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x55)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VANDNPS m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x55)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VANDNPS zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x55)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VANDNPS xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x55)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VANDNPS m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x55)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VANDNPS ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x55)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VANDNPS m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x55)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VANDNPS")
    }
    return p
}

// VANDPD performs "Bitwise Logical AND of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VANDPD
// ISA extensions  : AVX, AVX512DQ, AVX512VL
// Supported forms : (10 forms)
//
//    * VANDPD m128/m64bcst, xmm, xmm{k}{z}
//    * VANDPD xmm, xmm, xmm{k}{z}
//    * VANDPD m256/m64bcst, ymm, ymm{k}{z}
//    * VANDPD ymm, ymm, ymm{k}{z}
//    * VANDPD m512/m64bcst, zmm, zmm{k}{z}
//    * VANDPD zmm, zmm, zmm{k}{z}
//    * VANDPD xmm, xmm, xmm
//    * VANDPD m128, xmm, xmm
//    * VANDPD ymm, ymm, ymm
//    * VANDPD m256, ymm, ymm
//
func (self *Program) VANDPD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VANDPD m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x54)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VANDPD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x54)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VANDPD m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x54)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VANDPD ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x54)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VANDPD m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x54)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VANDPD zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x54)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VANDPD xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x54)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VANDPD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x54)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VANDPD ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x54)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VANDPD m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x54)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VANDPD")
    }
    return p
}

// VANDPS performs "Bitwise Logical AND of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VANDPS
// ISA extensions  : AVX, AVX512DQ, AVX512VL
// Supported forms : (10 forms)
//
//    * VANDPS m128/m32bcst, xmm, xmm{k}{z}
//    * VANDPS xmm, xmm, xmm{k}{z}
//    * VANDPS m256/m32bcst, ymm, ymm{k}{z}
//    * VANDPS ymm, ymm, ymm{k}{z}
//    * VANDPS m512/m32bcst, zmm, zmm{k}{z}
//    * VANDPS zmm, zmm, zmm{k}{z}
//    * VANDPS xmm, xmm, xmm
//    * VANDPS m128, xmm, xmm
//    * VANDPS ymm, ymm, ymm
//    * VANDPS m256, ymm, ymm
//
func (self *Program) VANDPS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VANDPS m128/m32bcst, xmm, xmm{k}{z}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x54)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VANDPS xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x54)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VANDPS m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x54)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VANDPS ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x54)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VANDPS m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x54)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VANDPS zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x54)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VANDPS xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x54)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VANDPS m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x54)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VANDPS ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x54)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VANDPS m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x54)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VANDPS")
    }
    return p
}

// VBLENDMPD performs "Blend Packed Double-Precision Floating-Point Vectors Using an OpMask Control".
//
// Mnemonic        : VBLENDMPD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VBLENDMPD m128/m64bcst, xmm, xmm{k}{z}
//    * VBLENDMPD xmm, xmm, xmm{k}{z}
//    * VBLENDMPD m256/m64bcst, ymm, ymm{k}{z}
//    * VBLENDMPD ymm, ymm, ymm{k}{z}
//    * VBLENDMPD m512/m64bcst, zmm, zmm{k}{z}
//    * VBLENDMPD zmm, zmm, zmm{k}{z}
//
func (self *Program) VBLENDMPD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VBLENDMPD m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x65)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VBLENDMPD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x65)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VBLENDMPD m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x65)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VBLENDMPD ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x65)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VBLENDMPD m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x65)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VBLENDMPD zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x65)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VBLENDMPD")
    }
    return p
}

// VBLENDMPS performs "Blend Packed Single-Precision Floating-Point Vectors Using an OpMask Control".
//
// Mnemonic        : VBLENDMPS
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VBLENDMPS m128/m32bcst, xmm, xmm{k}{z}
//    * VBLENDMPS xmm, xmm, xmm{k}{z}
//    * VBLENDMPS m256/m32bcst, ymm, ymm{k}{z}
//    * VBLENDMPS ymm, ymm, ymm{k}{z}
//    * VBLENDMPS m512/m32bcst, zmm, zmm{k}{z}
//    * VBLENDMPS zmm, zmm, zmm{k}{z}
//
func (self *Program) VBLENDMPS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VBLENDMPS m128/m32bcst, xmm, xmm{k}{z}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x65)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VBLENDMPS xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x65)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VBLENDMPS m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x65)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VBLENDMPS ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x65)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VBLENDMPS m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x65)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VBLENDMPS zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x65)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VBLENDMPS")
    }
    return p
}

// VBLENDPD performs "Blend Packed Double Precision Floating-Point Values".
//
// Mnemonic        : VBLENDPD
// ISA extensions  : AVX
// Supported forms : (4 forms)
//
//    * VBLENDPD imm8, xmm, xmm, xmm
//    * VBLENDPD imm8, m128, xmm, xmm
//    * VBLENDPD imm8, ymm, ymm, ymm
//    * VBLENDPD imm8, m256, ymm, ymm
//
func (self *Program) VBLENDPD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VBLENDPD imm8, xmm, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x0d)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VBLENDPD imm8, m128, xmm, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x0d)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VBLENDPD imm8, ymm, ymm, ymm
    if isImm8(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit(0x0d)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VBLENDPD imm8, m256, ymm, ymm
    if isImm8(v0) && isM256(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x0d)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VBLENDPD")
    }
    return p
}

// VBLENDPS performs " Blend Packed Single Precision Floating-Point Values".
//
// Mnemonic        : VBLENDPS
// ISA extensions  : AVX
// Supported forms : (4 forms)
//
//    * VBLENDPS imm8, xmm, xmm, xmm
//    * VBLENDPS imm8, m128, xmm, xmm
//    * VBLENDPS imm8, ymm, ymm, ymm
//    * VBLENDPS imm8, m256, ymm, ymm
//
func (self *Program) VBLENDPS(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VBLENDPS imm8, xmm, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x0c)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VBLENDPS imm8, m128, xmm, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x0c)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VBLENDPS imm8, ymm, ymm, ymm
    if isImm8(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit(0x0c)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VBLENDPS imm8, m256, ymm, ymm
    if isImm8(v0) && isM256(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x0c)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VBLENDPS")
    }
    return p
}

// VBLENDVPD performs " Variable Blend Packed Double Precision Floating-Point Values".
//
// Mnemonic        : VBLENDVPD
// ISA extensions  : AVX
// Supported forms : (4 forms)
//
//    * VBLENDVPD xmm, xmm, xmm, xmm
//    * VBLENDVPD xmm, m128, xmm, xmm
//    * VBLENDVPD ymm, ymm, ymm, ymm
//    * VBLENDVPD ymm, m256, ymm, ymm
//
func (self *Program) VBLENDVPD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VBLENDVPD xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x4b)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VBLENDVPD xmm, m128, xmm, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x4b)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VBLENDVPD ymm, ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit(0x4b)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VBLENDVPD ymm, m256, ymm, ymm
    if isYMM(v0) && isM256(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x4b)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VBLENDVPD")
    }
    return p
}

// VBLENDVPS performs " Variable Blend Packed Single Precision Floating-Point Values".
//
// Mnemonic        : VBLENDVPS
// ISA extensions  : AVX
// Supported forms : (4 forms)
//
//    * VBLENDVPS xmm, xmm, xmm, xmm
//    * VBLENDVPS xmm, m128, xmm, xmm
//    * VBLENDVPS ymm, ymm, ymm, ymm
//    * VBLENDVPS ymm, m256, ymm, ymm
//
func (self *Program) VBLENDVPS(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VBLENDVPS xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x4a)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VBLENDVPS xmm, m128, xmm, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x4a)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VBLENDVPS ymm, ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit(0x4a)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VBLENDVPS ymm, m256, ymm, ymm
    if isYMM(v0) && isM256(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x4a)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VBLENDVPS")
    }
    return p
}

// VBROADCASTF128 performs "Broadcast 128 Bit of Floating-Point Data".
//
// Mnemonic        : VBROADCASTF128
// ISA extensions  : AVX
// Supported forms : (1 form)
//
//    * VBROADCASTF128 m128, ymm
//
func (self *Program) VBROADCASTF128(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VBROADCASTF128 m128, ymm
    if isM128(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x1a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VBROADCASTF128")
    }
    return p
}

// VBROADCASTF32X2 performs "Broadcast Two Single-Precision Floating-Point Elements".
//
// Mnemonic        : VBROADCASTF32X2
// ISA extensions  : AVX512DQ, AVX512VL
// Supported forms : (4 forms)
//
//    * VBROADCASTF32X2 xmm, ymm{k}{z}
//    * VBROADCASTF32X2 xmm, zmm{k}{z}
//    * VBROADCASTF32X2 m64, ymm{k}{z}
//    * VBROADCASTF32X2 m64, zmm{k}{z}
//
func (self *Program) VBROADCASTF32X2(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VBROADCASTF32X2 xmm, ymm{k}{z}
    if isEVEXXMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x19)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VBROADCASTF32X2 xmm, zmm{k}{z}
    if isEVEXXMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x19)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VBROADCASTF32X2 m64, ymm{k}{z}
    if isM64(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x19)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VBROADCASTF32X2 m64, zmm{k}{z}
    if isM64(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x19)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VBROADCASTF32X2")
    }
    return p
}

// VBROADCASTF32X4 performs "Broadcast Four Single-Precision Floating-Point Elements".
//
// Mnemonic        : VBROADCASTF32X4
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (2 forms)
//
//    * VBROADCASTF32X4 m128, ymm{k}{z}
//    * VBROADCASTF32X4 m128, zmm{k}{z}
//
func (self *Program) VBROADCASTF32X4(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VBROADCASTF32X4 m128, ymm{k}{z}
    if isM128(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x1a)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VBROADCASTF32X4 m128, zmm{k}{z}
    if isM128(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x1a)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VBROADCASTF32X4")
    }
    return p
}

// VBROADCASTF32X8 performs "Broadcast Eight Single-Precision Floating-Point Elements".
//
// Mnemonic        : VBROADCASTF32X8
// ISA extensions  : AVX512DQ
// Supported forms : (1 form)
//
//    * VBROADCASTF32X8 m256, zmm{k}{z}
//
func (self *Program) VBROADCASTF32X8(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VBROADCASTF32X8 m256, zmm{k}{z}
    if isM256(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x1b)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VBROADCASTF32X8")
    }
    return p
}

// VBROADCASTF64X2 performs "Broadcast Two Double-Precision Floating-Point Elements".
//
// Mnemonic        : VBROADCASTF64X2
// ISA extensions  : AVX512DQ, AVX512VL
// Supported forms : (2 forms)
//
//    * VBROADCASTF64X2 m128, ymm{k}{z}
//    * VBROADCASTF64X2 m128, zmm{k}{z}
//
func (self *Program) VBROADCASTF64X2(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VBROADCASTF64X2 m128, ymm{k}{z}
    if isM128(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x1a)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VBROADCASTF64X2 m128, zmm{k}{z}
    if isM128(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x1a)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VBROADCASTF64X2")
    }
    return p
}

// VBROADCASTF64X4 performs "Broadcast Four Double-Precision Floating-Point Elements".
//
// Mnemonic        : VBROADCASTF64X4
// ISA extensions  : AVX512F
// Supported forms : (1 form)
//
//    * VBROADCASTF64X4 m256, zmm{k}{z}
//
func (self *Program) VBROADCASTF64X4(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VBROADCASTF64X4 m256, zmm{k}{z}
    if isM256(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x1b)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VBROADCASTF64X4")
    }
    return p
}

// VBROADCASTI128 performs "Broadcast 128 Bits of Integer Data".
//
// Mnemonic        : VBROADCASTI128
// ISA extensions  : AVX2
// Supported forms : (1 form)
//
//    * VBROADCASTI128 m128, ymm
//
func (self *Program) VBROADCASTI128(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VBROADCASTI128 m128, ymm
    if isM128(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x5a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VBROADCASTI128")
    }
    return p
}

// VBROADCASTI32X2 performs "Broadcast Two Doubleword Elements".
//
// Mnemonic        : VBROADCASTI32X2
// ISA extensions  : AVX512DQ, AVX512VL
// Supported forms : (6 forms)
//
//    * VBROADCASTI32X2 xmm, xmm{k}{z}
//    * VBROADCASTI32X2 xmm, ymm{k}{z}
//    * VBROADCASTI32X2 xmm, zmm{k}{z}
//    * VBROADCASTI32X2 m64, xmm{k}{z}
//    * VBROADCASTI32X2 m64, ymm{k}{z}
//    * VBROADCASTI32X2 m64, zmm{k}{z}
//
func (self *Program) VBROADCASTI32X2(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VBROADCASTI32X2 xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x59)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VBROADCASTI32X2 xmm, ymm{k}{z}
    if isEVEXXMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x59)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VBROADCASTI32X2 xmm, zmm{k}{z}
    if isEVEXXMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x59)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VBROADCASTI32X2 m64, xmm{k}{z}
    if isM64(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x59)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VBROADCASTI32X2 m64, ymm{k}{z}
    if isM64(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x59)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VBROADCASTI32X2 m64, zmm{k}{z}
    if isM64(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x59)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VBROADCASTI32X2")
    }
    return p
}

// VBROADCASTI32X4 performs "Broadcast Four Doubleword Elements".
//
// Mnemonic        : VBROADCASTI32X4
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (2 forms)
//
//    * VBROADCASTI32X4 m128, ymm{k}{z}
//    * VBROADCASTI32X4 m128, zmm{k}{z}
//
func (self *Program) VBROADCASTI32X4(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VBROADCASTI32X4 m128, ymm{k}{z}
    if isM128(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x5a)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VBROADCASTI32X4 m128, zmm{k}{z}
    if isM128(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x5a)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VBROADCASTI32X4")
    }
    return p
}

// VBROADCASTI32X8 performs "Broadcast Eight Doubleword Elements".
//
// Mnemonic        : VBROADCASTI32X8
// ISA extensions  : AVX512DQ
// Supported forms : (1 form)
//
//    * VBROADCASTI32X8 m256, zmm{k}{z}
//
func (self *Program) VBROADCASTI32X8(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VBROADCASTI32X8 m256, zmm{k}{z}
    if isM256(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x5b)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VBROADCASTI32X8")
    }
    return p
}

// VBROADCASTI64X2 performs "Broadcast Two Quadword Elements".
//
// Mnemonic        : VBROADCASTI64X2
// ISA extensions  : AVX512DQ, AVX512VL
// Supported forms : (2 forms)
//
//    * VBROADCASTI64X2 m128, ymm{k}{z}
//    * VBROADCASTI64X2 m128, zmm{k}{z}
//
func (self *Program) VBROADCASTI64X2(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VBROADCASTI64X2 m128, ymm{k}{z}
    if isM128(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x5a)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VBROADCASTI64X2 m128, zmm{k}{z}
    if isM128(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x5a)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VBROADCASTI64X2")
    }
    return p
}

// VBROADCASTI64X4 performs "Broadcast Four Quadword Elements".
//
// Mnemonic        : VBROADCASTI64X4
// ISA extensions  : AVX512F
// Supported forms : (1 form)
//
//    * VBROADCASTI64X4 m256, zmm{k}{z}
//
func (self *Program) VBROADCASTI64X4(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VBROADCASTI64X4 m256, zmm{k}{z}
    if isM256(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x5b)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VBROADCASTI64X4")
    }
    return p
}

// VBROADCASTSD performs "Broadcast Double-Precision Floating-Point Element".
//
// Mnemonic        : VBROADCASTSD
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VBROADCASTSD xmm, ymm{k}{z}
//    * VBROADCASTSD xmm, zmm{k}{z}
//    * VBROADCASTSD m64, ymm{k}{z}
//    * VBROADCASTSD m64, zmm{k}{z}
//    * VBROADCASTSD xmm, ymm
//    * VBROADCASTSD m64, ymm
//
func (self *Program) VBROADCASTSD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VBROADCASTSD xmm, ymm{k}{z}
    if isEVEXXMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x19)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VBROADCASTSD xmm, zmm{k}{z}
    if isEVEXXMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x19)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VBROADCASTSD m64, ymm{k}{z}
    if isM64(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x19)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VBROADCASTSD m64, zmm{k}{z}
    if isM64(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x19)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VBROADCASTSD xmm, ymm
    if isXMM(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d)
            m.emit(0x19)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VBROADCASTSD m64, ymm
    if isM64(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x19)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VBROADCASTSD")
    }
    return p
}

// VBROADCASTSS performs "Broadcast Single-Precision Floating-Point Element".
//
// Mnemonic        : VBROADCASTSS
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (8 forms)
//
//    * VBROADCASTSS xmm, ymm{k}{z}
//    * VBROADCASTSS xmm, zmm{k}{z}
//    * VBROADCASTSS m32, ymm{k}{z}
//    * VBROADCASTSS m32, zmm{k}{z}
//    * VBROADCASTSS xmm, xmm
//    * VBROADCASTSS m32, xmm
//    * VBROADCASTSS xmm, ymm
//    * VBROADCASTSS m32, ymm
//
func (self *Program) VBROADCASTSS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VBROADCASTSS xmm, ymm{k}{z}
    if isEVEXXMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x18)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VBROADCASTSS xmm, zmm{k}{z}
    if isEVEXXMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x18)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VBROADCASTSS m32, ymm{k}{z}
    if isM32(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x18)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VBROADCASTSS m32, zmm{k}{z}
    if isM32(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x18)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VBROADCASTSS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79)
            m.emit(0x18)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VBROADCASTSS m32, xmm
    if isM32(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x18)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VBROADCASTSS xmm, ymm
    if isXMM(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d)
            m.emit(0x18)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VBROADCASTSS m32, ymm
    if isM32(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x18)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VBROADCASTSS")
    }
    return p
}

// VCMPPD performs "Compare Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VCMPPD
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (11 forms)
//
//    * VCMPPD imm8, m128/m64bcst, xmm, k{k}
//    * VCMPPD imm8, xmm, xmm, k{k}
//    * VCMPPD imm8, m256/m64bcst, ymm, k{k}
//    * VCMPPD imm8, ymm, ymm, k{k}
//    * VCMPPD imm8, m512/m64bcst, zmm, k{k}
//    * VCMPPD imm8, xmm, xmm, xmm
//    * VCMPPD imm8, m128, xmm, xmm
//    * VCMPPD imm8, ymm, ymm, ymm
//    * VCMPPD imm8, m256, ymm, ymm
//    * VCMPPD imm8, {sae}, zmm, zmm, k{k}
//    * VCMPPD imm8, zmm, zmm, k{k}
//
func (self *Program) VCMPPD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(4, Operands{v0, v1, v2, v3})
        case 1  : p = self.alloc(5, Operands{v0, v1, v2, v3, vv[0]})
        default : panic("instruction VCMPPD takes 4 or 5 operands")
    }
    // VCMPPD imm8, m128/m64bcst, xmm, k{k}
    if len(vv) == 0 && isImm8(v0) && isM128M64bcst(v1) && isEVEXXMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), 0, bcode(v[1]))
            m.emit(0xc2)
            m.mrsd(lcode(v[3]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCMPPD imm8, xmm, xmm, k{k}
    if len(vv) == 0 && isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x00)
            m.emit(0xc2)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCMPPD imm8, m256/m64bcst, ymm, k{k}
    if len(vv) == 0 && isImm8(v0) && isM256M64bcst(v1) && isEVEXYMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), 0, bcode(v[1]))
            m.emit(0xc2)
            m.mrsd(lcode(v[3]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCMPPD imm8, ymm, ymm, k{k}
    if len(vv) == 0 && isImm8(v0) && isEVEXYMM(v1) && isEVEXYMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x20)
            m.emit(0xc2)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCMPPD imm8, m512/m64bcst, zmm, k{k}
    if len(vv) == 0 && isImm8(v0) && isM512M64bcst(v1) && isZMM(v2) && isKk(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), 0, bcode(v[1]))
            m.emit(0xc2)
            m.mrsd(lcode(v[3]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCMPPD imm8, xmm, xmm, xmm
    if len(vv) == 0 && isImm8(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[3]), v[1], hlcode(v[2]))
            m.emit(0xc2)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCMPPD imm8, m128, xmm, xmm
    if len(vv) == 0 && isImm8(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0xc2)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCMPPD imm8, ymm, ymm, ymm
    if len(vv) == 0 && isImm8(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[3]), v[1], hlcode(v[2]))
            m.emit(0xc2)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCMPPD imm8, m256, ymm, ymm
    if len(vv) == 0 && isImm8(v0) && isM256(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0xc2)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCMPPD imm8, {sae}, zmm, zmm, k{k}
    if len(vv) == 1 && isImm8(v0) && isSAE(v1) && isZMM(v2) && isZMM(v3) && isKk(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[4]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[4]) << 4)))
            m.emit(0xfd ^ (hlcode(v[3]) << 3))
            m.emit((0x08 ^ (ecode(v[3]) << 3)) | kcode(v[4]) | 0x10)
            m.emit(0xc2)
            m.emit(0xc0 | lcode(v[4]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCMPPD imm8, zmm, zmm, k{k}
    if len(vv) == 0 && isImm8(v0) && isZMM(v1) && isZMM(v2) && isKk(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0xc2)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCMPPD")
    }
    return p
}

// VCMPPS performs "Compare Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VCMPPS
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (11 forms)
//
//    * VCMPPS imm8, m128/m32bcst, xmm, k{k}
//    * VCMPPS imm8, xmm, xmm, k{k}
//    * VCMPPS imm8, m256/m32bcst, ymm, k{k}
//    * VCMPPS imm8, ymm, ymm, k{k}
//    * VCMPPS imm8, m512/m32bcst, zmm, k{k}
//    * VCMPPS imm8, xmm, xmm, xmm
//    * VCMPPS imm8, m128, xmm, xmm
//    * VCMPPS imm8, ymm, ymm, ymm
//    * VCMPPS imm8, m256, ymm, ymm
//    * VCMPPS imm8, {sae}, zmm, zmm, k{k}
//    * VCMPPS imm8, zmm, zmm, k{k}
//
func (self *Program) VCMPPS(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(4, Operands{v0, v1, v2, v3})
        case 1  : p = self.alloc(5, Operands{v0, v1, v2, v3, vv[0]})
        default : panic("instruction VCMPPS takes 4 or 5 operands")
    }
    // VCMPPS imm8, m128/m32bcst, xmm, k{k}
    if len(vv) == 0 && isImm8(v0) && isM128M32bcst(v1) && isEVEXXMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), 0, bcode(v[1]))
            m.emit(0xc2)
            m.mrsd(lcode(v[3]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCMPPS imm8, xmm, xmm, k{k}
    if len(vv) == 0 && isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7c ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x00)
            m.emit(0xc2)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCMPPS imm8, m256/m32bcst, ymm, k{k}
    if len(vv) == 0 && isImm8(v0) && isM256M32bcst(v1) && isEVEXYMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b01, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), 0, bcode(v[1]))
            m.emit(0xc2)
            m.mrsd(lcode(v[3]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCMPPS imm8, ymm, ymm, k{k}
    if len(vv) == 0 && isImm8(v0) && isEVEXYMM(v1) && isEVEXYMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7c ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x20)
            m.emit(0xc2)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCMPPS imm8, m512/m32bcst, zmm, k{k}
    if len(vv) == 0 && isImm8(v0) && isM512M32bcst(v1) && isZMM(v2) && isKk(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), 0, bcode(v[1]))
            m.emit(0xc2)
            m.mrsd(lcode(v[3]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCMPPS imm8, xmm, xmm, xmm
    if len(vv) == 0 && isImm8(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[3]), v[1], hlcode(v[2]))
            m.emit(0xc2)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCMPPS imm8, m128, xmm, xmm
    if len(vv) == 0 && isImm8(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0xc2)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCMPPS imm8, ymm, ymm, ymm
    if len(vv) == 0 && isImm8(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[3]), v[1], hlcode(v[2]))
            m.emit(0xc2)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCMPPS imm8, m256, ymm, ymm
    if len(vv) == 0 && isImm8(v0) && isM256(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0xc2)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCMPPS imm8, {sae}, zmm, zmm, k{k}
    if len(vv) == 1 && isImm8(v0) && isSAE(v1) && isZMM(v2) && isZMM(v3) && isKk(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[4]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[4]) << 4)))
            m.emit(0x7c ^ (hlcode(v[3]) << 3))
            m.emit((0x08 ^ (ecode(v[3]) << 3)) | kcode(v[4]) | 0x10)
            m.emit(0xc2)
            m.emit(0xc0 | lcode(v[4]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCMPPS imm8, zmm, zmm, k{k}
    if len(vv) == 0 && isImm8(v0) && isZMM(v1) && isZMM(v2) && isKk(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7c ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0xc2)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCMPPS")
    }
    return p
}

// VCMPSD performs "Compare Scalar Double-Precision Floating-Point Values".
//
// Mnemonic        : VCMPSD
// ISA extensions  : AVX, AVX512F
// Supported forms : (5 forms)
//
//    * VCMPSD imm8, m64, xmm, k{k}
//    * VCMPSD imm8, xmm, xmm, xmm
//    * VCMPSD imm8, m64, xmm, xmm
//    * VCMPSD imm8, {sae}, xmm, xmm, k{k}
//    * VCMPSD imm8, xmm, xmm, k{k}
//
func (self *Program) VCMPSD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(4, Operands{v0, v1, v2, v3})
        case 1  : p = self.alloc(5, Operands{v0, v1, v2, v3, vv[0]})
        default : panic("instruction VCMPSD takes 4 or 5 operands")
    }
    // VCMPSD imm8, m64, xmm, k{k}
    if len(vv) == 0 && isImm8(v0) && isM64(v1) && isEVEXXMM(v2) && isKk(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x87, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), 0, 0)
            m.emit(0xc2)
            m.mrsd(lcode(v[3]), addr(v[1]), 8)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCMPSD imm8, xmm, xmm, xmm
    if len(vv) == 0 && isImm8(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[3]), v[1], hlcode(v[2]))
            m.emit(0xc2)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCMPSD imm8, m64, xmm, xmm
    if len(vv) == 0 && isImm8(v0) && isM64(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0xc2)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCMPSD imm8, {sae}, xmm, xmm, k{k}
    if len(vv) == 1 && isImm8(v0) && isSAE(v1) && isEVEXXMM(v2) && isEVEXXMM(v3) && isKk(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[4]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[4]) << 4)))
            m.emit(0xff ^ (hlcode(v[3]) << 3))
            m.emit((0x08 ^ (ecode(v[3]) << 3)) | kcode(v[4]) | 0x10)
            m.emit(0xc2)
            m.emit(0xc0 | lcode(v[4]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCMPSD imm8, xmm, xmm, k{k}
    if len(vv) == 0 && isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isKk(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xff ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0xc2)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCMPSD")
    }
    return p
}

// VCMPSS performs "Compare Scalar Single-Precision Floating-Point Values".
//
// Mnemonic        : VCMPSS
// ISA extensions  : AVX, AVX512F
// Supported forms : (5 forms)
//
//    * VCMPSS imm8, m32, xmm, k{k}
//    * VCMPSS imm8, xmm, xmm, xmm
//    * VCMPSS imm8, m32, xmm, xmm
//    * VCMPSS imm8, {sae}, xmm, xmm, k{k}
//    * VCMPSS imm8, xmm, xmm, k{k}
//
func (self *Program) VCMPSS(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(4, Operands{v0, v1, v2, v3})
        case 1  : p = self.alloc(5, Operands{v0, v1, v2, v3, vv[0]})
        default : panic("instruction VCMPSS takes 4 or 5 operands")
    }
    // VCMPSS imm8, m32, xmm, k{k}
    if len(vv) == 0 && isImm8(v0) && isM32(v1) && isEVEXXMM(v2) && isKk(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), 0, 0)
            m.emit(0xc2)
            m.mrsd(lcode(v[3]), addr(v[1]), 4)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCMPSS imm8, xmm, xmm, xmm
    if len(vv) == 0 && isImm8(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[3]), v[1], hlcode(v[2]))
            m.emit(0xc2)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCMPSS imm8, m32, xmm, xmm
    if len(vv) == 0 && isImm8(v0) && isM32(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0xc2)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCMPSS imm8, {sae}, xmm, xmm, k{k}
    if len(vv) == 1 && isImm8(v0) && isSAE(v1) && isEVEXXMM(v2) && isEVEXXMM(v3) && isKk(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[4]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[4]) << 4)))
            m.emit(0x7e ^ (hlcode(v[3]) << 3))
            m.emit((0x08 ^ (ecode(v[3]) << 3)) | kcode(v[4]) | 0x10)
            m.emit(0xc2)
            m.emit(0xc0 | lcode(v[4]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCMPSS imm8, xmm, xmm, k{k}
    if len(vv) == 0 && isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isKk(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7e ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0xc2)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCMPSS")
    }
    return p
}

// VCOMISD performs "Compare Scalar Ordered Double-Precision Floating-Point Values and Set EFLAGS".
//
// Mnemonic        : VCOMISD
// ISA extensions  : AVX, AVX512F
// Supported forms : (5 forms)
//
//    * VCOMISD xmm, xmm
//    * VCOMISD m64, xmm
//    * VCOMISD m64, xmm
//    * VCOMISD {sae}, xmm, xmm
//    * VCOMISD xmm, xmm
//
func (self *Program) VCOMISD(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCOMISD takes 2 or 3 operands")
    }
    // VCOMISD xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[1]), v[0], 0)
            m.emit(0x2f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCOMISD m64, xmm
    if len(vv) == 0 && isM64(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x2f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VCOMISD m64, xmm
    if len(vv) == 0 && isM64(v0) && isEVEXXMM(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[1]), addr(v[0]), 0, 0, 0, 0)
            m.emit(0x2f)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VCOMISD {sae}, xmm, xmm
    if len(vv) == 1 && isSAE(v0) && isEVEXXMM(v1) && isEVEXXMM(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd)
            m.emit(0x18)
            m.emit(0x2f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCOMISD xmm, xmm
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit(0x48)
            m.emit(0x2f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCOMISD")
    }
    return p
}

// VCOMISS performs "Compare Scalar Ordered Single-Precision Floating-Point Values and Set EFLAGS".
//
// Mnemonic        : VCOMISS
// ISA extensions  : AVX, AVX512F
// Supported forms : (5 forms)
//
//    * VCOMISS xmm, xmm
//    * VCOMISS m32, xmm
//    * VCOMISS m32, xmm
//    * VCOMISS {sae}, xmm, xmm
//    * VCOMISS xmm, xmm
//
func (self *Program) VCOMISS(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCOMISS takes 2 or 3 operands")
    }
    // VCOMISS xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[1]), v[0], 0)
            m.emit(0x2f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCOMISS m32, xmm
    if len(vv) == 0 && isM32(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x2f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VCOMISS m32, xmm
    if len(vv) == 0 && isM32(v0) && isEVEXXMM(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b00, ehcode(v[1]), addr(v[0]), 0, 0, 0, 0)
            m.emit(0x2f)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VCOMISS {sae}, xmm, xmm
    if len(vv) == 1 && isSAE(v0) && isEVEXXMM(v1) && isEVEXXMM(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c)
            m.emit(0x18)
            m.emit(0x2f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCOMISS xmm, xmm
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7c)
            m.emit(0x48)
            m.emit(0x2f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCOMISS")
    }
    return p
}

// VCOMPRESSPD performs "Store Sparse Packed Double-Precision Floating-Point Values into Dense Memory/Register".
//
// Mnemonic        : VCOMPRESSPD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VCOMPRESSPD xmm, xmm{k}{z}
//    * VCOMPRESSPD xmm, m128{k}{z}
//    * VCOMPRESSPD ymm, ymm{k}{z}
//    * VCOMPRESSPD ymm, m256{k}{z}
//    * VCOMPRESSPD zmm, zmm{k}{z}
//    * VCOMPRESSPD zmm, m512{k}{z}
//
func (self *Program) VCOMPRESSPD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VCOMPRESSPD xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x8a)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VCOMPRESSPD xmm, m128{k}{z}
    if isEVEXXMM(v0) && isM128kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x8a)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    // VCOMPRESSPD ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x8a)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VCOMPRESSPD ymm, m256{k}{z}
    if isEVEXYMM(v0) && isM256kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x8a)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    // VCOMPRESSPD zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x8a)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VCOMPRESSPD zmm, m512{k}{z}
    if isZMM(v0) && isM512kz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x8a)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCOMPRESSPD")
    }
    return p
}

// VCOMPRESSPS performs "Store Sparse Packed Single-Precision Floating-Point Values into Dense Memory/Register".
//
// Mnemonic        : VCOMPRESSPS
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VCOMPRESSPS xmm, xmm{k}{z}
//    * VCOMPRESSPS xmm, m128{k}{z}
//    * VCOMPRESSPS ymm, ymm{k}{z}
//    * VCOMPRESSPS ymm, m256{k}{z}
//    * VCOMPRESSPS zmm, zmm{k}{z}
//    * VCOMPRESSPS zmm, m512{k}{z}
//
func (self *Program) VCOMPRESSPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VCOMPRESSPS xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x8a)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VCOMPRESSPS xmm, m128{k}{z}
    if isEVEXXMM(v0) && isM128kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x8a)
            m.mrsd(lcode(v[0]), addr(v[1]), 4)
        })
    }
    // VCOMPRESSPS ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x8a)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VCOMPRESSPS ymm, m256{k}{z}
    if isEVEXYMM(v0) && isM256kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x8a)
            m.mrsd(lcode(v[0]), addr(v[1]), 4)
        })
    }
    // VCOMPRESSPS zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x8a)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VCOMPRESSPS zmm, m512{k}{z}
    if isZMM(v0) && isM512kz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x8a)
            m.mrsd(lcode(v[0]), addr(v[1]), 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCOMPRESSPS")
    }
    return p
}

// VCVTDQ2PD performs "Convert Packed Dword Integers to Packed Double-Precision FP Values".
//
// Mnemonic        : VCVTDQ2PD
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VCVTDQ2PD m64/m32bcst, xmm{k}{z}
//    * VCVTDQ2PD m128/m32bcst, ymm{k}{z}
//    * VCVTDQ2PD m256/m32bcst, zmm{k}{z}
//    * VCVTDQ2PD xmm, xmm{k}{z}
//    * VCVTDQ2PD xmm, ymm{k}{z}
//    * VCVTDQ2PD ymm, zmm{k}{z}
//    * VCVTDQ2PD xmm, xmm
//    * VCVTDQ2PD m64, xmm
//    * VCVTDQ2PD xmm, ymm
//    * VCVTDQ2PD m128, ymm
//
func (self *Program) VCVTDQ2PD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VCVTDQ2PD m64/m32bcst, xmm{k}{z}
    if isM64M32bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0xe6)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VCVTDQ2PD m128/m32bcst, ymm{k}{z}
    if isM128M32bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0xe6)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VCVTDQ2PD m256/m32bcst, zmm{k}{z}
    if isM256M32bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0xe6)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VCVTDQ2PD xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0xe6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTDQ2PD xmm, ymm{k}{z}
    if isEVEXXMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0xe6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTDQ2PD ymm, zmm{k}{z}
    if isEVEXYMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0xe6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTDQ2PD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[1]), v[0], 0)
            m.emit(0xe6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTDQ2PD m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[1]), addr(v[0]), 0)
            m.emit(0xe6)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VCVTDQ2PD xmm, ymm
    if isXMM(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(6, hcode(v[1]), v[0], 0)
            m.emit(0xe6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTDQ2PD m128, ymm
    if isM128(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(6, hcode(v[1]), addr(v[0]), 0)
            m.emit(0xe6)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTDQ2PD")
    }
    return p
}

// VCVTDQ2PS performs "Convert Packed Dword Integers to Packed Single-Precision FP Values".
//
// Mnemonic        : VCVTDQ2PS
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (11 forms)
//
//    * VCVTDQ2PS m128/m32bcst, xmm{k}{z}
//    * VCVTDQ2PS m256/m32bcst, ymm{k}{z}
//    * VCVTDQ2PS m512/m32bcst, zmm{k}{z}
//    * VCVTDQ2PS xmm, xmm{k}{z}
//    * VCVTDQ2PS ymm, ymm{k}{z}
//    * VCVTDQ2PS xmm, xmm
//    * VCVTDQ2PS m128, xmm
//    * VCVTDQ2PS ymm, ymm
//    * VCVTDQ2PS m256, ymm
//    * VCVTDQ2PS {er}, zmm, zmm{k}{z}
//    * VCVTDQ2PS zmm, zmm{k}{z}
//
func (self *Program) VCVTDQ2PS(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCVTDQ2PS takes 2 or 3 operands")
    }
    // VCVTDQ2PS m128/m32bcst, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x5b)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VCVTDQ2PS m256/m32bcst, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x5b)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VCVTDQ2PS m512/m32bcst, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x5b)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VCVTDQ2PS xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7c)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x5b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTDQ2PS ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7c)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x5b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTDQ2PS xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[1]), v[0], 0)
            m.emit(0x5b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTDQ2PS m128, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x5b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VCVTDQ2PS ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[1]), v[0], 0)
            m.emit(0x5b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTDQ2PS m256, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x5b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VCVTDQ2PS {er}, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c)
            m.emit((zcode(v[2]) << 7) | (vcode(v[0]) << 5) | kcode(v[2]) | 0x18)
            m.emit(0x5b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTDQ2PS zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7c)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x5b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTDQ2PS")
    }
    return p
}

// VCVTPD2DQ performs "Convert Packed Double-Precision FP Values to Packed Dword Integers".
//
// Mnemonic        : VCVTPD2DQ
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (11 forms)
//
//    * VCVTPD2DQ m128/m64bcst, xmm{k}{z}
//    * VCVTPD2DQ m256/m64bcst, xmm{k}{z}
//    * VCVTPD2DQ m512/m64bcst, ymm{k}{z}
//    * VCVTPD2DQ xmm, xmm{k}{z}
//    * VCVTPD2DQ ymm, xmm{k}{z}
//    * VCVTPD2DQ xmm, xmm
//    * VCVTPD2DQ ymm, xmm
//    * VCVTPD2DQ m128, xmm
//    * VCVTPD2DQ m256, xmm
//    * VCVTPD2DQ {er}, zmm, ymm{k}{z}
//    * VCVTPD2DQ zmm, ymm{k}{z}
//
func (self *Program) VCVTPD2DQ(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCVTPD2DQ takes 2 or 3 operands")
    }
    // VCVTPD2DQ m128/m64bcst, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x87, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0xe6)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VCVTPD2DQ m256/m64bcst, xmm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x87, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0xe6)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VCVTPD2DQ m512/m64bcst, ymm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x87, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0xe6)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VCVTPD2DQ xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xff)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0xe6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTPD2DQ ymm, xmm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xff)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0xe6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTPD2DQ xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[1]), v[0], 0)
            m.emit(0xe6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTPD2DQ ymm, xmm
    if len(vv) == 0 && isYMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(7, hcode(v[1]), v[0], 0)
            m.emit(0xe6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTPD2DQ m128, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[1]), addr(v[0]), 0)
            m.emit(0xe6)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VCVTPD2DQ m256, xmm
    if len(vv) == 0 && isM256(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(7, hcode(v[1]), addr(v[0]), 0)
            m.emit(0xe6)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VCVTPD2DQ {er}, zmm, ymm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isYMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xff)
            m.emit((zcode(v[2]) << 7) | (vcode(v[0]) << 5) | kcode(v[2]) | 0x18)
            m.emit(0xe6)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTPD2DQ zmm, ymm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xff)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0xe6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTPD2DQ")
    }
    return p
}

// VCVTPD2PS performs "Convert Packed Double-Precision FP Values to Packed Single-Precision FP Values".
//
// Mnemonic        : VCVTPD2PS
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (11 forms)
//
//    * VCVTPD2PS m128/m64bcst, xmm{k}{z}
//    * VCVTPD2PS m256/m64bcst, xmm{k}{z}
//    * VCVTPD2PS m512/m64bcst, ymm{k}{z}
//    * VCVTPD2PS xmm, xmm{k}{z}
//    * VCVTPD2PS ymm, xmm{k}{z}
//    * VCVTPD2PS xmm, xmm
//    * VCVTPD2PS ymm, xmm
//    * VCVTPD2PS m128, xmm
//    * VCVTPD2PS m256, xmm
//    * VCVTPD2PS {er}, zmm, ymm{k}{z}
//    * VCVTPD2PS zmm, ymm{k}{z}
//
func (self *Program) VCVTPD2PS(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCVTPD2PS takes 2 or 3 operands")
    }
    // VCVTPD2PS m128/m64bcst, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x5a)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VCVTPD2PS m256/m64bcst, xmm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x5a)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VCVTPD2PS m512/m64bcst, ymm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x5a)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VCVTPD2PS xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x5a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTPD2PS ymm, xmm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x5a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTPD2PS xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[1]), v[0], 0)
            m.emit(0x5a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTPD2PS ymm, xmm
    if len(vv) == 0 && isYMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[1]), v[0], 0)
            m.emit(0x5a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTPD2PS m128, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x5a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VCVTPD2PS m256, xmm
    if len(vv) == 0 && isM256(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x5a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VCVTPD2PS {er}, zmm, ymm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isYMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[2]) << 7) | (vcode(v[0]) << 5) | kcode(v[2]) | 0x18)
            m.emit(0x5a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTPD2PS zmm, ymm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x5a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTPD2PS")
    }
    return p
}

// VCVTPD2QQ performs "Convert Packed Double-Precision Floating-Point Values to Packed Quadword Integers".
//
// Mnemonic        : VCVTPD2QQ
// ISA extensions  : AVX512DQ, AVX512VL
// Supported forms : (7 forms)
//
//    * VCVTPD2QQ m128/m64bcst, xmm{k}{z}
//    * VCVTPD2QQ m256/m64bcst, ymm{k}{z}
//    * VCVTPD2QQ m512/m64bcst, zmm{k}{z}
//    * VCVTPD2QQ xmm, xmm{k}{z}
//    * VCVTPD2QQ ymm, ymm{k}{z}
//    * VCVTPD2QQ {er}, zmm, zmm{k}{z}
//    * VCVTPD2QQ zmm, zmm{k}{z}
//
func (self *Program) VCVTPD2QQ(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCVTPD2QQ takes 2 or 3 operands")
    }
    // VCVTPD2QQ m128/m64bcst, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x7b)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VCVTPD2QQ m256/m64bcst, ymm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x7b)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VCVTPD2QQ m512/m64bcst, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x7b)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VCVTPD2QQ xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x7b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTPD2QQ ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x7b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTPD2QQ {er}, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[2]) << 7) | (vcode(v[0]) << 5) | kcode(v[2]) | 0x18)
            m.emit(0x7b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTPD2QQ zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x7b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTPD2QQ")
    }
    return p
}

// VCVTPD2UDQ performs "Convert Packed Double-Precision Floating-Point Values to Packed Unsigned Doubleword Integers".
//
// Mnemonic        : VCVTPD2UDQ
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (7 forms)
//
//    * VCVTPD2UDQ m128/m64bcst, xmm{k}{z}
//    * VCVTPD2UDQ m256/m64bcst, xmm{k}{z}
//    * VCVTPD2UDQ m512/m64bcst, ymm{k}{z}
//    * VCVTPD2UDQ xmm, xmm{k}{z}
//    * VCVTPD2UDQ ymm, xmm{k}{z}
//    * VCVTPD2UDQ {er}, zmm, ymm{k}{z}
//    * VCVTPD2UDQ zmm, ymm{k}{z}
//
func (self *Program) VCVTPD2UDQ(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCVTPD2UDQ takes 2 or 3 operands")
    }
    // VCVTPD2UDQ m128/m64bcst, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x84, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x79)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VCVTPD2UDQ m256/m64bcst, xmm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x84, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x79)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VCVTPD2UDQ m512/m64bcst, ymm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x84, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x79)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VCVTPD2UDQ xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfc)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTPD2UDQ ymm, xmm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfc)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTPD2UDQ {er}, zmm, ymm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isYMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfc)
            m.emit((zcode(v[2]) << 7) | (vcode(v[0]) << 5) | kcode(v[2]) | 0x18)
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTPD2UDQ zmm, ymm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfc)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTPD2UDQ")
    }
    return p
}

// VCVTPD2UQQ performs "Convert Packed Double-Precision Floating-Point Values to Packed Unsigned Quadword Integers".
//
// Mnemonic        : VCVTPD2UQQ
// ISA extensions  : AVX512DQ, AVX512VL
// Supported forms : (7 forms)
//
//    * VCVTPD2UQQ m128/m64bcst, xmm{k}{z}
//    * VCVTPD2UQQ m256/m64bcst, ymm{k}{z}
//    * VCVTPD2UQQ m512/m64bcst, zmm{k}{z}
//    * VCVTPD2UQQ xmm, xmm{k}{z}
//    * VCVTPD2UQQ ymm, ymm{k}{z}
//    * VCVTPD2UQQ {er}, zmm, zmm{k}{z}
//    * VCVTPD2UQQ zmm, zmm{k}{z}
//
func (self *Program) VCVTPD2UQQ(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCVTPD2UQQ takes 2 or 3 operands")
    }
    // VCVTPD2UQQ m128/m64bcst, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x79)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VCVTPD2UQQ m256/m64bcst, ymm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x79)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VCVTPD2UQQ m512/m64bcst, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x79)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VCVTPD2UQQ xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTPD2UQQ ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTPD2UQQ {er}, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[2]) << 7) | (vcode(v[0]) << 5) | kcode(v[2]) | 0x18)
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTPD2UQQ zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTPD2UQQ")
    }
    return p
}

// VCVTPH2PS performs "Convert Half-Precision FP Values to Single-Precision FP Values".
//
// Mnemonic        : VCVTPH2PS
// ISA extensions  : AVX512F, AVX512VL, F16C
// Supported forms : (11 forms)
//
//    * VCVTPH2PS xmm, xmm{k}{z}
//    * VCVTPH2PS xmm, ymm{k}{z}
//    * VCVTPH2PS m64, xmm{k}{z}
//    * VCVTPH2PS m128, ymm{k}{z}
//    * VCVTPH2PS m256, zmm{k}{z}
//    * VCVTPH2PS xmm, xmm
//    * VCVTPH2PS m64, xmm
//    * VCVTPH2PS xmm, ymm
//    * VCVTPH2PS m128, ymm
//    * VCVTPH2PS {sae}, ymm, zmm{k}{z}
//    * VCVTPH2PS ymm, zmm{k}{z}
//
func (self *Program) VCVTPH2PS(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCVTPH2PS takes 2 or 3 operands")
    }
    // VCVTPH2PS xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x13)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTPH2PS xmm, ymm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x13)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTPH2PS m64, xmm{k}{z}
    if len(vv) == 0 && isM64(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x13)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VCVTPH2PS m128, ymm{k}{z}
    if len(vv) == 0 && isM128(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x13)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VCVTPH2PS m256, zmm{k}{z}
    if len(vv) == 0 && isM256(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x13)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VCVTPH2PS xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) {
        self.require(ISA_F16C)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79)
            m.emit(0x13)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTPH2PS m64, xmm
    if len(vv) == 0 && isM64(v0) && isXMM(v1) {
        self.require(ISA_F16C)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x13)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VCVTPH2PS xmm, ymm
    if len(vv) == 0 && isXMM(v0) && isYMM(v1) {
        self.require(ISA_F16C)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d)
            m.emit(0x13)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTPH2PS m128, ymm
    if len(vv) == 0 && isM128(v0) && isYMM(v1) {
        self.require(ISA_F16C)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x13)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VCVTPH2PS {sae}, ymm, zmm{k}{z}
    if len(vv) == 1 && isSAE(v0) && isEVEXYMM(v1) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x18)
            m.emit(0x13)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTPH2PS ymm, zmm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x13)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTPH2PS")
    }
    return p
}

// VCVTPS2DQ performs "Convert Packed Single-Precision FP Values to Packed Dword Integers".
//
// Mnemonic        : VCVTPS2DQ
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (11 forms)
//
//    * VCVTPS2DQ m128/m32bcst, xmm{k}{z}
//    * VCVTPS2DQ m256/m32bcst, ymm{k}{z}
//    * VCVTPS2DQ m512/m32bcst, zmm{k}{z}
//    * VCVTPS2DQ xmm, xmm{k}{z}
//    * VCVTPS2DQ ymm, ymm{k}{z}
//    * VCVTPS2DQ xmm, xmm
//    * VCVTPS2DQ m128, xmm
//    * VCVTPS2DQ ymm, ymm
//    * VCVTPS2DQ m256, ymm
//    * VCVTPS2DQ {er}, zmm, zmm{k}{z}
//    * VCVTPS2DQ zmm, zmm{k}{z}
//
func (self *Program) VCVTPS2DQ(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCVTPS2DQ takes 2 or 3 operands")
    }
    // VCVTPS2DQ m128/m32bcst, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x5b)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VCVTPS2DQ m256/m32bcst, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x5b)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VCVTPS2DQ m512/m32bcst, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x5b)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VCVTPS2DQ xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x5b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTPS2DQ ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x5b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTPS2DQ xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[1]), v[0], 0)
            m.emit(0x5b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTPS2DQ m128, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x5b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VCVTPS2DQ ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[1]), v[0], 0)
            m.emit(0x5b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTPS2DQ m256, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x5b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VCVTPS2DQ {er}, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | (vcode(v[0]) << 5) | kcode(v[2]) | 0x18)
            m.emit(0x5b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTPS2DQ zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x5b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTPS2DQ")
    }
    return p
}

// VCVTPS2PD performs "Convert Packed Single-Precision FP Values to Packed Double-Precision FP Values".
//
// Mnemonic        : VCVTPS2PD
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (11 forms)
//
//    * VCVTPS2PD m64/m32bcst, xmm{k}{z}
//    * VCVTPS2PD m128/m32bcst, ymm{k}{z}
//    * VCVTPS2PD m256/m32bcst, zmm{k}{z}
//    * VCVTPS2PD xmm, xmm{k}{z}
//    * VCVTPS2PD xmm, ymm{k}{z}
//    * VCVTPS2PD xmm, xmm
//    * VCVTPS2PD m64, xmm
//    * VCVTPS2PD xmm, ymm
//    * VCVTPS2PD m128, ymm
//    * VCVTPS2PD {sae}, ymm, zmm{k}{z}
//    * VCVTPS2PD ymm, zmm{k}{z}
//
func (self *Program) VCVTPS2PD(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCVTPS2PD takes 2 or 3 operands")
    }
    // VCVTPS2PD m64/m32bcst, xmm{k}{z}
    if len(vv) == 0 && isM64M32bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x5a)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VCVTPS2PD m128/m32bcst, ymm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x5a)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VCVTPS2PD m256/m32bcst, zmm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x5a)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VCVTPS2PD xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7c)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x5a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTPS2PD xmm, ymm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7c)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x5a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTPS2PD xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[1]), v[0], 0)
            m.emit(0x5a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTPS2PD m64, xmm
    if len(vv) == 0 && isM64(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x5a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VCVTPS2PD xmm, ymm
    if len(vv) == 0 && isXMM(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[1]), v[0], 0)
            m.emit(0x5a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTPS2PD m128, ymm
    if len(vv) == 0 && isM128(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x5a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VCVTPS2PD {sae}, ymm, zmm{k}{z}
    if len(vv) == 1 && isSAE(v0) && isEVEXYMM(v1) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x18)
            m.emit(0x5a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTPS2PD ymm, zmm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7c)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x5a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTPS2PD")
    }
    return p
}

// VCVTPS2PH performs "Convert Single-Precision FP value to Half-Precision FP value".
//
// Mnemonic        : VCVTPS2PH
// ISA extensions  : AVX512F, AVX512VL, F16C
// Supported forms : (11 forms)
//
//    * VCVTPS2PH imm8, xmm, xmm{k}{z}
//    * VCVTPS2PH imm8, xmm, m64{k}{z}
//    * VCVTPS2PH imm8, ymm, xmm{k}{z}
//    * VCVTPS2PH imm8, ymm, m128{k}{z}
//    * VCVTPS2PH imm8, zmm, m256{k}{z}
//    * VCVTPS2PH imm8, xmm, xmm
//    * VCVTPS2PH imm8, ymm, xmm
//    * VCVTPS2PH imm8, xmm, m64
//    * VCVTPS2PH imm8, ymm, m128
//    * VCVTPS2PH imm8, {sae}, zmm, ymm{k}{z}
//    * VCVTPS2PH imm8, zmm, ymm{k}{z}
//
func (self *Program) VCVTPS2PH(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VCVTPS2PH takes 3 or 4 operands")
    }
    // VCVTPS2PH imm8, xmm, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[1]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x08)
            m.emit(0x1d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCVTPS2PH imm8, xmm, m64{k}{z}
    if len(vv) == 0 && isImm8(v0) && isEVEXXMM(v1) && isM64kz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b00, ehcode(v[1]), addr(v[2]), 0, kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x1d)
            m.mrsd(lcode(v[1]), addr(v[2]), 8)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCVTPS2PH imm8, ymm, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isEVEXYMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[1]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x28)
            m.emit(0x1d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCVTPS2PH imm8, ymm, m128{k}{z}
    if len(vv) == 0 && isImm8(v0) && isEVEXYMM(v1) && isM128kz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b01, ehcode(v[1]), addr(v[2]), 0, kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x1d)
            m.mrsd(lcode(v[1]), addr(v[2]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCVTPS2PH imm8, zmm, m256{k}{z}
    if len(vv) == 0 && isImm8(v0) && isZMM(v1) && isM256kz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b10, ehcode(v[1]), addr(v[2]), 0, kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x1d)
            m.mrsd(lcode(v[1]), addr(v[2]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCVTPS2PH imm8, xmm, xmm
    if len(vv) == 0 && isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_F16C)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[1]) << 7) ^ (hcode(v[2]) << 5))
            m.emit(0x79)
            m.emit(0x1d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCVTPS2PH imm8, ymm, xmm
    if len(vv) == 0 && isImm8(v0) && isYMM(v1) && isXMM(v2) {
        self.require(ISA_F16C)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[1]) << 7) ^ (hcode(v[2]) << 5))
            m.emit(0x7d)
            m.emit(0x1d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCVTPS2PH imm8, xmm, m64
    if len(vv) == 0 && isImm8(v0) && isXMM(v1) && isM64(v2) {
        self.require(ISA_F16C)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[1]), addr(v[2]), 0)
            m.emit(0x1d)
            m.mrsd(lcode(v[1]), addr(v[2]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCVTPS2PH imm8, ymm, m128
    if len(vv) == 0 && isImm8(v0) && isYMM(v1) && isM128(v2) {
        self.require(ISA_F16C)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[1]), addr(v[2]), 0)
            m.emit(0x1d)
            m.mrsd(lcode(v[1]), addr(v[2]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCVTPS2PH imm8, {sae}, zmm, ymm{k}{z}
    if len(vv) == 1 && isImm8(v0) && isSAE(v1) && isZMM(v2) && isYMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[3]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[3]) << 7) | kcode(v[3]) | 0x18)
            m.emit(0x1d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[3]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VCVTPS2PH imm8, zmm, ymm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isZMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[1]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x48)
            m.emit(0x1d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTPS2PH")
    }
    return p
}

// VCVTPS2QQ performs "Convert Packed Single Precision Floating-Point Values to Packed Singed Quadword Integer Values".
//
// Mnemonic        : VCVTPS2QQ
// ISA extensions  : AVX512DQ, AVX512VL
// Supported forms : (7 forms)
//
//    * VCVTPS2QQ m64/m32bcst, xmm{k}{z}
//    * VCVTPS2QQ m128/m32bcst, ymm{k}{z}
//    * VCVTPS2QQ m256/m32bcst, zmm{k}{z}
//    * VCVTPS2QQ xmm, xmm{k}{z}
//    * VCVTPS2QQ xmm, ymm{k}{z}
//    * VCVTPS2QQ {er}, ymm, zmm{k}{z}
//    * VCVTPS2QQ ymm, zmm{k}{z}
//
func (self *Program) VCVTPS2QQ(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCVTPS2QQ takes 2 or 3 operands")
    }
    // VCVTPS2QQ m64/m32bcst, xmm{k}{z}
    if len(vv) == 0 && isM64M32bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x7b)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VCVTPS2QQ m128/m32bcst, ymm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x7b)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VCVTPS2QQ m256/m32bcst, zmm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x7b)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VCVTPS2QQ xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x7b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTPS2QQ xmm, ymm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x7b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTPS2QQ {er}, ymm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXYMM(v1) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | (vcode(v[0]) << 5) | kcode(v[2]) | 0x18)
            m.emit(0x7b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTPS2QQ ymm, zmm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x7b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTPS2QQ")
    }
    return p
}

// VCVTPS2UDQ performs "Convert Packed Single-Precision Floating-Point Values to Packed Unsigned Doubleword Integer Values".
//
// Mnemonic        : VCVTPS2UDQ
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (7 forms)
//
//    * VCVTPS2UDQ m128/m32bcst, xmm{k}{z}
//    * VCVTPS2UDQ m256/m32bcst, ymm{k}{z}
//    * VCVTPS2UDQ m512/m32bcst, zmm{k}{z}
//    * VCVTPS2UDQ xmm, xmm{k}{z}
//    * VCVTPS2UDQ ymm, ymm{k}{z}
//    * VCVTPS2UDQ {er}, zmm, zmm{k}{z}
//    * VCVTPS2UDQ zmm, zmm{k}{z}
//
func (self *Program) VCVTPS2UDQ(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCVTPS2UDQ takes 2 or 3 operands")
    }
    // VCVTPS2UDQ m128/m32bcst, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x79)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VCVTPS2UDQ m256/m32bcst, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x79)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VCVTPS2UDQ m512/m32bcst, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x79)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VCVTPS2UDQ xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7c)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTPS2UDQ ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7c)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTPS2UDQ {er}, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c)
            m.emit((zcode(v[2]) << 7) | (vcode(v[0]) << 5) | kcode(v[2]) | 0x18)
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTPS2UDQ zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7c)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTPS2UDQ")
    }
    return p
}

// VCVTPS2UQQ performs "Convert Packed Single Precision Floating-Point Values to Packed Unsigned Quadword Integer Values".
//
// Mnemonic        : VCVTPS2UQQ
// ISA extensions  : AVX512DQ, AVX512VL
// Supported forms : (7 forms)
//
//    * VCVTPS2UQQ m64/m32bcst, xmm{k}{z}
//    * VCVTPS2UQQ m128/m32bcst, ymm{k}{z}
//    * VCVTPS2UQQ m256/m32bcst, zmm{k}{z}
//    * VCVTPS2UQQ xmm, xmm{k}{z}
//    * VCVTPS2UQQ xmm, ymm{k}{z}
//    * VCVTPS2UQQ {er}, ymm, zmm{k}{z}
//    * VCVTPS2UQQ ymm, zmm{k}{z}
//
func (self *Program) VCVTPS2UQQ(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCVTPS2UQQ takes 2 or 3 operands")
    }
    // VCVTPS2UQQ m64/m32bcst, xmm{k}{z}
    if len(vv) == 0 && isM64M32bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x79)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VCVTPS2UQQ m128/m32bcst, ymm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x79)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VCVTPS2UQQ m256/m32bcst, zmm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x79)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VCVTPS2UQQ xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTPS2UQQ xmm, ymm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTPS2UQQ {er}, ymm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXYMM(v1) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | (vcode(v[0]) << 5) | kcode(v[2]) | 0x18)
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTPS2UQQ ymm, zmm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTPS2UQQ")
    }
    return p
}

// VCVTQQ2PD performs "Convert Packed Quadword Integers to Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VCVTQQ2PD
// ISA extensions  : AVX512DQ, AVX512VL
// Supported forms : (7 forms)
//
//    * VCVTQQ2PD m128/m64bcst, xmm{k}{z}
//    * VCVTQQ2PD m256/m64bcst, ymm{k}{z}
//    * VCVTQQ2PD m512/m64bcst, zmm{k}{z}
//    * VCVTQQ2PD xmm, xmm{k}{z}
//    * VCVTQQ2PD ymm, ymm{k}{z}
//    * VCVTQQ2PD {er}, zmm, zmm{k}{z}
//    * VCVTQQ2PD zmm, zmm{k}{z}
//
func (self *Program) VCVTQQ2PD(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCVTQQ2PD takes 2 or 3 operands")
    }
    // VCVTQQ2PD m128/m64bcst, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x86, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0xe6)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VCVTQQ2PD m256/m64bcst, ymm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x86, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0xe6)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VCVTQQ2PD m512/m64bcst, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x86, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0xe6)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VCVTQQ2PD xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfe)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0xe6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTQQ2PD ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfe)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0xe6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTQQ2PD {er}, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfe)
            m.emit((zcode(v[2]) << 7) | (vcode(v[0]) << 5) | kcode(v[2]) | 0x18)
            m.emit(0xe6)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTQQ2PD zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfe)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0xe6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTQQ2PD")
    }
    return p
}

// VCVTQQ2PS performs "Convert Packed Quadword Integers to Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VCVTQQ2PS
// ISA extensions  : AVX512DQ, AVX512VL
// Supported forms : (7 forms)
//
//    * VCVTQQ2PS m128/m64bcst, xmm{k}{z}
//    * VCVTQQ2PS m256/m64bcst, xmm{k}{z}
//    * VCVTQQ2PS m512/m64bcst, ymm{k}{z}
//    * VCVTQQ2PS xmm, xmm{k}{z}
//    * VCVTQQ2PS ymm, xmm{k}{z}
//    * VCVTQQ2PS {er}, zmm, ymm{k}{z}
//    * VCVTQQ2PS zmm, ymm{k}{z}
//
func (self *Program) VCVTQQ2PS(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCVTQQ2PS takes 2 or 3 operands")
    }
    // VCVTQQ2PS m128/m64bcst, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x84, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x5b)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VCVTQQ2PS m256/m64bcst, xmm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x84, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x5b)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VCVTQQ2PS m512/m64bcst, ymm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x84, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x5b)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VCVTQQ2PS xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfc)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x5b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTQQ2PS ymm, xmm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfc)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x5b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTQQ2PS {er}, zmm, ymm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isYMMkz(vv[0]) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfc)
            m.emit((zcode(v[2]) << 7) | (vcode(v[0]) << 5) | kcode(v[2]) | 0x18)
            m.emit(0x5b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTQQ2PS zmm, ymm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfc)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x5b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTQQ2PS")
    }
    return p
}

// VCVTSD2SI performs "Convert Scalar Double-Precision FP Value to Integer".
//
// Mnemonic        : VCVTSD2SI
// ISA extensions  : AVX, AVX512F
// Supported forms : (10 forms)
//
//    * VCVTSD2SI xmm, r32
//    * VCVTSD2SI m64, r32
//    * VCVTSD2SI m64, r32
//    * VCVTSD2SI xmm, r64
//    * VCVTSD2SI m64, r64
//    * VCVTSD2SI m64, r64
//    * VCVTSD2SI {er}, xmm, r32
//    * VCVTSD2SI {er}, xmm, r64
//    * VCVTSD2SI xmm, r32
//    * VCVTSD2SI xmm, r64
//
func (self *Program) VCVTSD2SI(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCVTSD2SI takes 2 or 3 operands")
    }
    // VCVTSD2SI xmm, r32
    if len(vv) == 0 && isXMM(v0) && isReg32(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[1]), v[0], 0)
            m.emit(0x2d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTSD2SI m64, r32
    if len(vv) == 0 && isM64(v0) && isReg32(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x2d)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VCVTSD2SI m64, r32
    if len(vv) == 0 && isM64(v0) && isReg32(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x07, 0b00, ehcode(v[1]), addr(v[0]), 0, 0, 0, 0)
            m.emit(0x2d)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VCVTSD2SI xmm, r64
    if len(vv) == 0 && isXMM(v0) && isReg64(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe1 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfb)
            m.emit(0x2d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTSD2SI m64, r64
    if len(vv) == 0 && isM64(v0) && isReg64(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b1, 0x83, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x2d)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VCVTSD2SI m64, r64
    if len(vv) == 0 && isM64(v0) && isReg64(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x87, 0b00, ehcode(v[1]), addr(v[0]), 0, 0, 0, 0)
            m.emit(0x2d)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VCVTSD2SI {er}, xmm, r32
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isReg32(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7f)
            m.emit((vcode(v[0]) << 5) | 0x18)
            m.emit(0x2d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTSD2SI {er}, xmm, r64
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isReg64(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xff)
            m.emit((vcode(v[0]) << 5) | 0x18)
            m.emit(0x2d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTSD2SI xmm, r32
    if len(vv) == 0 && isEVEXXMM(v0) && isReg32(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7f)
            m.emit(0x48)
            m.emit(0x2d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTSD2SI xmm, r64
    if len(vv) == 0 && isEVEXXMM(v0) && isReg64(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xff)
            m.emit(0x48)
            m.emit(0x2d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTSD2SI")
    }
    return p
}

// VCVTSD2SS performs "Convert Scalar Double-Precision FP Value to Scalar Single-Precision FP Value".
//
// Mnemonic        : VCVTSD2SS
// ISA extensions  : AVX, AVX512F
// Supported forms : (5 forms)
//
//    * VCVTSD2SS m64, xmm, xmm{k}{z}
//    * VCVTSD2SS xmm, xmm, xmm
//    * VCVTSD2SS m64, xmm, xmm
//    * VCVTSD2SS {er}, xmm, xmm, xmm{k}{z}
//    * VCVTSD2SS xmm, xmm, xmm{k}{z}
//
func (self *Program) VCVTSD2SS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VCVTSD2SS takes 3 or 4 operands")
    }
    // VCVTSD2SS m64, xmm, xmm{k}{z}
    if len(vv) == 0 && isM64(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x87, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x5a)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    // VCVTSD2SS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x5a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VCVTSD2SS m64, xmm, xmm
    if len(vv) == 0 && isM64(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x5a)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VCVTSD2SS {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xff ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x5a)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VCVTSD2SS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xff ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x5a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTSD2SS")
    }
    return p
}

// VCVTSD2USI performs "Convert Scalar Double-Precision Floating-Point Value to Unsigned Doubleword Integer".
//
// Mnemonic        : VCVTSD2USI
// ISA extensions  : AVX512F
// Supported forms : (6 forms)
//
//    * VCVTSD2USI m64, r32
//    * VCVTSD2USI m64, r64
//    * VCVTSD2USI {er}, xmm, r32
//    * VCVTSD2USI {er}, xmm, r64
//    * VCVTSD2USI xmm, r32
//    * VCVTSD2USI xmm, r64
//
func (self *Program) VCVTSD2USI(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCVTSD2USI takes 2 or 3 operands")
    }
    // VCVTSD2USI m64, r32
    if len(vv) == 0 && isM64(v0) && isReg32(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x07, 0b00, ehcode(v[1]), addr(v[0]), 0, 0, 0, 0)
            m.emit(0x79)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VCVTSD2USI m64, r64
    if len(vv) == 0 && isM64(v0) && isReg64(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x87, 0b00, ehcode(v[1]), addr(v[0]), 0, 0, 0, 0)
            m.emit(0x79)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VCVTSD2USI {er}, xmm, r32
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isReg32(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7f)
            m.emit((vcode(v[0]) << 5) | 0x18)
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTSD2USI {er}, xmm, r64
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isReg64(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xff)
            m.emit((vcode(v[0]) << 5) | 0x18)
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTSD2USI xmm, r32
    if len(vv) == 0 && isEVEXXMM(v0) && isReg32(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7f)
            m.emit(0x48)
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTSD2USI xmm, r64
    if len(vv) == 0 && isEVEXXMM(v0) && isReg64(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xff)
            m.emit(0x48)
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTSD2USI")
    }
    return p
}

// VCVTSI2SD performs "Convert Dword Integer to Scalar Double-Precision FP Value".
//
// Mnemonic        : VCVTSI2SD
// ISA extensions  : AVX, AVX512F
// Supported forms : (9 forms)
//
//    * VCVTSI2SD r32, xmm, xmm
//    * VCVTSI2SD r32, xmm, xmm
//    * VCVTSI2SD r64, xmm, xmm
//    * VCVTSI2SD m32, xmm, xmm
//    * VCVTSI2SD m32, xmm, xmm
//    * VCVTSI2SD m64, xmm, xmm
//    * VCVTSI2SD m64, xmm, xmm
//    * VCVTSI2SD {er}, r64, xmm, xmm
//    * VCVTSI2SD r64, xmm, xmm
//
func (self *Program) VCVTSI2SD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VCVTSI2SD takes 3 or 4 operands")
    }
    // VCVTSI2SD r32, xmm, xmm
    if len(vv) == 0 && isReg32(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x2a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VCVTSI2SD r32, xmm, xmm
    if len(vv) == 0 && isReg32(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7f ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | 0x00)
            m.emit(0x2a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VCVTSI2SD r64, xmm, xmm
    if len(vv) == 0 && isReg64(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe1 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfb ^ (hlcode(v[1]) << 3))
            m.emit(0x2a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VCVTSI2SD m32, xmm, xmm
    if len(vv) == 0 && isM32(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x2a)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VCVTSI2SD m32, xmm, xmm
    if len(vv) == 0 && isM32(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x07, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), 0, 0, 0)
            m.emit(0x2a)
            m.mrsd(lcode(v[2]), addr(v[0]), 4)
        })
    }
    // VCVTSI2SD m64, xmm, xmm
    if len(vv) == 0 && isM64(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b1, 0x83, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x2a)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VCVTSI2SD m64, xmm, xmm
    if len(vv) == 0 && isM64(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x87, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), 0, 0, 0)
            m.emit(0x2a)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    // VCVTSI2SD {er}, r64, xmm, xmm
    if len(vv) == 1 && isER(v0) && isReg64(v1) && isEVEXXMM(v2) && isEVEXXMM(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xff ^ (hlcode(v[2]) << 3))
            m.emit((vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | 0x10)
            m.emit(0x2a)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VCVTSI2SD r64, xmm, xmm
    if len(vv) == 0 && isReg64(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xff ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | 0x40)
            m.emit(0x2a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTSI2SD")
    }
    return p
}

// VCVTSI2SS performs "Convert Dword Integer to Scalar Single-Precision FP Value".
//
// Mnemonic        : VCVTSI2SS
// ISA extensions  : AVX, AVX512F
// Supported forms : (10 forms)
//
//    * VCVTSI2SS r32, xmm, xmm
//    * VCVTSI2SS r64, xmm, xmm
//    * VCVTSI2SS m32, xmm, xmm
//    * VCVTSI2SS m32, xmm, xmm
//    * VCVTSI2SS m64, xmm, xmm
//    * VCVTSI2SS m64, xmm, xmm
//    * VCVTSI2SS {er}, r32, xmm, xmm
//    * VCVTSI2SS {er}, r64, xmm, xmm
//    * VCVTSI2SS r32, xmm, xmm
//    * VCVTSI2SS r64, xmm, xmm
//
func (self *Program) VCVTSI2SS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VCVTSI2SS takes 3 or 4 operands")
    }
    // VCVTSI2SS r32, xmm, xmm
    if len(vv) == 0 && isReg32(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x2a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VCVTSI2SS r64, xmm, xmm
    if len(vv) == 0 && isReg64(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe1 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfa ^ (hlcode(v[1]) << 3))
            m.emit(0x2a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VCVTSI2SS m32, xmm, xmm
    if len(vv) == 0 && isM32(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x2a)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VCVTSI2SS m32, xmm, xmm
    if len(vv) == 0 && isM32(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), 0, 0, 0)
            m.emit(0x2a)
            m.mrsd(lcode(v[2]), addr(v[0]), 4)
        })
    }
    // VCVTSI2SS m64, xmm, xmm
    if len(vv) == 0 && isM64(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b1, 0x82, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x2a)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VCVTSI2SS m64, xmm, xmm
    if len(vv) == 0 && isM64(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x86, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), 0, 0, 0)
            m.emit(0x2a)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    // VCVTSI2SS {er}, r32, xmm, xmm
    if len(vv) == 1 && isER(v0) && isReg32(v1) && isEVEXXMM(v2) && isEVEXXMM(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7e ^ (hlcode(v[2]) << 3))
            m.emit((vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | 0x10)
            m.emit(0x2a)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VCVTSI2SS {er}, r64, xmm, xmm
    if len(vv) == 1 && isER(v0) && isReg64(v1) && isEVEXXMM(v2) && isEVEXXMM(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfe ^ (hlcode(v[2]) << 3))
            m.emit((vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | 0x10)
            m.emit(0x2a)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VCVTSI2SS r32, xmm, xmm
    if len(vv) == 0 && isReg32(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7e ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | 0x40)
            m.emit(0x2a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VCVTSI2SS r64, xmm, xmm
    if len(vv) == 0 && isReg64(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfe ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | 0x40)
            m.emit(0x2a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTSI2SS")
    }
    return p
}

// VCVTSS2SD performs "Convert Scalar Single-Precision FP Value to Scalar Double-Precision FP Value".
//
// Mnemonic        : VCVTSS2SD
// ISA extensions  : AVX, AVX512F
// Supported forms : (5 forms)
//
//    * VCVTSS2SD m32, xmm, xmm{k}{z}
//    * VCVTSS2SD xmm, xmm, xmm
//    * VCVTSS2SD m32, xmm, xmm
//    * VCVTSS2SD {sae}, xmm, xmm, xmm{k}{z}
//    * VCVTSS2SD xmm, xmm, xmm{k}{z}
//
func (self *Program) VCVTSS2SD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VCVTSS2SD takes 3 or 4 operands")
    }
    // VCVTSS2SD m32, xmm, xmm{k}{z}
    if len(vv) == 0 && isM32(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x5a)
            m.mrsd(lcode(v[2]), addr(v[0]), 4)
        })
    }
    // VCVTSS2SD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x5a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VCVTSS2SD m32, xmm, xmm
    if len(vv) == 0 && isM32(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x5a)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VCVTSS2SD {sae}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isSAE(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7e ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x5a)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VCVTSS2SD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7e ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x5a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTSS2SD")
    }
    return p
}

// VCVTSS2SI performs "Convert Scalar Single-Precision FP Value to Dword Integer".
//
// Mnemonic        : VCVTSS2SI
// ISA extensions  : AVX, AVX512F
// Supported forms : (10 forms)
//
//    * VCVTSS2SI xmm, r32
//    * VCVTSS2SI m32, r32
//    * VCVTSS2SI m32, r32
//    * VCVTSS2SI xmm, r64
//    * VCVTSS2SI m32, r64
//    * VCVTSS2SI m32, r64
//    * VCVTSS2SI {er}, xmm, r32
//    * VCVTSS2SI {er}, xmm, r64
//    * VCVTSS2SI xmm, r32
//    * VCVTSS2SI xmm, r64
//
func (self *Program) VCVTSS2SI(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCVTSS2SI takes 2 or 3 operands")
    }
    // VCVTSS2SI xmm, r32
    if len(vv) == 0 && isXMM(v0) && isReg32(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[1]), v[0], 0)
            m.emit(0x2d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTSS2SI m32, r32
    if len(vv) == 0 && isM32(v0) && isReg32(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x2d)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VCVTSS2SI m32, r32
    if len(vv) == 0 && isM32(v0) && isReg32(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b00, ehcode(v[1]), addr(v[0]), 0, 0, 0, 0)
            m.emit(0x2d)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VCVTSS2SI xmm, r64
    if len(vv) == 0 && isXMM(v0) && isReg64(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe1 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfa)
            m.emit(0x2d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTSS2SI m32, r64
    if len(vv) == 0 && isM32(v0) && isReg64(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b1, 0x82, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x2d)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VCVTSS2SI m32, r64
    if len(vv) == 0 && isM32(v0) && isReg64(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x86, 0b00, ehcode(v[1]), addr(v[0]), 0, 0, 0, 0)
            m.emit(0x2d)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VCVTSS2SI {er}, xmm, r32
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isReg32(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7e)
            m.emit((vcode(v[0]) << 5) | 0x18)
            m.emit(0x2d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTSS2SI {er}, xmm, r64
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isReg64(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfe)
            m.emit((vcode(v[0]) << 5) | 0x18)
            m.emit(0x2d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTSS2SI xmm, r32
    if len(vv) == 0 && isEVEXXMM(v0) && isReg32(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit(0x48)
            m.emit(0x2d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTSS2SI xmm, r64
    if len(vv) == 0 && isEVEXXMM(v0) && isReg64(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfe)
            m.emit(0x48)
            m.emit(0x2d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTSS2SI")
    }
    return p
}

// VCVTSS2USI performs "Convert Scalar Single-Precision Floating-Point Value to Unsigned Doubleword Integer".
//
// Mnemonic        : VCVTSS2USI
// ISA extensions  : AVX512F
// Supported forms : (6 forms)
//
//    * VCVTSS2USI m32, r32
//    * VCVTSS2USI m32, r64
//    * VCVTSS2USI {er}, xmm, r32
//    * VCVTSS2USI {er}, xmm, r64
//    * VCVTSS2USI xmm, r32
//    * VCVTSS2USI xmm, r64
//
func (self *Program) VCVTSS2USI(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCVTSS2USI takes 2 or 3 operands")
    }
    // VCVTSS2USI m32, r32
    if len(vv) == 0 && isM32(v0) && isReg32(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b00, ehcode(v[1]), addr(v[0]), 0, 0, 0, 0)
            m.emit(0x79)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VCVTSS2USI m32, r64
    if len(vv) == 0 && isM32(v0) && isReg64(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x86, 0b00, ehcode(v[1]), addr(v[0]), 0, 0, 0, 0)
            m.emit(0x79)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VCVTSS2USI {er}, xmm, r32
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isReg32(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7e)
            m.emit((vcode(v[0]) << 5) | 0x18)
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTSS2USI {er}, xmm, r64
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isReg64(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfe)
            m.emit((vcode(v[0]) << 5) | 0x18)
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTSS2USI xmm, r32
    if len(vv) == 0 && isEVEXXMM(v0) && isReg32(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit(0x48)
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTSS2USI xmm, r64
    if len(vv) == 0 && isEVEXXMM(v0) && isReg64(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfe)
            m.emit(0x48)
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTSS2USI")
    }
    return p
}

// VCVTTPD2DQ performs "Convert with Truncation Packed Double-Precision FP Values to Packed Dword Integers".
//
// Mnemonic        : VCVTTPD2DQ
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (11 forms)
//
//    * VCVTTPD2DQ m128/m64bcst, xmm{k}{z}
//    * VCVTTPD2DQ m256/m64bcst, xmm{k}{z}
//    * VCVTTPD2DQ m512/m64bcst, ymm{k}{z}
//    * VCVTTPD2DQ xmm, xmm{k}{z}
//    * VCVTTPD2DQ ymm, xmm{k}{z}
//    * VCVTTPD2DQ xmm, xmm
//    * VCVTTPD2DQ ymm, xmm
//    * VCVTTPD2DQ m128, xmm
//    * VCVTTPD2DQ m256, xmm
//    * VCVTTPD2DQ {sae}, zmm, ymm{k}{z}
//    * VCVTTPD2DQ zmm, ymm{k}{z}
//
func (self *Program) VCVTTPD2DQ(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCVTTPD2DQ takes 2 or 3 operands")
    }
    // VCVTTPD2DQ m128/m64bcst, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0xe6)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VCVTTPD2DQ m256/m64bcst, xmm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0xe6)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VCVTTPD2DQ m512/m64bcst, ymm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0xe6)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VCVTTPD2DQ xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0xe6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTTPD2DQ ymm, xmm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0xe6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTTPD2DQ xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[1]), v[0], 0)
            m.emit(0xe6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTTPD2DQ ymm, xmm
    if len(vv) == 0 && isYMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[1]), v[0], 0)
            m.emit(0xe6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTTPD2DQ m128, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[1]), addr(v[0]), 0)
            m.emit(0xe6)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VCVTTPD2DQ m256, xmm
    if len(vv) == 0 && isM256(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[1]), addr(v[0]), 0)
            m.emit(0xe6)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VCVTTPD2DQ {sae}, zmm, ymm{k}{z}
    if len(vv) == 1 && isSAE(v0) && isZMM(v1) && isYMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x18)
            m.emit(0xe6)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTTPD2DQ zmm, ymm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0xe6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTTPD2DQ")
    }
    return p
}

// VCVTTPD2QQ performs "Convert with Truncation Packed Double-Precision Floating-Point Values to Packed Quadword Integers".
//
// Mnemonic        : VCVTTPD2QQ
// ISA extensions  : AVX512DQ, AVX512VL
// Supported forms : (7 forms)
//
//    * VCVTTPD2QQ m128/m64bcst, xmm{k}{z}
//    * VCVTTPD2QQ m256/m64bcst, ymm{k}{z}
//    * VCVTTPD2QQ m512/m64bcst, zmm{k}{z}
//    * VCVTTPD2QQ xmm, xmm{k}{z}
//    * VCVTTPD2QQ ymm, ymm{k}{z}
//    * VCVTTPD2QQ {sae}, zmm, zmm{k}{z}
//    * VCVTTPD2QQ zmm, zmm{k}{z}
//
func (self *Program) VCVTTPD2QQ(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCVTTPD2QQ takes 2 or 3 operands")
    }
    // VCVTTPD2QQ m128/m64bcst, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x7a)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VCVTTPD2QQ m256/m64bcst, ymm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x7a)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VCVTTPD2QQ m512/m64bcst, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x7a)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VCVTTPD2QQ xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x7a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTTPD2QQ ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x7a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTTPD2QQ {sae}, zmm, zmm{k}{z}
    if len(vv) == 1 && isSAE(v0) && isZMM(v1) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x18)
            m.emit(0x7a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTTPD2QQ zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x7a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTTPD2QQ")
    }
    return p
}

// VCVTTPD2UDQ performs "Convert with Truncation Packed Double-Precision Floating-Point Values to Packed Unsigned Doubleword Integers".
//
// Mnemonic        : VCVTTPD2UDQ
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (7 forms)
//
//    * VCVTTPD2UDQ m128/m64bcst, xmm{k}{z}
//    * VCVTTPD2UDQ m256/m64bcst, xmm{k}{z}
//    * VCVTTPD2UDQ m512/m64bcst, ymm{k}{z}
//    * VCVTTPD2UDQ xmm, xmm{k}{z}
//    * VCVTTPD2UDQ ymm, xmm{k}{z}
//    * VCVTTPD2UDQ {sae}, zmm, ymm{k}{z}
//    * VCVTTPD2UDQ zmm, ymm{k}{z}
//
func (self *Program) VCVTTPD2UDQ(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCVTTPD2UDQ takes 2 or 3 operands")
    }
    // VCVTTPD2UDQ m128/m64bcst, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x84, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x78)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VCVTTPD2UDQ m256/m64bcst, xmm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x84, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x78)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VCVTTPD2UDQ m512/m64bcst, ymm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x84, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x78)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VCVTTPD2UDQ xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfc)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTTPD2UDQ ymm, xmm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfc)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTTPD2UDQ {sae}, zmm, ymm{k}{z}
    if len(vv) == 1 && isSAE(v0) && isZMM(v1) && isYMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfc)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x18)
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTTPD2UDQ zmm, ymm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfc)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTTPD2UDQ")
    }
    return p
}

// VCVTTPD2UQQ performs "Convert with Truncation Packed Double-Precision Floating-Point Values to Packed Unsigned Quadword Integers".
//
// Mnemonic        : VCVTTPD2UQQ
// ISA extensions  : AVX512DQ, AVX512VL
// Supported forms : (7 forms)
//
//    * VCVTTPD2UQQ m128/m64bcst, xmm{k}{z}
//    * VCVTTPD2UQQ m256/m64bcst, ymm{k}{z}
//    * VCVTTPD2UQQ m512/m64bcst, zmm{k}{z}
//    * VCVTTPD2UQQ xmm, xmm{k}{z}
//    * VCVTTPD2UQQ ymm, ymm{k}{z}
//    * VCVTTPD2UQQ {sae}, zmm, zmm{k}{z}
//    * VCVTTPD2UQQ zmm, zmm{k}{z}
//
func (self *Program) VCVTTPD2UQQ(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCVTTPD2UQQ takes 2 or 3 operands")
    }
    // VCVTTPD2UQQ m128/m64bcst, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x78)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VCVTTPD2UQQ m256/m64bcst, ymm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x78)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VCVTTPD2UQQ m512/m64bcst, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x78)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VCVTTPD2UQQ xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTTPD2UQQ ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTTPD2UQQ {sae}, zmm, zmm{k}{z}
    if len(vv) == 1 && isSAE(v0) && isZMM(v1) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x18)
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTTPD2UQQ zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTTPD2UQQ")
    }
    return p
}

// VCVTTPS2DQ performs "Convert with Truncation Packed Single-Precision FP Values to Packed Dword Integers".
//
// Mnemonic        : VCVTTPS2DQ
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (11 forms)
//
//    * VCVTTPS2DQ m128/m32bcst, xmm{k}{z}
//    * VCVTTPS2DQ m256/m32bcst, ymm{k}{z}
//    * VCVTTPS2DQ m512/m32bcst, zmm{k}{z}
//    * VCVTTPS2DQ xmm, xmm{k}{z}
//    * VCVTTPS2DQ ymm, ymm{k}{z}
//    * VCVTTPS2DQ xmm, xmm
//    * VCVTTPS2DQ m128, xmm
//    * VCVTTPS2DQ ymm, ymm
//    * VCVTTPS2DQ m256, ymm
//    * VCVTTPS2DQ {sae}, zmm, zmm{k}{z}
//    * VCVTTPS2DQ zmm, zmm{k}{z}
//
func (self *Program) VCVTTPS2DQ(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCVTTPS2DQ takes 2 or 3 operands")
    }
    // VCVTTPS2DQ m128/m32bcst, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x5b)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VCVTTPS2DQ m256/m32bcst, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x5b)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VCVTTPS2DQ m512/m32bcst, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x5b)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VCVTTPS2DQ xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x5b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTTPS2DQ ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x5b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTTPS2DQ xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[1]), v[0], 0)
            m.emit(0x5b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTTPS2DQ m128, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x5b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VCVTTPS2DQ ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(6, hcode(v[1]), v[0], 0)
            m.emit(0x5b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTTPS2DQ m256, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(6, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x5b)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VCVTTPS2DQ {sae}, zmm, zmm{k}{z}
    if len(vv) == 1 && isSAE(v0) && isZMM(v1) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x18)
            m.emit(0x5b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTTPS2DQ zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x5b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTTPS2DQ")
    }
    return p
}

// VCVTTPS2QQ performs "Convert with Truncation Packed Single Precision Floating-Point Values to Packed Singed Quadword Integer Values".
//
// Mnemonic        : VCVTTPS2QQ
// ISA extensions  : AVX512DQ, AVX512VL
// Supported forms : (7 forms)
//
//    * VCVTTPS2QQ m64/m32bcst, xmm{k}{z}
//    * VCVTTPS2QQ m128/m32bcst, ymm{k}{z}
//    * VCVTTPS2QQ m256/m32bcst, zmm{k}{z}
//    * VCVTTPS2QQ xmm, xmm{k}{z}
//    * VCVTTPS2QQ xmm, ymm{k}{z}
//    * VCVTTPS2QQ {sae}, ymm, zmm{k}{z}
//    * VCVTTPS2QQ ymm, zmm{k}{z}
//
func (self *Program) VCVTTPS2QQ(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCVTTPS2QQ takes 2 or 3 operands")
    }
    // VCVTTPS2QQ m64/m32bcst, xmm{k}{z}
    if len(vv) == 0 && isM64M32bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x7a)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VCVTTPS2QQ m128/m32bcst, ymm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x7a)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VCVTTPS2QQ m256/m32bcst, zmm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x7a)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VCVTTPS2QQ xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x7a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTTPS2QQ xmm, ymm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x7a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTTPS2QQ {sae}, ymm, zmm{k}{z}
    if len(vv) == 1 && isSAE(v0) && isEVEXYMM(v1) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x18)
            m.emit(0x7a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTTPS2QQ ymm, zmm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x7a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTTPS2QQ")
    }
    return p
}

// VCVTTPS2UDQ performs "Convert with Truncation Packed Single-Precision Floating-Point Values to Packed Unsigned Doubleword Integer Values".
//
// Mnemonic        : VCVTTPS2UDQ
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (7 forms)
//
//    * VCVTTPS2UDQ m128/m32bcst, xmm{k}{z}
//    * VCVTTPS2UDQ m256/m32bcst, ymm{k}{z}
//    * VCVTTPS2UDQ m512/m32bcst, zmm{k}{z}
//    * VCVTTPS2UDQ xmm, xmm{k}{z}
//    * VCVTTPS2UDQ ymm, ymm{k}{z}
//    * VCVTTPS2UDQ {sae}, zmm, zmm{k}{z}
//    * VCVTTPS2UDQ zmm, zmm{k}{z}
//
func (self *Program) VCVTTPS2UDQ(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCVTTPS2UDQ takes 2 or 3 operands")
    }
    // VCVTTPS2UDQ m128/m32bcst, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x78)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VCVTTPS2UDQ m256/m32bcst, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x78)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VCVTTPS2UDQ m512/m32bcst, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x78)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VCVTTPS2UDQ xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7c)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTTPS2UDQ ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7c)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTTPS2UDQ {sae}, zmm, zmm{k}{z}
    if len(vv) == 1 && isSAE(v0) && isZMM(v1) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x18)
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTTPS2UDQ zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7c)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTTPS2UDQ")
    }
    return p
}

// VCVTTPS2UQQ performs "Convert with Truncation Packed Single Precision Floating-Point Values to Packed Unsigned Quadword Integer Values".
//
// Mnemonic        : VCVTTPS2UQQ
// ISA extensions  : AVX512DQ, AVX512VL
// Supported forms : (7 forms)
//
//    * VCVTTPS2UQQ m64/m32bcst, xmm{k}{z}
//    * VCVTTPS2UQQ m128/m32bcst, ymm{k}{z}
//    * VCVTTPS2UQQ m256/m32bcst, zmm{k}{z}
//    * VCVTTPS2UQQ xmm, xmm{k}{z}
//    * VCVTTPS2UQQ xmm, ymm{k}{z}
//    * VCVTTPS2UQQ {sae}, ymm, zmm{k}{z}
//    * VCVTTPS2UQQ ymm, zmm{k}{z}
//
func (self *Program) VCVTTPS2UQQ(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCVTTPS2UQQ takes 2 or 3 operands")
    }
    // VCVTTPS2UQQ m64/m32bcst, xmm{k}{z}
    if len(vv) == 0 && isM64M32bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x78)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VCVTTPS2UQQ m128/m32bcst, ymm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x78)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VCVTTPS2UQQ m256/m32bcst, zmm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x78)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VCVTTPS2UQQ xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTTPS2UQQ xmm, ymm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTTPS2UQQ {sae}, ymm, zmm{k}{z}
    if len(vv) == 1 && isSAE(v0) && isEVEXYMM(v1) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x18)
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTTPS2UQQ ymm, zmm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTTPS2UQQ")
    }
    return p
}

// VCVTTSD2SI performs "Convert with Truncation Scalar Double-Precision FP Value to Signed Integer".
//
// Mnemonic        : VCVTTSD2SI
// ISA extensions  : AVX, AVX512F
// Supported forms : (10 forms)
//
//    * VCVTTSD2SI xmm, r32
//    * VCVTTSD2SI m64, r32
//    * VCVTTSD2SI m64, r32
//    * VCVTTSD2SI xmm, r64
//    * VCVTTSD2SI m64, r64
//    * VCVTTSD2SI m64, r64
//    * VCVTTSD2SI {sae}, xmm, r32
//    * VCVTTSD2SI {sae}, xmm, r64
//    * VCVTTSD2SI xmm, r32
//    * VCVTTSD2SI xmm, r64
//
func (self *Program) VCVTTSD2SI(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCVTTSD2SI takes 2 or 3 operands")
    }
    // VCVTTSD2SI xmm, r32
    if len(vv) == 0 && isXMM(v0) && isReg32(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[1]), v[0], 0)
            m.emit(0x2c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTTSD2SI m64, r32
    if len(vv) == 0 && isM64(v0) && isReg32(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x2c)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VCVTTSD2SI m64, r32
    if len(vv) == 0 && isM64(v0) && isReg32(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x07, 0b00, ehcode(v[1]), addr(v[0]), 0, 0, 0, 0)
            m.emit(0x2c)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VCVTTSD2SI xmm, r64
    if len(vv) == 0 && isXMM(v0) && isReg64(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe1 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfb)
            m.emit(0x2c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTTSD2SI m64, r64
    if len(vv) == 0 && isM64(v0) && isReg64(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b1, 0x83, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x2c)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VCVTTSD2SI m64, r64
    if len(vv) == 0 && isM64(v0) && isReg64(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x87, 0b00, ehcode(v[1]), addr(v[0]), 0, 0, 0, 0)
            m.emit(0x2c)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VCVTTSD2SI {sae}, xmm, r32
    if len(vv) == 1 && isSAE(v0) && isEVEXXMM(v1) && isReg32(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7f)
            m.emit(0x18)
            m.emit(0x2c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTTSD2SI {sae}, xmm, r64
    if len(vv) == 1 && isSAE(v0) && isEVEXXMM(v1) && isReg64(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xff)
            m.emit(0x18)
            m.emit(0x2c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTTSD2SI xmm, r32
    if len(vv) == 0 && isEVEXXMM(v0) && isReg32(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7f)
            m.emit(0x48)
            m.emit(0x2c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTTSD2SI xmm, r64
    if len(vv) == 0 && isEVEXXMM(v0) && isReg64(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xff)
            m.emit(0x48)
            m.emit(0x2c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTTSD2SI")
    }
    return p
}

// VCVTTSD2USI performs "Convert with Truncation Scalar Double-Precision Floating-Point Value to Unsigned Integer".
//
// Mnemonic        : VCVTTSD2USI
// ISA extensions  : AVX512F
// Supported forms : (6 forms)
//
//    * VCVTTSD2USI m64, r32
//    * VCVTTSD2USI m64, r64
//    * VCVTTSD2USI {sae}, xmm, r32
//    * VCVTTSD2USI {sae}, xmm, r64
//    * VCVTTSD2USI xmm, r32
//    * VCVTTSD2USI xmm, r64
//
func (self *Program) VCVTTSD2USI(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCVTTSD2USI takes 2 or 3 operands")
    }
    // VCVTTSD2USI m64, r32
    if len(vv) == 0 && isM64(v0) && isReg32(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x07, 0b00, ehcode(v[1]), addr(v[0]), 0, 0, 0, 0)
            m.emit(0x78)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VCVTTSD2USI m64, r64
    if len(vv) == 0 && isM64(v0) && isReg64(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x87, 0b00, ehcode(v[1]), addr(v[0]), 0, 0, 0, 0)
            m.emit(0x78)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VCVTTSD2USI {sae}, xmm, r32
    if len(vv) == 1 && isSAE(v0) && isEVEXXMM(v1) && isReg32(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7f)
            m.emit(0x18)
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTTSD2USI {sae}, xmm, r64
    if len(vv) == 1 && isSAE(v0) && isEVEXXMM(v1) && isReg64(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xff)
            m.emit(0x18)
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTTSD2USI xmm, r32
    if len(vv) == 0 && isEVEXXMM(v0) && isReg32(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7f)
            m.emit(0x48)
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTTSD2USI xmm, r64
    if len(vv) == 0 && isEVEXXMM(v0) && isReg64(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xff)
            m.emit(0x48)
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTTSD2USI")
    }
    return p
}

// VCVTTSS2SI performs "Convert with Truncation Scalar Single-Precision FP Value to Dword Integer".
//
// Mnemonic        : VCVTTSS2SI
// ISA extensions  : AVX, AVX512F
// Supported forms : (10 forms)
//
//    * VCVTTSS2SI xmm, r32
//    * VCVTTSS2SI m32, r32
//    * VCVTTSS2SI m32, r32
//    * VCVTTSS2SI xmm, r64
//    * VCVTTSS2SI m32, r64
//    * VCVTTSS2SI m32, r64
//    * VCVTTSS2SI {sae}, xmm, r32
//    * VCVTTSS2SI {sae}, xmm, r64
//    * VCVTTSS2SI xmm, r32
//    * VCVTTSS2SI xmm, r64
//
func (self *Program) VCVTTSS2SI(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCVTTSS2SI takes 2 or 3 operands")
    }
    // VCVTTSS2SI xmm, r32
    if len(vv) == 0 && isXMM(v0) && isReg32(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[1]), v[0], 0)
            m.emit(0x2c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTTSS2SI m32, r32
    if len(vv) == 0 && isM32(v0) && isReg32(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x2c)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VCVTTSS2SI m32, r32
    if len(vv) == 0 && isM32(v0) && isReg32(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b00, ehcode(v[1]), addr(v[0]), 0, 0, 0, 0)
            m.emit(0x2c)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VCVTTSS2SI xmm, r64
    if len(vv) == 0 && isXMM(v0) && isReg64(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe1 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfa)
            m.emit(0x2c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTTSS2SI m32, r64
    if len(vv) == 0 && isM32(v0) && isReg64(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b1, 0x82, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x2c)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VCVTTSS2SI m32, r64
    if len(vv) == 0 && isM32(v0) && isReg64(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x86, 0b00, ehcode(v[1]), addr(v[0]), 0, 0, 0, 0)
            m.emit(0x2c)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VCVTTSS2SI {sae}, xmm, r32
    if len(vv) == 1 && isSAE(v0) && isEVEXXMM(v1) && isReg32(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7e)
            m.emit(0x18)
            m.emit(0x2c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTTSS2SI {sae}, xmm, r64
    if len(vv) == 1 && isSAE(v0) && isEVEXXMM(v1) && isReg64(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfe)
            m.emit(0x18)
            m.emit(0x2c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTTSS2SI xmm, r32
    if len(vv) == 0 && isEVEXXMM(v0) && isReg32(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit(0x48)
            m.emit(0x2c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTTSS2SI xmm, r64
    if len(vv) == 0 && isEVEXXMM(v0) && isReg64(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfe)
            m.emit(0x48)
            m.emit(0x2c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTTSS2SI")
    }
    return p
}

// VCVTTSS2USI performs "Convert with Truncation Scalar Single-Precision Floating-Point Value to Unsigned Integer".
//
// Mnemonic        : VCVTTSS2USI
// ISA extensions  : AVX512F
// Supported forms : (6 forms)
//
//    * VCVTTSS2USI m32, r32
//    * VCVTTSS2USI m32, r64
//    * VCVTTSS2USI {sae}, xmm, r32
//    * VCVTTSS2USI {sae}, xmm, r64
//    * VCVTTSS2USI xmm, r32
//    * VCVTTSS2USI xmm, r64
//
func (self *Program) VCVTTSS2USI(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCVTTSS2USI takes 2 or 3 operands")
    }
    // VCVTTSS2USI m32, r32
    if len(vv) == 0 && isM32(v0) && isReg32(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b00, ehcode(v[1]), addr(v[0]), 0, 0, 0, 0)
            m.emit(0x78)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VCVTTSS2USI m32, r64
    if len(vv) == 0 && isM32(v0) && isReg64(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x86, 0b00, ehcode(v[1]), addr(v[0]), 0, 0, 0, 0)
            m.emit(0x78)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VCVTTSS2USI {sae}, xmm, r32
    if len(vv) == 1 && isSAE(v0) && isEVEXXMM(v1) && isReg32(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7e)
            m.emit(0x18)
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTTSS2USI {sae}, xmm, r64
    if len(vv) == 1 && isSAE(v0) && isEVEXXMM(v1) && isReg64(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfe)
            m.emit(0x18)
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTTSS2USI xmm, r32
    if len(vv) == 0 && isEVEXXMM(v0) && isReg32(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit(0x48)
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTTSS2USI xmm, r64
    if len(vv) == 0 && isEVEXXMM(v0) && isReg64(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfe)
            m.emit(0x48)
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTTSS2USI")
    }
    return p
}

// VCVTUDQ2PD performs "Convert Packed Unsigned Doubleword Integers to Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VCVTUDQ2PD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VCVTUDQ2PD m64/m32bcst, xmm{k}{z}
//    * VCVTUDQ2PD m128/m32bcst, ymm{k}{z}
//    * VCVTUDQ2PD m256/m32bcst, zmm{k}{z}
//    * VCVTUDQ2PD xmm, xmm{k}{z}
//    * VCVTUDQ2PD xmm, ymm{k}{z}
//    * VCVTUDQ2PD ymm, zmm{k}{z}
//
func (self *Program) VCVTUDQ2PD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VCVTUDQ2PD m64/m32bcst, xmm{k}{z}
    if isM64M32bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x7a)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VCVTUDQ2PD m128/m32bcst, ymm{k}{z}
    if isM128M32bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x7a)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VCVTUDQ2PD m256/m32bcst, zmm{k}{z}
    if isM256M32bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x7a)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VCVTUDQ2PD xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x7a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTUDQ2PD xmm, ymm{k}{z}
    if isEVEXXMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x7a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTUDQ2PD ymm, zmm{k}{z}
    if isEVEXYMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x7a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTUDQ2PD")
    }
    return p
}

// VCVTUDQ2PS performs "Convert Packed Unsigned Doubleword Integers to Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VCVTUDQ2PS
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (7 forms)
//
//    * VCVTUDQ2PS m128/m32bcst, xmm{k}{z}
//    * VCVTUDQ2PS m256/m32bcst, ymm{k}{z}
//    * VCVTUDQ2PS m512/m32bcst, zmm{k}{z}
//    * VCVTUDQ2PS xmm, xmm{k}{z}
//    * VCVTUDQ2PS ymm, ymm{k}{z}
//    * VCVTUDQ2PS {er}, zmm, zmm{k}{z}
//    * VCVTUDQ2PS zmm, zmm{k}{z}
//
func (self *Program) VCVTUDQ2PS(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCVTUDQ2PS takes 2 or 3 operands")
    }
    // VCVTUDQ2PS m128/m32bcst, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x07, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x7a)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VCVTUDQ2PS m256/m32bcst, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x07, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x7a)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VCVTUDQ2PS m512/m32bcst, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x07, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x7a)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VCVTUDQ2PS xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7f)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x7a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTUDQ2PS ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7f)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x7a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTUDQ2PS {er}, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7f)
            m.emit((zcode(v[2]) << 7) | (vcode(v[0]) << 5) | kcode(v[2]) | 0x18)
            m.emit(0x7a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTUDQ2PS zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7f)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x7a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTUDQ2PS")
    }
    return p
}

// VCVTUQQ2PD performs "Convert Packed Unsigned Quadword Integers to Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VCVTUQQ2PD
// ISA extensions  : AVX512DQ, AVX512VL
// Supported forms : (7 forms)
//
//    * VCVTUQQ2PD m128/m64bcst, xmm{k}{z}
//    * VCVTUQQ2PD m256/m64bcst, ymm{k}{z}
//    * VCVTUQQ2PD m512/m64bcst, zmm{k}{z}
//    * VCVTUQQ2PD xmm, xmm{k}{z}
//    * VCVTUQQ2PD ymm, ymm{k}{z}
//    * VCVTUQQ2PD {er}, zmm, zmm{k}{z}
//    * VCVTUQQ2PD zmm, zmm{k}{z}
//
func (self *Program) VCVTUQQ2PD(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCVTUQQ2PD takes 2 or 3 operands")
    }
    // VCVTUQQ2PD m128/m64bcst, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x86, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x7a)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VCVTUQQ2PD m256/m64bcst, ymm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x86, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x7a)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VCVTUQQ2PD m512/m64bcst, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x86, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x7a)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VCVTUQQ2PD xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfe)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x7a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTUQQ2PD ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfe)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x7a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTUQQ2PD {er}, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfe)
            m.emit((zcode(v[2]) << 7) | (vcode(v[0]) << 5) | kcode(v[2]) | 0x18)
            m.emit(0x7a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTUQQ2PD zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfe)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x7a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTUQQ2PD")
    }
    return p
}

// VCVTUQQ2PS performs "Convert Packed Unsigned Quadword Integers to Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VCVTUQQ2PS
// ISA extensions  : AVX512DQ, AVX512VL
// Supported forms : (7 forms)
//
//    * VCVTUQQ2PS m128/m64bcst, xmm{k}{z}
//    * VCVTUQQ2PS m256/m64bcst, xmm{k}{z}
//    * VCVTUQQ2PS m512/m64bcst, ymm{k}{z}
//    * VCVTUQQ2PS xmm, xmm{k}{z}
//    * VCVTUQQ2PS ymm, xmm{k}{z}
//    * VCVTUQQ2PS {er}, zmm, ymm{k}{z}
//    * VCVTUQQ2PS zmm, ymm{k}{z}
//
func (self *Program) VCVTUQQ2PS(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VCVTUQQ2PS takes 2 or 3 operands")
    }
    // VCVTUQQ2PS m128/m64bcst, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x87, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x7a)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VCVTUQQ2PS m256/m64bcst, xmm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x87, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x7a)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VCVTUQQ2PS m512/m64bcst, ymm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x87, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x7a)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VCVTUQQ2PS xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xff)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x7a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTUQQ2PS ymm, xmm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xff)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x7a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VCVTUQQ2PS {er}, zmm, ymm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isYMMkz(vv[0]) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xff)
            m.emit((zcode(v[2]) << 7) | (vcode(v[0]) << 5) | kcode(v[2]) | 0x18)
            m.emit(0x7a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VCVTUQQ2PS zmm, ymm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xff)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x7a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTUQQ2PS")
    }
    return p
}

// VCVTUSI2SD performs "Convert Unsigned Integer to Scalar Double-Precision Floating-Point Value".
//
// Mnemonic        : VCVTUSI2SD
// ISA extensions  : AVX512F
// Supported forms : (5 forms)
//
//    * VCVTUSI2SD r32, xmm, xmm
//    * VCVTUSI2SD m32, xmm, xmm
//    * VCVTUSI2SD m64, xmm, xmm
//    * VCVTUSI2SD {er}, r64, xmm, xmm
//    * VCVTUSI2SD r64, xmm, xmm
//
func (self *Program) VCVTUSI2SD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VCVTUSI2SD takes 3 or 4 operands")
    }
    // VCVTUSI2SD r32, xmm, xmm
    if len(vv) == 0 && isReg32(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7f ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | 0x00)
            m.emit(0x7b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VCVTUSI2SD m32, xmm, xmm
    if len(vv) == 0 && isM32(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x07, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), 0, 0, 0)
            m.emit(0x7b)
            m.mrsd(lcode(v[2]), addr(v[0]), 4)
        })
    }
    // VCVTUSI2SD m64, xmm, xmm
    if len(vv) == 0 && isM64(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x87, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), 0, 0, 0)
            m.emit(0x7b)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    // VCVTUSI2SD {er}, r64, xmm, xmm
    if len(vv) == 1 && isER(v0) && isReg64(v1) && isEVEXXMM(v2) && isEVEXXMM(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xff ^ (hlcode(v[2]) << 3))
            m.emit((vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | 0x10)
            m.emit(0x7b)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VCVTUSI2SD r64, xmm, xmm
    if len(vv) == 0 && isReg64(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xff ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | 0x40)
            m.emit(0x7b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTUSI2SD")
    }
    return p
}

// VCVTUSI2SS performs "Convert Unsigned Integer to Scalar Single-Precision Floating-Point Value".
//
// Mnemonic        : VCVTUSI2SS
// ISA extensions  : AVX512F
// Supported forms : (6 forms)
//
//    * VCVTUSI2SS m32, xmm, xmm
//    * VCVTUSI2SS m64, xmm, xmm
//    * VCVTUSI2SS {er}, r32, xmm, xmm
//    * VCVTUSI2SS {er}, r64, xmm, xmm
//    * VCVTUSI2SS r32, xmm, xmm
//    * VCVTUSI2SS r64, xmm, xmm
//
func (self *Program) VCVTUSI2SS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VCVTUSI2SS takes 3 or 4 operands")
    }
    // VCVTUSI2SS m32, xmm, xmm
    if len(vv) == 0 && isM32(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), 0, 0, 0)
            m.emit(0x7b)
            m.mrsd(lcode(v[2]), addr(v[0]), 4)
        })
    }
    // VCVTUSI2SS m64, xmm, xmm
    if len(vv) == 0 && isM64(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x86, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), 0, 0, 0)
            m.emit(0x7b)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    // VCVTUSI2SS {er}, r32, xmm, xmm
    if len(vv) == 1 && isER(v0) && isReg32(v1) && isEVEXXMM(v2) && isEVEXXMM(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7e ^ (hlcode(v[2]) << 3))
            m.emit((vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | 0x10)
            m.emit(0x7b)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VCVTUSI2SS {er}, r64, xmm, xmm
    if len(vv) == 1 && isER(v0) && isReg64(v1) && isEVEXXMM(v2) && isEVEXXMM(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfe ^ (hlcode(v[2]) << 3))
            m.emit((vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | 0x10)
            m.emit(0x7b)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VCVTUSI2SS r32, xmm, xmm
    if len(vv) == 0 && isReg32(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7e ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | 0x40)
            m.emit(0x7b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VCVTUSI2SS r64, xmm, xmm
    if len(vv) == 0 && isReg64(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfe ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | 0x40)
            m.emit(0x7b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VCVTUSI2SS")
    }
    return p
}

// VDBPSADBW performs "Double Block Packed Sum-Absolute-Differences on Unsigned Bytes".
//
// Mnemonic        : VDBPSADBW
// ISA extensions  : AVX512BW, AVX512VL
// Supported forms : (6 forms)
//
//    * VDBPSADBW imm8, xmm, xmm, xmm{k}{z}
//    * VDBPSADBW imm8, m128, xmm, xmm{k}{z}
//    * VDBPSADBW imm8, ymm, ymm, ymm{k}{z}
//    * VDBPSADBW imm8, m256, ymm, ymm{k}{z}
//    * VDBPSADBW imm8, zmm, zmm, zmm{k}{z}
//    * VDBPSADBW imm8, m512, zmm, zmm{k}{z}
//
func (self *Program) VDBPSADBW(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VDBPSADBW imm8, xmm, xmm, xmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x00)
            m.emit(0x42)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VDBPSADBW imm8, m128, xmm, xmm{k}{z}
    if isImm8(v0) && isM128(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), 0)
            m.emit(0x42)
            m.mrsd(lcode(v[3]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VDBPSADBW imm8, ymm, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x20)
            m.emit(0x42)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VDBPSADBW imm8, m256, ymm, ymm{k}{z}
    if isImm8(v0) && isM256(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b01, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), 0)
            m.emit(0x42)
            m.mrsd(lcode(v[3]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VDBPSADBW imm8, zmm, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x42)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VDBPSADBW imm8, m512, zmm, zmm{k}{z}
    if isImm8(v0) && isM512(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), 0)
            m.emit(0x42)
            m.mrsd(lcode(v[3]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VDBPSADBW")
    }
    return p
}

// VDIVPD performs "Divide Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VDIVPD
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (11 forms)
//
//    * VDIVPD m128/m64bcst, xmm, xmm{k}{z}
//    * VDIVPD xmm, xmm, xmm{k}{z}
//    * VDIVPD m256/m64bcst, ymm, ymm{k}{z}
//    * VDIVPD ymm, ymm, ymm{k}{z}
//    * VDIVPD m512/m64bcst, zmm, zmm{k}{z}
//    * VDIVPD xmm, xmm, xmm
//    * VDIVPD m128, xmm, xmm
//    * VDIVPD ymm, ymm, ymm
//    * VDIVPD m256, ymm, ymm
//    * VDIVPD {er}, zmm, zmm, zmm{k}{z}
//    * VDIVPD zmm, zmm, zmm{k}{z}
//
func (self *Program) VDIVPD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VDIVPD takes 3 or 4 operands")
    }
    // VDIVPD m128/m64bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x5e)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VDIVPD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x5e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VDIVPD m256/m64bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x5e)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VDIVPD ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x5e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VDIVPD m512/m64bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x5e)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VDIVPD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x5e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VDIVPD m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x5e)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VDIVPD ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x5e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VDIVPD m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x5e)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VDIVPD {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x5e)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VDIVPD zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x5e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VDIVPD")
    }
    return p
}

// VDIVPS performs "Divide Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VDIVPS
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (11 forms)
//
//    * VDIVPS m128/m32bcst, xmm, xmm{k}{z}
//    * VDIVPS xmm, xmm, xmm{k}{z}
//    * VDIVPS m256/m32bcst, ymm, ymm{k}{z}
//    * VDIVPS ymm, ymm, ymm{k}{z}
//    * VDIVPS m512/m32bcst, zmm, zmm{k}{z}
//    * VDIVPS xmm, xmm, xmm
//    * VDIVPS m128, xmm, xmm
//    * VDIVPS ymm, ymm, ymm
//    * VDIVPS m256, ymm, ymm
//    * VDIVPS {er}, zmm, zmm, zmm{k}{z}
//    * VDIVPS zmm, zmm, zmm{k}{z}
//
func (self *Program) VDIVPS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VDIVPS takes 3 or 4 operands")
    }
    // VDIVPS m128/m32bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x5e)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VDIVPS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x5e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VDIVPS m256/m32bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x5e)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VDIVPS ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x5e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VDIVPS m512/m32bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x5e)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VDIVPS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x5e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VDIVPS m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x5e)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VDIVPS ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x5e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VDIVPS m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x5e)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VDIVPS {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7c ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x5e)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VDIVPS zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x5e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VDIVPS")
    }
    return p
}

// VDIVSD performs "Divide Scalar Double-Precision Floating-Point Values".
//
// Mnemonic        : VDIVSD
// ISA extensions  : AVX, AVX512F
// Supported forms : (5 forms)
//
//    * VDIVSD m64, xmm, xmm{k}{z}
//    * VDIVSD xmm, xmm, xmm
//    * VDIVSD m64, xmm, xmm
//    * VDIVSD {er}, xmm, xmm, xmm{k}{z}
//    * VDIVSD xmm, xmm, xmm{k}{z}
//
func (self *Program) VDIVSD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VDIVSD takes 3 or 4 operands")
    }
    // VDIVSD m64, xmm, xmm{k}{z}
    if len(vv) == 0 && isM64(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x87, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x5e)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    // VDIVSD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x5e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VDIVSD m64, xmm, xmm
    if len(vv) == 0 && isM64(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x5e)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VDIVSD {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xff ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x5e)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VDIVSD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xff ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x5e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VDIVSD")
    }
    return p
}

// VDIVSS performs "Divide Scalar Single-Precision Floating-Point Values".
//
// Mnemonic        : VDIVSS
// ISA extensions  : AVX, AVX512F
// Supported forms : (5 forms)
//
//    * VDIVSS m32, xmm, xmm{k}{z}
//    * VDIVSS xmm, xmm, xmm
//    * VDIVSS m32, xmm, xmm
//    * VDIVSS {er}, xmm, xmm, xmm{k}{z}
//    * VDIVSS xmm, xmm, xmm{k}{z}
//
func (self *Program) VDIVSS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VDIVSS takes 3 or 4 operands")
    }
    // VDIVSS m32, xmm, xmm{k}{z}
    if len(vv) == 0 && isM32(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x5e)
            m.mrsd(lcode(v[2]), addr(v[0]), 4)
        })
    }
    // VDIVSS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x5e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VDIVSS m32, xmm, xmm
    if len(vv) == 0 && isM32(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x5e)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VDIVSS {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7e ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x5e)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VDIVSS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7e ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x5e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VDIVSS")
    }
    return p
}

// VDPPD performs "Dot Product of Packed Double Precision Floating-Point Values".
//
// Mnemonic        : VDPPD
// ISA extensions  : AVX
// Supported forms : (2 forms)
//
//    * VDPPD imm8, xmm, xmm, xmm
//    * VDPPD imm8, m128, xmm, xmm
//
func (self *Program) VDPPD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VDPPD imm8, xmm, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x41)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VDPPD imm8, m128, xmm, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x41)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VDPPD")
    }
    return p
}

// VDPPS performs "Dot Product of Packed Single Precision Floating-Point Values".
//
// Mnemonic        : VDPPS
// ISA extensions  : AVX
// Supported forms : (4 forms)
//
//    * VDPPS imm8, xmm, xmm, xmm
//    * VDPPS imm8, m128, xmm, xmm
//    * VDPPS imm8, ymm, ymm, ymm
//    * VDPPS imm8, m256, ymm, ymm
//
func (self *Program) VDPPS(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VDPPS imm8, xmm, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x40)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VDPPS imm8, m128, xmm, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x40)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VDPPS imm8, ymm, ymm, ymm
    if isImm8(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit(0x40)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VDPPS imm8, m256, ymm, ymm
    if isImm8(v0) && isM256(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x40)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VDPPS")
    }
    return p
}

// VEXP2PD performs "Approximation to the Exponential 2^x of Packed Double-Precision Floating-Point Values with Less Than 2^-23 Relative Error".
//
// Mnemonic        : VEXP2PD
// ISA extensions  : AVX512ER
// Supported forms : (3 forms)
//
//    * VEXP2PD m512/m64bcst, zmm{k}{z}
//    * VEXP2PD {sae}, zmm, zmm{k}{z}
//    * VEXP2PD zmm, zmm{k}{z}
//
func (self *Program) VEXP2PD(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VEXP2PD takes 2 or 3 operands")
    }
    // VEXP2PD m512/m64bcst, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512ER)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0xc8)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VEXP2PD {sae}, zmm, zmm{k}{z}
    if len(vv) == 1 && isSAE(v0) && isZMM(v1) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512ER)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x18)
            m.emit(0xc8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VEXP2PD zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512ER)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0xc8)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VEXP2PD")
    }
    return p
}

// VEXP2PS performs "Approximation to the Exponential 2^x of Packed Single-Precision Floating-Point Values with Less Than 2^-23 Relative Error".
//
// Mnemonic        : VEXP2PS
// ISA extensions  : AVX512ER
// Supported forms : (3 forms)
//
//    * VEXP2PS m512/m32bcst, zmm{k}{z}
//    * VEXP2PS {sae}, zmm, zmm{k}{z}
//    * VEXP2PS zmm, zmm{k}{z}
//
func (self *Program) VEXP2PS(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VEXP2PS takes 2 or 3 operands")
    }
    // VEXP2PS m512/m32bcst, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512ER)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0xc8)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VEXP2PS {sae}, zmm, zmm{k}{z}
    if len(vv) == 1 && isSAE(v0) && isZMM(v1) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512ER)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x18)
            m.emit(0xc8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VEXP2PS zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512ER)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0xc8)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VEXP2PS")
    }
    return p
}

// VEXPANDPD performs "Load Sparse Packed Double-Precision Floating-Point Values from Dense Memory".
//
// Mnemonic        : VEXPANDPD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VEXPANDPD xmm, xmm{k}{z}
//    * VEXPANDPD ymm, ymm{k}{z}
//    * VEXPANDPD zmm, zmm{k}{z}
//    * VEXPANDPD m128, xmm{k}{z}
//    * VEXPANDPD m256, ymm{k}{z}
//    * VEXPANDPD m512, zmm{k}{z}
//
func (self *Program) VEXPANDPD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VEXPANDPD xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x88)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VEXPANDPD ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x88)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VEXPANDPD zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x88)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VEXPANDPD m128, xmm{k}{z}
    if isM128(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x88)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VEXPANDPD m256, ymm{k}{z}
    if isM256(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x88)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VEXPANDPD m512, zmm{k}{z}
    if isM512(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x88)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VEXPANDPD")
    }
    return p
}

// VEXPANDPS performs "Load Sparse Packed Single-Precision Floating-Point Values from Dense Memory".
//
// Mnemonic        : VEXPANDPS
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VEXPANDPS xmm, xmm{k}{z}
//    * VEXPANDPS ymm, ymm{k}{z}
//    * VEXPANDPS zmm, zmm{k}{z}
//    * VEXPANDPS m128, xmm{k}{z}
//    * VEXPANDPS m256, ymm{k}{z}
//    * VEXPANDPS m512, zmm{k}{z}
//
func (self *Program) VEXPANDPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VEXPANDPS xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x88)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VEXPANDPS ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x88)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VEXPANDPS zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x88)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VEXPANDPS m128, xmm{k}{z}
    if isM128(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x88)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VEXPANDPS m256, ymm{k}{z}
    if isM256(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x88)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VEXPANDPS m512, zmm{k}{z}
    if isM512(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x88)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VEXPANDPS")
    }
    return p
}

// VEXTRACTF128 performs "Extract Packed Floating-Point Values".
//
// Mnemonic        : VEXTRACTF128
// ISA extensions  : AVX
// Supported forms : (2 forms)
//
//    * VEXTRACTF128 imm8, ymm, xmm
//    * VEXTRACTF128 imm8, ymm, m128
//
func (self *Program) VEXTRACTF128(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VEXTRACTF128 imm8, ymm, xmm
    if isImm8(v0) && isYMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[1]) << 7) ^ (hcode(v[2]) << 5))
            m.emit(0x7d)
            m.emit(0x19)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VEXTRACTF128 imm8, ymm, m128
    if isImm8(v0) && isYMM(v1) && isM128(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[1]), addr(v[2]), 0)
            m.emit(0x19)
            m.mrsd(lcode(v[1]), addr(v[2]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VEXTRACTF128")
    }
    return p
}

// VEXTRACTF32X4 performs "Extract 128 Bits of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VEXTRACTF32X4
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (4 forms)
//
//    * VEXTRACTF32X4 imm8, ymm, xmm{k}{z}
//    * VEXTRACTF32X4 imm8, ymm, m128{k}{z}
//    * VEXTRACTF32X4 imm8, zmm, xmm{k}{z}
//    * VEXTRACTF32X4 imm8, zmm, m128{k}{z}
//
func (self *Program) VEXTRACTF32X4(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VEXTRACTF32X4 imm8, ymm, xmm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[1]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x28)
            m.emit(0x19)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VEXTRACTF32X4 imm8, ymm, m128{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isM128kz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b01, ehcode(v[1]), addr(v[2]), 0, kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x19)
            m.mrsd(lcode(v[1]), addr(v[2]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VEXTRACTF32X4 imm8, zmm, xmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[1]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x48)
            m.emit(0x19)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VEXTRACTF32X4 imm8, zmm, m128{k}{z}
    if isImm8(v0) && isZMM(v1) && isM128kz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b10, ehcode(v[1]), addr(v[2]), 0, kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x19)
            m.mrsd(lcode(v[1]), addr(v[2]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VEXTRACTF32X4")
    }
    return p
}

// VEXTRACTF32X8 performs "Extract 256 Bits of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VEXTRACTF32X8
// ISA extensions  : AVX512DQ
// Supported forms : (2 forms)
//
//    * VEXTRACTF32X8 imm8, zmm, ymm{k}{z}
//    * VEXTRACTF32X8 imm8, zmm, m256{k}{z}
//
func (self *Program) VEXTRACTF32X8(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VEXTRACTF32X8 imm8, zmm, ymm{k}{z}
    if isImm8(v0) && isZMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[1]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x48)
            m.emit(0x1b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VEXTRACTF32X8 imm8, zmm, m256{k}{z}
    if isImm8(v0) && isZMM(v1) && isM256kz(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b10, ehcode(v[1]), addr(v[2]), 0, kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x1b)
            m.mrsd(lcode(v[1]), addr(v[2]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VEXTRACTF32X8")
    }
    return p
}

// VEXTRACTF64X2 performs "Extract 128 Bits of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VEXTRACTF64X2
// ISA extensions  : AVX512DQ, AVX512VL
// Supported forms : (4 forms)
//
//    * VEXTRACTF64X2 imm8, ymm, xmm{k}{z}
//    * VEXTRACTF64X2 imm8, ymm, m128{k}{z}
//    * VEXTRACTF64X2 imm8, zmm, xmm{k}{z}
//    * VEXTRACTF64X2 imm8, zmm, m128{k}{z}
//
func (self *Program) VEXTRACTF64X2(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VEXTRACTF64X2 imm8, ymm, xmm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[1]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x28)
            m.emit(0x19)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VEXTRACTF64X2 imm8, ymm, m128{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isM128kz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b01, ehcode(v[1]), addr(v[2]), 0, kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x19)
            m.mrsd(lcode(v[1]), addr(v[2]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VEXTRACTF64X2 imm8, zmm, xmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[1]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x48)
            m.emit(0x19)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VEXTRACTF64X2 imm8, zmm, m128{k}{z}
    if isImm8(v0) && isZMM(v1) && isM128kz(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b10, ehcode(v[1]), addr(v[2]), 0, kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x19)
            m.mrsd(lcode(v[1]), addr(v[2]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VEXTRACTF64X2")
    }
    return p
}

// VEXTRACTF64X4 performs "Extract 256 Bits of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VEXTRACTF64X4
// ISA extensions  : AVX512F
// Supported forms : (2 forms)
//
//    * VEXTRACTF64X4 imm8, zmm, ymm{k}{z}
//    * VEXTRACTF64X4 imm8, zmm, m256{k}{z}
//
func (self *Program) VEXTRACTF64X4(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VEXTRACTF64X4 imm8, zmm, ymm{k}{z}
    if isImm8(v0) && isZMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[1]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x48)
            m.emit(0x1b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VEXTRACTF64X4 imm8, zmm, m256{k}{z}
    if isImm8(v0) && isZMM(v1) && isM256kz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b10, ehcode(v[1]), addr(v[2]), 0, kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x1b)
            m.mrsd(lcode(v[1]), addr(v[2]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VEXTRACTF64X4")
    }
    return p
}

// VEXTRACTI128 performs "Extract Packed Integer Values".
//
// Mnemonic        : VEXTRACTI128
// ISA extensions  : AVX2
// Supported forms : (2 forms)
//
//    * VEXTRACTI128 imm8, ymm, xmm
//    * VEXTRACTI128 imm8, ymm, m128
//
func (self *Program) VEXTRACTI128(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VEXTRACTI128 imm8, ymm, xmm
    if isImm8(v0) && isYMM(v1) && isXMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[1]) << 7) ^ (hcode(v[2]) << 5))
            m.emit(0x7d)
            m.emit(0x39)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VEXTRACTI128 imm8, ymm, m128
    if isImm8(v0) && isYMM(v1) && isM128(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[1]), addr(v[2]), 0)
            m.emit(0x39)
            m.mrsd(lcode(v[1]), addr(v[2]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VEXTRACTI128")
    }
    return p
}

// VEXTRACTI32X4 performs "Extract 128 Bits of Packed Doubleword Integer Values".
//
// Mnemonic        : VEXTRACTI32X4
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (4 forms)
//
//    * VEXTRACTI32X4 imm8, ymm, xmm{k}{z}
//    * VEXTRACTI32X4 imm8, ymm, m128{k}{z}
//    * VEXTRACTI32X4 imm8, zmm, xmm{k}{z}
//    * VEXTRACTI32X4 imm8, zmm, m128{k}{z}
//
func (self *Program) VEXTRACTI32X4(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VEXTRACTI32X4 imm8, ymm, xmm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[1]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x28)
            m.emit(0x39)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VEXTRACTI32X4 imm8, ymm, m128{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isM128kz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b01, ehcode(v[1]), addr(v[2]), 0, kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x39)
            m.mrsd(lcode(v[1]), addr(v[2]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VEXTRACTI32X4 imm8, zmm, xmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[1]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x48)
            m.emit(0x39)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VEXTRACTI32X4 imm8, zmm, m128{k}{z}
    if isImm8(v0) && isZMM(v1) && isM128kz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b10, ehcode(v[1]), addr(v[2]), 0, kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x39)
            m.mrsd(lcode(v[1]), addr(v[2]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VEXTRACTI32X4")
    }
    return p
}

// VEXTRACTI32X8 performs "Extract 256 Bits of Packed Doubleword Integer Values".
//
// Mnemonic        : VEXTRACTI32X8
// ISA extensions  : AVX512DQ
// Supported forms : (2 forms)
//
//    * VEXTRACTI32X8 imm8, zmm, ymm{k}{z}
//    * VEXTRACTI32X8 imm8, zmm, m256{k}{z}
//
func (self *Program) VEXTRACTI32X8(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VEXTRACTI32X8 imm8, zmm, ymm{k}{z}
    if isImm8(v0) && isZMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[1]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x48)
            m.emit(0x3b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VEXTRACTI32X8 imm8, zmm, m256{k}{z}
    if isImm8(v0) && isZMM(v1) && isM256kz(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b10, ehcode(v[1]), addr(v[2]), 0, kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x3b)
            m.mrsd(lcode(v[1]), addr(v[2]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VEXTRACTI32X8")
    }
    return p
}

// VEXTRACTI64X2 performs "Extract 128 Bits of Packed Quadword Integer Values".
//
// Mnemonic        : VEXTRACTI64X2
// ISA extensions  : AVX512DQ, AVX512VL
// Supported forms : (4 forms)
//
//    * VEXTRACTI64X2 imm8, ymm, xmm{k}{z}
//    * VEXTRACTI64X2 imm8, ymm, m128{k}{z}
//    * VEXTRACTI64X2 imm8, zmm, xmm{k}{z}
//    * VEXTRACTI64X2 imm8, zmm, m128{k}{z}
//
func (self *Program) VEXTRACTI64X2(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VEXTRACTI64X2 imm8, ymm, xmm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[1]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x28)
            m.emit(0x39)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VEXTRACTI64X2 imm8, ymm, m128{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isM128kz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b01, ehcode(v[1]), addr(v[2]), 0, kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x39)
            m.mrsd(lcode(v[1]), addr(v[2]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VEXTRACTI64X2 imm8, zmm, xmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[1]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x48)
            m.emit(0x39)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VEXTRACTI64X2 imm8, zmm, m128{k}{z}
    if isImm8(v0) && isZMM(v1) && isM128kz(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b10, ehcode(v[1]), addr(v[2]), 0, kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x39)
            m.mrsd(lcode(v[1]), addr(v[2]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VEXTRACTI64X2")
    }
    return p
}

// VEXTRACTI64X4 performs "Extract 256 Bits of Packed Quadword Integer Values".
//
// Mnemonic        : VEXTRACTI64X4
// ISA extensions  : AVX512F
// Supported forms : (2 forms)
//
//    * VEXTRACTI64X4 imm8, zmm, ymm{k}{z}
//    * VEXTRACTI64X4 imm8, zmm, m256{k}{z}
//
func (self *Program) VEXTRACTI64X4(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VEXTRACTI64X4 imm8, zmm, ymm{k}{z}
    if isImm8(v0) && isZMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[1]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x48)
            m.emit(0x3b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VEXTRACTI64X4 imm8, zmm, m256{k}{z}
    if isImm8(v0) && isZMM(v1) && isM256kz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b10, ehcode(v[1]), addr(v[2]), 0, kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x3b)
            m.mrsd(lcode(v[1]), addr(v[2]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VEXTRACTI64X4")
    }
    return p
}

// VEXTRACTPS performs "Extract Packed Single Precision Floating-Point Value".
//
// Mnemonic        : VEXTRACTPS
// ISA extensions  : AVX, AVX512F
// Supported forms : (4 forms)
//
//    * VEXTRACTPS imm8, xmm, r32
//    * VEXTRACTPS imm8, xmm, r32
//    * VEXTRACTPS imm8, xmm, m32
//    * VEXTRACTPS imm8, xmm, m32
//
func (self *Program) VEXTRACTPS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VEXTRACTPS imm8, xmm, r32
    if isImm8(v0) && isXMM(v1) && isReg32(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[1]) << 7) ^ (hcode(v[2]) << 5))
            m.emit(0x79)
            m.emit(0x17)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VEXTRACTPS imm8, xmm, r32
    if isImm8(v0) && isEVEXXMM(v1) && isReg32(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[1]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit(0x08)
            m.emit(0x17)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VEXTRACTPS imm8, xmm, m32
    if isImm8(v0) && isXMM(v1) && isM32(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[1]), addr(v[2]), 0)
            m.emit(0x17)
            m.mrsd(lcode(v[1]), addr(v[2]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VEXTRACTPS imm8, xmm, m32
    if isImm8(v0) && isEVEXXMM(v1) && isM32(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b00, ehcode(v[1]), addr(v[2]), 0, 0, 0, 0)
            m.emit(0x17)
            m.mrsd(lcode(v[1]), addr(v[2]), 4)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VEXTRACTPS")
    }
    return p
}

// VFIXUPIMMPD performs "Fix Up Special Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VFIXUPIMMPD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (7 forms)
//
//    * VFIXUPIMMPD imm8, m128/m64bcst, xmm, xmm{k}{z}
//    * VFIXUPIMMPD imm8, xmm, xmm, xmm{k}{z}
//    * VFIXUPIMMPD imm8, m256/m64bcst, ymm, ymm{k}{z}
//    * VFIXUPIMMPD imm8, ymm, ymm, ymm{k}{z}
//    * VFIXUPIMMPD imm8, m512/m64bcst, zmm, zmm{k}{z}
//    * VFIXUPIMMPD imm8, {sae}, zmm, zmm, zmm{k}{z}
//    * VFIXUPIMMPD imm8, zmm, zmm, zmm{k}{z}
//
func (self *Program) VFIXUPIMMPD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(4, Operands{v0, v1, v2, v3})
        case 1  : p = self.alloc(5, Operands{v0, v1, v2, v3, vv[0]})
        default : panic("instruction VFIXUPIMMPD takes 4 or 5 operands")
    }
    // VFIXUPIMMPD imm8, m128/m64bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isM128M64bcst(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0x54)
            m.mrsd(lcode(v[3]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VFIXUPIMMPD imm8, xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x00)
            m.emit(0x54)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VFIXUPIMMPD imm8, m256/m64bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isM256M64bcst(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b01, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0x54)
            m.mrsd(lcode(v[3]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VFIXUPIMMPD imm8, ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isEVEXYMM(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x20)
            m.emit(0x54)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VFIXUPIMMPD imm8, m512/m64bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isM512M64bcst(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0x54)
            m.mrsd(lcode(v[3]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VFIXUPIMMPD imm8, {sae}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isImm8(v0) && isSAE(v1) && isZMM(v2) && isZMM(v3) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[4]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[4]) << 4)))
            m.emit(0xfd ^ (hlcode(v[3]) << 3))
            m.emit((zcode(v[4]) << 7) | (0x08 ^ (ecode(v[3]) << 3)) | kcode(v[4]) | 0x10)
            m.emit(0x54)
            m.emit(0xc0 | lcode(v[4]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VFIXUPIMMPD imm8, zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x54)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFIXUPIMMPD")
    }
    return p
}

// VFIXUPIMMPS performs "Fix Up Special Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VFIXUPIMMPS
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (7 forms)
//
//    * VFIXUPIMMPS imm8, m128/m32bcst, xmm, xmm{k}{z}
//    * VFIXUPIMMPS imm8, xmm, xmm, xmm{k}{z}
//    * VFIXUPIMMPS imm8, m256/m32bcst, ymm, ymm{k}{z}
//    * VFIXUPIMMPS imm8, ymm, ymm, ymm{k}{z}
//    * VFIXUPIMMPS imm8, m512/m32bcst, zmm, zmm{k}{z}
//    * VFIXUPIMMPS imm8, {sae}, zmm, zmm, zmm{k}{z}
//    * VFIXUPIMMPS imm8, zmm, zmm, zmm{k}{z}
//
func (self *Program) VFIXUPIMMPS(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(4, Operands{v0, v1, v2, v3})
        case 1  : p = self.alloc(5, Operands{v0, v1, v2, v3, vv[0]})
        default : panic("instruction VFIXUPIMMPS takes 4 or 5 operands")
    }
    // VFIXUPIMMPS imm8, m128/m32bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isM128M32bcst(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512VL)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0x54)
            m.mrsd(lcode(v[3]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VFIXUPIMMPS imm8, xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512VL)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x00)
            m.emit(0x54)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VFIXUPIMMPS imm8, m256/m32bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isM256M32bcst(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b01, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0x54)
            m.mrsd(lcode(v[3]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VFIXUPIMMPS imm8, ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isEVEXYMM(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x20)
            m.emit(0x54)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VFIXUPIMMPS imm8, m512/m32bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isM512M32bcst(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0x54)
            m.mrsd(lcode(v[3]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VFIXUPIMMPS imm8, {sae}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isImm8(v0) && isSAE(v1) && isZMM(v2) && isZMM(v3) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[4]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[4]) << 4)))
            m.emit(0x7d ^ (hlcode(v[3]) << 3))
            m.emit((zcode(v[4]) << 7) | (0x08 ^ (ecode(v[3]) << 3)) | kcode(v[4]) | 0x10)
            m.emit(0x54)
            m.emit(0xc0 | lcode(v[4]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VFIXUPIMMPS imm8, zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x54)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFIXUPIMMPS")
    }
    return p
}

// VFIXUPIMMSD performs "Fix Up Special Scalar Double-Precision Floating-Point Value".
//
// Mnemonic        : VFIXUPIMMSD
// ISA extensions  : AVX512F
// Supported forms : (3 forms)
//
//    * VFIXUPIMMSD imm8, m64, xmm, xmm{k}{z}
//    * VFIXUPIMMSD imm8, {sae}, xmm, xmm, xmm{k}{z}
//    * VFIXUPIMMSD imm8, xmm, xmm, xmm{k}{z}
//
func (self *Program) VFIXUPIMMSD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(4, Operands{v0, v1, v2, v3})
        case 1  : p = self.alloc(5, Operands{v0, v1, v2, v3, vv[0]})
        default : panic("instruction VFIXUPIMMSD takes 4 or 5 operands")
    }
    // VFIXUPIMMSD imm8, m64, xmm, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isM64(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), 0)
            m.emit(0x55)
            m.mrsd(lcode(v[3]), addr(v[1]), 8)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VFIXUPIMMSD imm8, {sae}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isImm8(v0) && isSAE(v1) && isEVEXXMM(v2) && isEVEXXMM(v3) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[4]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[4]) << 4)))
            m.emit(0xfd ^ (hlcode(v[3]) << 3))
            m.emit((zcode(v[4]) << 7) | (0x08 ^ (ecode(v[3]) << 3)) | kcode(v[4]) | 0x10)
            m.emit(0x55)
            m.emit(0xc0 | lcode(v[4]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VFIXUPIMMSD imm8, xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x55)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFIXUPIMMSD")
    }
    return p
}

// VFIXUPIMMSS performs "Fix Up Special Scalar Single-Precision Floating-Point Value".
//
// Mnemonic        : VFIXUPIMMSS
// ISA extensions  : AVX512F
// Supported forms : (3 forms)
//
//    * VFIXUPIMMSS imm8, m32, xmm, xmm{k}{z}
//    * VFIXUPIMMSS imm8, {sae}, xmm, xmm, xmm{k}{z}
//    * VFIXUPIMMSS imm8, xmm, xmm, xmm{k}{z}
//
func (self *Program) VFIXUPIMMSS(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(4, Operands{v0, v1, v2, v3})
        case 1  : p = self.alloc(5, Operands{v0, v1, v2, v3, vv[0]})
        default : panic("instruction VFIXUPIMMSS takes 4 or 5 operands")
    }
    // VFIXUPIMMSS imm8, m32, xmm, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isM32(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), 0)
            m.emit(0x55)
            m.mrsd(lcode(v[3]), addr(v[1]), 4)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VFIXUPIMMSS imm8, {sae}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isImm8(v0) && isSAE(v1) && isEVEXXMM(v2) && isEVEXXMM(v3) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[4]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[4]) << 4)))
            m.emit(0x7d ^ (hlcode(v[3]) << 3))
            m.emit((zcode(v[4]) << 7) | (0x08 ^ (ecode(v[3]) << 3)) | kcode(v[4]) | 0x10)
            m.emit(0x55)
            m.emit(0xc0 | lcode(v[4]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VFIXUPIMMSS imm8, xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x55)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFIXUPIMMSS")
    }
    return p
}

// VFMADD132PD performs "Fused Multiply-Add of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VFMADD132PD
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFMADD132PD m128/m64bcst, xmm, xmm{k}{z}
//    * VFMADD132PD xmm, xmm, xmm{k}{z}
//    * VFMADD132PD m256/m64bcst, ymm, ymm{k}{z}
//    * VFMADD132PD ymm, ymm, ymm{k}{z}
//    * VFMADD132PD m512/m64bcst, zmm, zmm{k}{z}
//    * VFMADD132PD xmm, xmm, xmm
//    * VFMADD132PD m128, xmm, xmm
//    * VFMADD132PD ymm, ymm, ymm
//    * VFMADD132PD m256, ymm, ymm
//    * VFMADD132PD {er}, zmm, zmm, zmm{k}{z}
//    * VFMADD132PD zmm, zmm, zmm{k}{z}
//
func (self *Program) VFMADD132PD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMADD132PD takes 3 or 4 operands")
    }
    // VFMADD132PD m128/m64bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x98)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFMADD132PD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x98)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADD132PD m256/m64bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x98)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFMADD132PD ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x98)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADD132PD m512/m64bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x98)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFMADD132PD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[1]) << 3))
            m.emit(0x98)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADD132PD m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x98)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMADD132PD ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit(0x98)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADD132PD m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x85, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x98)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMADD132PD {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x98)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMADD132PD zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x98)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMADD132PD")
    }
    return p
}

// VFMADD132PS performs "Fused Multiply-Add of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VFMADD132PS
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFMADD132PS m128/m32bcst, xmm, xmm{k}{z}
//    * VFMADD132PS xmm, xmm, xmm{k}{z}
//    * VFMADD132PS m256/m32bcst, ymm, ymm{k}{z}
//    * VFMADD132PS ymm, ymm, ymm{k}{z}
//    * VFMADD132PS m512/m32bcst, zmm, zmm{k}{z}
//    * VFMADD132PS xmm, xmm, xmm
//    * VFMADD132PS m128, xmm, xmm
//    * VFMADD132PS ymm, ymm, ymm
//    * VFMADD132PS m256, ymm, ymm
//    * VFMADD132PS {er}, zmm, zmm, zmm{k}{z}
//    * VFMADD132PS zmm, zmm, zmm{k}{z}
//
func (self *Program) VFMADD132PS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMADD132PS takes 3 or 4 operands")
    }
    // VFMADD132PS m128/m32bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x98)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFMADD132PS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x98)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADD132PS m256/m32bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x98)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFMADD132PS ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x98)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADD132PS m512/m32bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x98)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFMADD132PS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x98)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADD132PS m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x98)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMADD132PS ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x98)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADD132PS m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x98)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMADD132PS {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x98)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMADD132PS zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x98)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMADD132PS")
    }
    return p
}

// VFMADD132SD performs "Fused Multiply-Add of Scalar Double-Precision Floating-Point Values".
//
// Mnemonic        : VFMADD132SD
// ISA extensions  : AVX512F, FMA3
// Supported forms : (5 forms)
//
//    * VFMADD132SD m64, xmm, xmm{k}{z}
//    * VFMADD132SD xmm, xmm, xmm
//    * VFMADD132SD m64, xmm, xmm
//    * VFMADD132SD {er}, xmm, xmm, xmm{k}{z}
//    * VFMADD132SD xmm, xmm, xmm{k}{z}
//
func (self *Program) VFMADD132SD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMADD132SD takes 3 or 4 operands")
    }
    // VFMADD132SD m64, xmm, xmm{k}{z}
    if len(vv) == 0 && isM64(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x99)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    // VFMADD132SD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[1]) << 3))
            m.emit(0x99)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADD132SD m64, xmm, xmm
    if len(vv) == 0 && isM64(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x99)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMADD132SD {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x99)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMADD132SD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x99)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMADD132SD")
    }
    return p
}

// VFMADD132SS performs "Fused Multiply-Add of Scalar Single-Precision Floating-Point Values".
//
// Mnemonic        : VFMADD132SS
// ISA extensions  : AVX512F, FMA3
// Supported forms : (5 forms)
//
//    * VFMADD132SS m32, xmm, xmm{k}{z}
//    * VFMADD132SS xmm, xmm, xmm
//    * VFMADD132SS m32, xmm, xmm
//    * VFMADD132SS {er}, xmm, xmm, xmm{k}{z}
//    * VFMADD132SS xmm, xmm, xmm{k}{z}
//
func (self *Program) VFMADD132SS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMADD132SS takes 3 or 4 operands")
    }
    // VFMADD132SS m32, xmm, xmm{k}{z}
    if len(vv) == 0 && isM32(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x99)
            m.mrsd(lcode(v[2]), addr(v[0]), 4)
        })
    }
    // VFMADD132SS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x99)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADD132SS m32, xmm, xmm
    if len(vv) == 0 && isM32(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x99)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMADD132SS {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x99)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMADD132SS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x99)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMADD132SS")
    }
    return p
}

// VFMADD213PD performs "Fused Multiply-Add of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VFMADD213PD
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFMADD213PD m128/m64bcst, xmm, xmm{k}{z}
//    * VFMADD213PD xmm, xmm, xmm{k}{z}
//    * VFMADD213PD m256/m64bcst, ymm, ymm{k}{z}
//    * VFMADD213PD ymm, ymm, ymm{k}{z}
//    * VFMADD213PD m512/m64bcst, zmm, zmm{k}{z}
//    * VFMADD213PD xmm, xmm, xmm
//    * VFMADD213PD m128, xmm, xmm
//    * VFMADD213PD ymm, ymm, ymm
//    * VFMADD213PD m256, ymm, ymm
//    * VFMADD213PD {er}, zmm, zmm, zmm{k}{z}
//    * VFMADD213PD zmm, zmm, zmm{k}{z}
//
func (self *Program) VFMADD213PD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMADD213PD takes 3 or 4 operands")
    }
    // VFMADD213PD m128/m64bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xa8)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFMADD213PD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xa8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADD213PD m256/m64bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xa8)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFMADD213PD ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xa8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADD213PD m512/m64bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xa8)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFMADD213PD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[1]) << 3))
            m.emit(0xa8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADD213PD m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xa8)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMADD213PD ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit(0xa8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADD213PD m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x85, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xa8)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMADD213PD {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xa8)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMADD213PD zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xa8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMADD213PD")
    }
    return p
}

// VFMADD213PS performs "Fused Multiply-Add of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VFMADD213PS
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFMADD213PS m128/m32bcst, xmm, xmm{k}{z}
//    * VFMADD213PS xmm, xmm, xmm{k}{z}
//    * VFMADD213PS m256/m32bcst, ymm, ymm{k}{z}
//    * VFMADD213PS ymm, ymm, ymm{k}{z}
//    * VFMADD213PS m512/m32bcst, zmm, zmm{k}{z}
//    * VFMADD213PS xmm, xmm, xmm
//    * VFMADD213PS m128, xmm, xmm
//    * VFMADD213PS ymm, ymm, ymm
//    * VFMADD213PS m256, ymm, ymm
//    * VFMADD213PS {er}, zmm, zmm, zmm{k}{z}
//    * VFMADD213PS zmm, zmm, zmm{k}{z}
//
func (self *Program) VFMADD213PS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMADD213PS takes 3 or 4 operands")
    }
    // VFMADD213PS m128/m32bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xa8)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFMADD213PS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xa8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADD213PS m256/m32bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xa8)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFMADD213PS ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xa8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADD213PS m512/m32bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xa8)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFMADD213PS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0xa8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADD213PS m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xa8)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMADD213PS ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0xa8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADD213PS m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xa8)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMADD213PS {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xa8)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMADD213PS zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xa8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMADD213PS")
    }
    return p
}

// VFMADD213SD performs "Fused Multiply-Add of Scalar Double-Precision Floating-Point Values".
//
// Mnemonic        : VFMADD213SD
// ISA extensions  : AVX512F, FMA3
// Supported forms : (5 forms)
//
//    * VFMADD213SD m64, xmm, xmm{k}{z}
//    * VFMADD213SD xmm, xmm, xmm
//    * VFMADD213SD m64, xmm, xmm
//    * VFMADD213SD {er}, xmm, xmm, xmm{k}{z}
//    * VFMADD213SD xmm, xmm, xmm{k}{z}
//
func (self *Program) VFMADD213SD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMADD213SD takes 3 or 4 operands")
    }
    // VFMADD213SD m64, xmm, xmm{k}{z}
    if len(vv) == 0 && isM64(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xa9)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    // VFMADD213SD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[1]) << 3))
            m.emit(0xa9)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADD213SD m64, xmm, xmm
    if len(vv) == 0 && isM64(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xa9)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMADD213SD {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xa9)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMADD213SD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xa9)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMADD213SD")
    }
    return p
}

// VFMADD213SS performs "Fused Multiply-Add of Scalar Single-Precision Floating-Point Values".
//
// Mnemonic        : VFMADD213SS
// ISA extensions  : AVX512F, FMA3
// Supported forms : (5 forms)
//
//    * VFMADD213SS m32, xmm, xmm{k}{z}
//    * VFMADD213SS xmm, xmm, xmm
//    * VFMADD213SS m32, xmm, xmm
//    * VFMADD213SS {er}, xmm, xmm, xmm{k}{z}
//    * VFMADD213SS xmm, xmm, xmm{k}{z}
//
func (self *Program) VFMADD213SS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMADD213SS takes 3 or 4 operands")
    }
    // VFMADD213SS m32, xmm, xmm{k}{z}
    if len(vv) == 0 && isM32(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xa9)
            m.mrsd(lcode(v[2]), addr(v[0]), 4)
        })
    }
    // VFMADD213SS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0xa9)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADD213SS m32, xmm, xmm
    if len(vv) == 0 && isM32(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xa9)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMADD213SS {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xa9)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMADD213SS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xa9)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMADD213SS")
    }
    return p
}

// VFMADD231PD performs "Fused Multiply-Add of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VFMADD231PD
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFMADD231PD m128/m64bcst, xmm, xmm{k}{z}
//    * VFMADD231PD xmm, xmm, xmm{k}{z}
//    * VFMADD231PD m256/m64bcst, ymm, ymm{k}{z}
//    * VFMADD231PD ymm, ymm, ymm{k}{z}
//    * VFMADD231PD m512/m64bcst, zmm, zmm{k}{z}
//    * VFMADD231PD xmm, xmm, xmm
//    * VFMADD231PD m128, xmm, xmm
//    * VFMADD231PD ymm, ymm, ymm
//    * VFMADD231PD m256, ymm, ymm
//    * VFMADD231PD {er}, zmm, zmm, zmm{k}{z}
//    * VFMADD231PD zmm, zmm, zmm{k}{z}
//
func (self *Program) VFMADD231PD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMADD231PD takes 3 or 4 operands")
    }
    // VFMADD231PD m128/m64bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xb8)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFMADD231PD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xb8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADD231PD m256/m64bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xb8)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFMADD231PD ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xb8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADD231PD m512/m64bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xb8)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFMADD231PD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[1]) << 3))
            m.emit(0xb8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADD231PD m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xb8)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMADD231PD ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit(0xb8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADD231PD m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x85, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xb8)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMADD231PD {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xb8)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMADD231PD zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xb8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMADD231PD")
    }
    return p
}

// VFMADD231PS performs "Fused Multiply-Add of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VFMADD231PS
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFMADD231PS m128/m32bcst, xmm, xmm{k}{z}
//    * VFMADD231PS xmm, xmm, xmm{k}{z}
//    * VFMADD231PS m256/m32bcst, ymm, ymm{k}{z}
//    * VFMADD231PS ymm, ymm, ymm{k}{z}
//    * VFMADD231PS m512/m32bcst, zmm, zmm{k}{z}
//    * VFMADD231PS xmm, xmm, xmm
//    * VFMADD231PS m128, xmm, xmm
//    * VFMADD231PS ymm, ymm, ymm
//    * VFMADD231PS m256, ymm, ymm
//    * VFMADD231PS {er}, zmm, zmm, zmm{k}{z}
//    * VFMADD231PS zmm, zmm, zmm{k}{z}
//
func (self *Program) VFMADD231PS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMADD231PS takes 3 or 4 operands")
    }
    // VFMADD231PS m128/m32bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xb8)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFMADD231PS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xb8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADD231PS m256/m32bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xb8)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFMADD231PS ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xb8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADD231PS m512/m32bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xb8)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFMADD231PS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0xb8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADD231PS m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xb8)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMADD231PS ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0xb8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADD231PS m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xb8)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMADD231PS {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xb8)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMADD231PS zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xb8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMADD231PS")
    }
    return p
}

// VFMADD231SD performs "Fused Multiply-Add of Scalar Double-Precision Floating-Point Values".
//
// Mnemonic        : VFMADD231SD
// ISA extensions  : AVX512F, FMA3
// Supported forms : (5 forms)
//
//    * VFMADD231SD m64, xmm, xmm{k}{z}
//    * VFMADD231SD xmm, xmm, xmm
//    * VFMADD231SD m64, xmm, xmm
//    * VFMADD231SD {er}, xmm, xmm, xmm{k}{z}
//    * VFMADD231SD xmm, xmm, xmm{k}{z}
//
func (self *Program) VFMADD231SD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMADD231SD takes 3 or 4 operands")
    }
    // VFMADD231SD m64, xmm, xmm{k}{z}
    if len(vv) == 0 && isM64(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xb9)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    // VFMADD231SD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[1]) << 3))
            m.emit(0xb9)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADD231SD m64, xmm, xmm
    if len(vv) == 0 && isM64(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xb9)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMADD231SD {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xb9)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMADD231SD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xb9)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMADD231SD")
    }
    return p
}

// VFMADD231SS performs "Fused Multiply-Add of Scalar Single-Precision Floating-Point Values".
//
// Mnemonic        : VFMADD231SS
// ISA extensions  : AVX512F, FMA3
// Supported forms : (5 forms)
//
//    * VFMADD231SS m32, xmm, xmm{k}{z}
//    * VFMADD231SS xmm, xmm, xmm
//    * VFMADD231SS m32, xmm, xmm
//    * VFMADD231SS {er}, xmm, xmm, xmm{k}{z}
//    * VFMADD231SS xmm, xmm, xmm{k}{z}
//
func (self *Program) VFMADD231SS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMADD231SS takes 3 or 4 operands")
    }
    // VFMADD231SS m32, xmm, xmm{k}{z}
    if len(vv) == 0 && isM32(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xb9)
            m.mrsd(lcode(v[2]), addr(v[0]), 4)
        })
    }
    // VFMADD231SS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0xb9)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADD231SS m32, xmm, xmm
    if len(vv) == 0 && isM32(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xb9)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMADD231SS {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xb9)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMADD231SS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xb9)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMADD231SS")
    }
    return p
}

// VFMADDPD performs "Fused Multiply-Add of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VFMADDPD
// ISA extensions  : FMA4
// Supported forms : (6 forms)
//
//    * VFMADDPD xmm, xmm, xmm, xmm
//    * VFMADDPD m128, xmm, xmm, xmm
//    * VFMADDPD xmm, m128, xmm, xmm
//    * VFMADDPD ymm, ymm, ymm, ymm
//    * VFMADDPD m256, ymm, ymm, ymm
//    * VFMADDPD ymm, m256, ymm, ymm
//
func (self *Program) VFMADDPD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VFMADDPD xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[2]) << 3))
            m.emit(0x69)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x69)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFMADDPD m128, xmm, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x81, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0x69)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VFMADDPD xmm, m128, xmm, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x69)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFMADDPD ymm, ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit(0x69)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit(0x69)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFMADDPD m256, ymm, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x85, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0x69)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VFMADDPD ymm, m256, ymm, ymm
    if isYMM(v0) && isM256(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x69)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMADDPD")
    }
    return p
}

// VFMADDPS performs "Fused Multiply-Add of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VFMADDPS
// ISA extensions  : FMA4
// Supported forms : (6 forms)
//
//    * VFMADDPS xmm, xmm, xmm, xmm
//    * VFMADDPS m128, xmm, xmm, xmm
//    * VFMADDPS xmm, m128, xmm, xmm
//    * VFMADDPS ymm, ymm, ymm, ymm
//    * VFMADDPS m256, ymm, ymm, ymm
//    * VFMADDPS ymm, m256, ymm, ymm
//
func (self *Program) VFMADDPS(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VFMADDPS xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[2]) << 3))
            m.emit(0x68)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x68)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFMADDPS m128, xmm, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x81, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0x68)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VFMADDPS xmm, m128, xmm, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x68)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFMADDPS ymm, ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit(0x68)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit(0x68)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFMADDPS m256, ymm, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x85, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0x68)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VFMADDPS ymm, m256, ymm, ymm
    if isYMM(v0) && isM256(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x68)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMADDPS")
    }
    return p
}

// VFMADDSD performs "Fused Multiply-Add of Scalar Double-Precision Floating-Point Values".
//
// Mnemonic        : VFMADDSD
// ISA extensions  : FMA4
// Supported forms : (3 forms)
//
//    * VFMADDSD xmm, xmm, xmm, xmm
//    * VFMADDSD m64, xmm, xmm, xmm
//    * VFMADDSD xmm, m64, xmm, xmm
//
func (self *Program) VFMADDSD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VFMADDSD xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[2]) << 3))
            m.emit(0x6b)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x6b)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFMADDSD m64, xmm, xmm, xmm
    if isM64(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x81, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0x6b)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VFMADDSD xmm, m64, xmm, xmm
    if isXMM(v0) && isM64(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x6b)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMADDSD")
    }
    return p
}

// VFMADDSS performs "Fused Multiply-Add of Scalar Single-Precision Floating-Point Values".
//
// Mnemonic        : VFMADDSS
// ISA extensions  : FMA4
// Supported forms : (3 forms)
//
//    * VFMADDSS xmm, xmm, xmm, xmm
//    * VFMADDSS m32, xmm, xmm, xmm
//    * VFMADDSS xmm, m32, xmm, xmm
//
func (self *Program) VFMADDSS(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VFMADDSS xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[2]) << 3))
            m.emit(0x6a)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x6a)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFMADDSS m32, xmm, xmm, xmm
    if isM32(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x81, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0x6a)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VFMADDSS xmm, m32, xmm, xmm
    if isXMM(v0) && isM32(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x6a)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMADDSS")
    }
    return p
}

// VFMADDSUB132PD performs "Fused Multiply-Alternating Add/Subtract of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VFMADDSUB132PD
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFMADDSUB132PD m128/m64bcst, xmm, xmm{k}{z}
//    * VFMADDSUB132PD xmm, xmm, xmm{k}{z}
//    * VFMADDSUB132PD m256/m64bcst, ymm, ymm{k}{z}
//    * VFMADDSUB132PD ymm, ymm, ymm{k}{z}
//    * VFMADDSUB132PD m512/m64bcst, zmm, zmm{k}{z}
//    * VFMADDSUB132PD xmm, xmm, xmm
//    * VFMADDSUB132PD m128, xmm, xmm
//    * VFMADDSUB132PD ymm, ymm, ymm
//    * VFMADDSUB132PD m256, ymm, ymm
//    * VFMADDSUB132PD {er}, zmm, zmm, zmm{k}{z}
//    * VFMADDSUB132PD zmm, zmm, zmm{k}{z}
//
func (self *Program) VFMADDSUB132PD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMADDSUB132PD takes 3 or 4 operands")
    }
    // VFMADDSUB132PD m128/m64bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x96)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFMADDSUB132PD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x96)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADDSUB132PD m256/m64bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x96)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFMADDSUB132PD ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x96)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADDSUB132PD m512/m64bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x96)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFMADDSUB132PD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[1]) << 3))
            m.emit(0x96)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADDSUB132PD m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x96)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMADDSUB132PD ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit(0x96)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADDSUB132PD m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x85, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x96)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMADDSUB132PD {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x96)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMADDSUB132PD zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x96)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMADDSUB132PD")
    }
    return p
}

// VFMADDSUB132PS performs "Fused Multiply-Alternating Add/Subtract of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VFMADDSUB132PS
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFMADDSUB132PS m128/m32bcst, xmm, xmm{k}{z}
//    * VFMADDSUB132PS xmm, xmm, xmm{k}{z}
//    * VFMADDSUB132PS m256/m32bcst, ymm, ymm{k}{z}
//    * VFMADDSUB132PS ymm, ymm, ymm{k}{z}
//    * VFMADDSUB132PS m512/m32bcst, zmm, zmm{k}{z}
//    * VFMADDSUB132PS xmm, xmm, xmm
//    * VFMADDSUB132PS m128, xmm, xmm
//    * VFMADDSUB132PS ymm, ymm, ymm
//    * VFMADDSUB132PS m256, ymm, ymm
//    * VFMADDSUB132PS {er}, zmm, zmm, zmm{k}{z}
//    * VFMADDSUB132PS zmm, zmm, zmm{k}{z}
//
func (self *Program) VFMADDSUB132PS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMADDSUB132PS takes 3 or 4 operands")
    }
    // VFMADDSUB132PS m128/m32bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x96)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFMADDSUB132PS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x96)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADDSUB132PS m256/m32bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x96)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFMADDSUB132PS ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x96)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADDSUB132PS m512/m32bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x96)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFMADDSUB132PS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x96)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADDSUB132PS m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x96)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMADDSUB132PS ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x96)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADDSUB132PS m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x96)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMADDSUB132PS {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x96)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMADDSUB132PS zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x96)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMADDSUB132PS")
    }
    return p
}

// VFMADDSUB213PD performs "Fused Multiply-Alternating Add/Subtract of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VFMADDSUB213PD
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFMADDSUB213PD m128/m64bcst, xmm, xmm{k}{z}
//    * VFMADDSUB213PD xmm, xmm, xmm{k}{z}
//    * VFMADDSUB213PD m256/m64bcst, ymm, ymm{k}{z}
//    * VFMADDSUB213PD ymm, ymm, ymm{k}{z}
//    * VFMADDSUB213PD m512/m64bcst, zmm, zmm{k}{z}
//    * VFMADDSUB213PD xmm, xmm, xmm
//    * VFMADDSUB213PD m128, xmm, xmm
//    * VFMADDSUB213PD ymm, ymm, ymm
//    * VFMADDSUB213PD m256, ymm, ymm
//    * VFMADDSUB213PD {er}, zmm, zmm, zmm{k}{z}
//    * VFMADDSUB213PD zmm, zmm, zmm{k}{z}
//
func (self *Program) VFMADDSUB213PD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMADDSUB213PD takes 3 or 4 operands")
    }
    // VFMADDSUB213PD m128/m64bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xa6)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFMADDSUB213PD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xa6)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADDSUB213PD m256/m64bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xa6)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFMADDSUB213PD ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xa6)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADDSUB213PD m512/m64bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xa6)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFMADDSUB213PD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[1]) << 3))
            m.emit(0xa6)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADDSUB213PD m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xa6)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMADDSUB213PD ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit(0xa6)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADDSUB213PD m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x85, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xa6)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMADDSUB213PD {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xa6)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMADDSUB213PD zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xa6)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMADDSUB213PD")
    }
    return p
}

// VFMADDSUB213PS performs "Fused Multiply-Alternating Add/Subtract of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VFMADDSUB213PS
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFMADDSUB213PS m128/m32bcst, xmm, xmm{k}{z}
//    * VFMADDSUB213PS xmm, xmm, xmm{k}{z}
//    * VFMADDSUB213PS m256/m32bcst, ymm, ymm{k}{z}
//    * VFMADDSUB213PS ymm, ymm, ymm{k}{z}
//    * VFMADDSUB213PS m512/m32bcst, zmm, zmm{k}{z}
//    * VFMADDSUB213PS xmm, xmm, xmm
//    * VFMADDSUB213PS m128, xmm, xmm
//    * VFMADDSUB213PS ymm, ymm, ymm
//    * VFMADDSUB213PS m256, ymm, ymm
//    * VFMADDSUB213PS {er}, zmm, zmm, zmm{k}{z}
//    * VFMADDSUB213PS zmm, zmm, zmm{k}{z}
//
func (self *Program) VFMADDSUB213PS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMADDSUB213PS takes 3 or 4 operands")
    }
    // VFMADDSUB213PS m128/m32bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xa6)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFMADDSUB213PS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xa6)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADDSUB213PS m256/m32bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xa6)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFMADDSUB213PS ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xa6)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADDSUB213PS m512/m32bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xa6)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFMADDSUB213PS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0xa6)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADDSUB213PS m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xa6)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMADDSUB213PS ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0xa6)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADDSUB213PS m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xa6)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMADDSUB213PS {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xa6)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMADDSUB213PS zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xa6)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMADDSUB213PS")
    }
    return p
}

// VFMADDSUB231PD performs "Fused Multiply-Alternating Add/Subtract of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VFMADDSUB231PD
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFMADDSUB231PD m128/m64bcst, xmm, xmm{k}{z}
//    * VFMADDSUB231PD xmm, xmm, xmm{k}{z}
//    * VFMADDSUB231PD m256/m64bcst, ymm, ymm{k}{z}
//    * VFMADDSUB231PD ymm, ymm, ymm{k}{z}
//    * VFMADDSUB231PD m512/m64bcst, zmm, zmm{k}{z}
//    * VFMADDSUB231PD xmm, xmm, xmm
//    * VFMADDSUB231PD m128, xmm, xmm
//    * VFMADDSUB231PD ymm, ymm, ymm
//    * VFMADDSUB231PD m256, ymm, ymm
//    * VFMADDSUB231PD {er}, zmm, zmm, zmm{k}{z}
//    * VFMADDSUB231PD zmm, zmm, zmm{k}{z}
//
func (self *Program) VFMADDSUB231PD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMADDSUB231PD takes 3 or 4 operands")
    }
    // VFMADDSUB231PD m128/m64bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xb6)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFMADDSUB231PD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xb6)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADDSUB231PD m256/m64bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xb6)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFMADDSUB231PD ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xb6)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADDSUB231PD m512/m64bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xb6)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFMADDSUB231PD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[1]) << 3))
            m.emit(0xb6)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADDSUB231PD m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xb6)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMADDSUB231PD ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit(0xb6)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADDSUB231PD m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x85, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xb6)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMADDSUB231PD {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xb6)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMADDSUB231PD zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xb6)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMADDSUB231PD")
    }
    return p
}

// VFMADDSUB231PS performs "Fused Multiply-Alternating Add/Subtract of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VFMADDSUB231PS
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFMADDSUB231PS m128/m32bcst, xmm, xmm{k}{z}
//    * VFMADDSUB231PS xmm, xmm, xmm{k}{z}
//    * VFMADDSUB231PS m256/m32bcst, ymm, ymm{k}{z}
//    * VFMADDSUB231PS ymm, ymm, ymm{k}{z}
//    * VFMADDSUB231PS m512/m32bcst, zmm, zmm{k}{z}
//    * VFMADDSUB231PS xmm, xmm, xmm
//    * VFMADDSUB231PS m128, xmm, xmm
//    * VFMADDSUB231PS ymm, ymm, ymm
//    * VFMADDSUB231PS m256, ymm, ymm
//    * VFMADDSUB231PS {er}, zmm, zmm, zmm{k}{z}
//    * VFMADDSUB231PS zmm, zmm, zmm{k}{z}
//
func (self *Program) VFMADDSUB231PS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMADDSUB231PS takes 3 or 4 operands")
    }
    // VFMADDSUB231PS m128/m32bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xb6)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFMADDSUB231PS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xb6)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADDSUB231PS m256/m32bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xb6)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFMADDSUB231PS ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xb6)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADDSUB231PS m512/m32bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xb6)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFMADDSUB231PS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0xb6)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADDSUB231PS m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xb6)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMADDSUB231PS ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0xb6)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMADDSUB231PS m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xb6)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMADDSUB231PS {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xb6)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMADDSUB231PS zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xb6)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMADDSUB231PS")
    }
    return p
}

// VFMADDSUBPD performs "Fused Multiply-Alternating Add/Subtract of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VFMADDSUBPD
// ISA extensions  : FMA4
// Supported forms : (6 forms)
//
//    * VFMADDSUBPD xmm, xmm, xmm, xmm
//    * VFMADDSUBPD m128, xmm, xmm, xmm
//    * VFMADDSUBPD xmm, m128, xmm, xmm
//    * VFMADDSUBPD ymm, ymm, ymm, ymm
//    * VFMADDSUBPD m256, ymm, ymm, ymm
//    * VFMADDSUBPD ymm, m256, ymm, ymm
//
func (self *Program) VFMADDSUBPD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VFMADDSUBPD xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[2]) << 3))
            m.emit(0x5d)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x5d)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFMADDSUBPD m128, xmm, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x81, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0x5d)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VFMADDSUBPD xmm, m128, xmm, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x5d)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFMADDSUBPD ymm, ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit(0x5d)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit(0x5d)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFMADDSUBPD m256, ymm, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x85, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0x5d)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VFMADDSUBPD ymm, m256, ymm, ymm
    if isYMM(v0) && isM256(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x5d)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMADDSUBPD")
    }
    return p
}

// VFMADDSUBPS performs "Fused Multiply-Alternating Add/Subtract of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VFMADDSUBPS
// ISA extensions  : FMA4
// Supported forms : (6 forms)
//
//    * VFMADDSUBPS xmm, xmm, xmm, xmm
//    * VFMADDSUBPS m128, xmm, xmm, xmm
//    * VFMADDSUBPS xmm, m128, xmm, xmm
//    * VFMADDSUBPS ymm, ymm, ymm, ymm
//    * VFMADDSUBPS m256, ymm, ymm, ymm
//    * VFMADDSUBPS ymm, m256, ymm, ymm
//
func (self *Program) VFMADDSUBPS(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VFMADDSUBPS xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[2]) << 3))
            m.emit(0x5c)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x5c)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFMADDSUBPS m128, xmm, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x81, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0x5c)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VFMADDSUBPS xmm, m128, xmm, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x5c)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFMADDSUBPS ymm, ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit(0x5c)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit(0x5c)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFMADDSUBPS m256, ymm, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x85, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0x5c)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VFMADDSUBPS ymm, m256, ymm, ymm
    if isYMM(v0) && isM256(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x5c)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMADDSUBPS")
    }
    return p
}

// VFMSUB132PD performs "Fused Multiply-Subtract of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VFMSUB132PD
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFMSUB132PD m128/m64bcst, xmm, xmm{k}{z}
//    * VFMSUB132PD xmm, xmm, xmm{k}{z}
//    * VFMSUB132PD m256/m64bcst, ymm, ymm{k}{z}
//    * VFMSUB132PD ymm, ymm, ymm{k}{z}
//    * VFMSUB132PD m512/m64bcst, zmm, zmm{k}{z}
//    * VFMSUB132PD xmm, xmm, xmm
//    * VFMSUB132PD m128, xmm, xmm
//    * VFMSUB132PD ymm, ymm, ymm
//    * VFMSUB132PD m256, ymm, ymm
//    * VFMSUB132PD {er}, zmm, zmm, zmm{k}{z}
//    * VFMSUB132PD zmm, zmm, zmm{k}{z}
//
func (self *Program) VFMSUB132PD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMSUB132PD takes 3 or 4 operands")
    }
    // VFMSUB132PD m128/m64bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x9a)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFMSUB132PD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x9a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUB132PD m256/m64bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x9a)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFMSUB132PD ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x9a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUB132PD m512/m64bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x9a)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFMSUB132PD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[1]) << 3))
            m.emit(0x9a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUB132PD m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x9a)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMSUB132PD ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit(0x9a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUB132PD m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x85, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x9a)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMSUB132PD {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x9a)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMSUB132PD zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x9a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMSUB132PD")
    }
    return p
}

// VFMSUB132PS performs "Fused Multiply-Subtract of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VFMSUB132PS
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFMSUB132PS m128/m32bcst, xmm, xmm{k}{z}
//    * VFMSUB132PS xmm, xmm, xmm{k}{z}
//    * VFMSUB132PS m256/m32bcst, ymm, ymm{k}{z}
//    * VFMSUB132PS ymm, ymm, ymm{k}{z}
//    * VFMSUB132PS m512/m32bcst, zmm, zmm{k}{z}
//    * VFMSUB132PS xmm, xmm, xmm
//    * VFMSUB132PS m128, xmm, xmm
//    * VFMSUB132PS ymm, ymm, ymm
//    * VFMSUB132PS m256, ymm, ymm
//    * VFMSUB132PS {er}, zmm, zmm, zmm{k}{z}
//    * VFMSUB132PS zmm, zmm, zmm{k}{z}
//
func (self *Program) VFMSUB132PS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMSUB132PS takes 3 or 4 operands")
    }
    // VFMSUB132PS m128/m32bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x9a)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFMSUB132PS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x9a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUB132PS m256/m32bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x9a)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFMSUB132PS ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x9a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUB132PS m512/m32bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x9a)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFMSUB132PS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x9a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUB132PS m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x9a)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMSUB132PS ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x9a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUB132PS m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x9a)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMSUB132PS {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x9a)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMSUB132PS zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x9a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMSUB132PS")
    }
    return p
}

// VFMSUB132SD performs "Fused Multiply-Subtract of Scalar Double-Precision Floating-Point Values".
//
// Mnemonic        : VFMSUB132SD
// ISA extensions  : AVX512F, FMA3
// Supported forms : (5 forms)
//
//    * VFMSUB132SD m64, xmm, xmm{k}{z}
//    * VFMSUB132SD xmm, xmm, xmm
//    * VFMSUB132SD m64, xmm, xmm
//    * VFMSUB132SD {er}, xmm, xmm, xmm{k}{z}
//    * VFMSUB132SD xmm, xmm, xmm{k}{z}
//
func (self *Program) VFMSUB132SD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMSUB132SD takes 3 or 4 operands")
    }
    // VFMSUB132SD m64, xmm, xmm{k}{z}
    if len(vv) == 0 && isM64(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x9b)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    // VFMSUB132SD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[1]) << 3))
            m.emit(0x9b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUB132SD m64, xmm, xmm
    if len(vv) == 0 && isM64(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x9b)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMSUB132SD {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x9b)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMSUB132SD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x9b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMSUB132SD")
    }
    return p
}

// VFMSUB132SS performs "Fused Multiply-Subtract of Scalar Single-Precision Floating-Point Values".
//
// Mnemonic        : VFMSUB132SS
// ISA extensions  : AVX512F, FMA3
// Supported forms : (5 forms)
//
//    * VFMSUB132SS m32, xmm, xmm{k}{z}
//    * VFMSUB132SS xmm, xmm, xmm
//    * VFMSUB132SS m32, xmm, xmm
//    * VFMSUB132SS {er}, xmm, xmm, xmm{k}{z}
//    * VFMSUB132SS xmm, xmm, xmm{k}{z}
//
func (self *Program) VFMSUB132SS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMSUB132SS takes 3 or 4 operands")
    }
    // VFMSUB132SS m32, xmm, xmm{k}{z}
    if len(vv) == 0 && isM32(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x9b)
            m.mrsd(lcode(v[2]), addr(v[0]), 4)
        })
    }
    // VFMSUB132SS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x9b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUB132SS m32, xmm, xmm
    if len(vv) == 0 && isM32(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x9b)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMSUB132SS {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x9b)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMSUB132SS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x9b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMSUB132SS")
    }
    return p
}

// VFMSUB213PD performs "Fused Multiply-Subtract of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VFMSUB213PD
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFMSUB213PD m128/m64bcst, xmm, xmm{k}{z}
//    * VFMSUB213PD xmm, xmm, xmm{k}{z}
//    * VFMSUB213PD m256/m64bcst, ymm, ymm{k}{z}
//    * VFMSUB213PD ymm, ymm, ymm{k}{z}
//    * VFMSUB213PD m512/m64bcst, zmm, zmm{k}{z}
//    * VFMSUB213PD xmm, xmm, xmm
//    * VFMSUB213PD m128, xmm, xmm
//    * VFMSUB213PD ymm, ymm, ymm
//    * VFMSUB213PD m256, ymm, ymm
//    * VFMSUB213PD {er}, zmm, zmm, zmm{k}{z}
//    * VFMSUB213PD zmm, zmm, zmm{k}{z}
//
func (self *Program) VFMSUB213PD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMSUB213PD takes 3 or 4 operands")
    }
    // VFMSUB213PD m128/m64bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xaa)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFMSUB213PD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xaa)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUB213PD m256/m64bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xaa)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFMSUB213PD ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xaa)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUB213PD m512/m64bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xaa)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFMSUB213PD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[1]) << 3))
            m.emit(0xaa)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUB213PD m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xaa)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMSUB213PD ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit(0xaa)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUB213PD m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x85, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xaa)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMSUB213PD {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xaa)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMSUB213PD zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xaa)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMSUB213PD")
    }
    return p
}

// VFMSUB213PS performs "Fused Multiply-Subtract of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VFMSUB213PS
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFMSUB213PS m128/m32bcst, xmm, xmm{k}{z}
//    * VFMSUB213PS xmm, xmm, xmm{k}{z}
//    * VFMSUB213PS m256/m32bcst, ymm, ymm{k}{z}
//    * VFMSUB213PS ymm, ymm, ymm{k}{z}
//    * VFMSUB213PS m512/m32bcst, zmm, zmm{k}{z}
//    * VFMSUB213PS xmm, xmm, xmm
//    * VFMSUB213PS m128, xmm, xmm
//    * VFMSUB213PS ymm, ymm, ymm
//    * VFMSUB213PS m256, ymm, ymm
//    * VFMSUB213PS {er}, zmm, zmm, zmm{k}{z}
//    * VFMSUB213PS zmm, zmm, zmm{k}{z}
//
func (self *Program) VFMSUB213PS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMSUB213PS takes 3 or 4 operands")
    }
    // VFMSUB213PS m128/m32bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xaa)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFMSUB213PS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xaa)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUB213PS m256/m32bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xaa)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFMSUB213PS ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xaa)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUB213PS m512/m32bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xaa)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFMSUB213PS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0xaa)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUB213PS m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xaa)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMSUB213PS ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0xaa)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUB213PS m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xaa)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMSUB213PS {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xaa)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMSUB213PS zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xaa)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMSUB213PS")
    }
    return p
}

// VFMSUB213SD performs "Fused Multiply-Subtract of Scalar Double-Precision Floating-Point Values".
//
// Mnemonic        : VFMSUB213SD
// ISA extensions  : AVX512F, FMA3
// Supported forms : (5 forms)
//
//    * VFMSUB213SD m64, xmm, xmm{k}{z}
//    * VFMSUB213SD xmm, xmm, xmm
//    * VFMSUB213SD m64, xmm, xmm
//    * VFMSUB213SD {er}, xmm, xmm, xmm{k}{z}
//    * VFMSUB213SD xmm, xmm, xmm{k}{z}
//
func (self *Program) VFMSUB213SD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMSUB213SD takes 3 or 4 operands")
    }
    // VFMSUB213SD m64, xmm, xmm{k}{z}
    if len(vv) == 0 && isM64(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xab)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    // VFMSUB213SD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[1]) << 3))
            m.emit(0xab)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUB213SD m64, xmm, xmm
    if len(vv) == 0 && isM64(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xab)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMSUB213SD {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xab)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMSUB213SD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xab)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMSUB213SD")
    }
    return p
}

// VFMSUB213SS performs "Fused Multiply-Subtract of Scalar Single-Precision Floating-Point Values".
//
// Mnemonic        : VFMSUB213SS
// ISA extensions  : AVX512F, FMA3
// Supported forms : (5 forms)
//
//    * VFMSUB213SS m32, xmm, xmm{k}{z}
//    * VFMSUB213SS xmm, xmm, xmm
//    * VFMSUB213SS m32, xmm, xmm
//    * VFMSUB213SS {er}, xmm, xmm, xmm{k}{z}
//    * VFMSUB213SS xmm, xmm, xmm{k}{z}
//
func (self *Program) VFMSUB213SS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMSUB213SS takes 3 or 4 operands")
    }
    // VFMSUB213SS m32, xmm, xmm{k}{z}
    if len(vv) == 0 && isM32(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xab)
            m.mrsd(lcode(v[2]), addr(v[0]), 4)
        })
    }
    // VFMSUB213SS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0xab)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUB213SS m32, xmm, xmm
    if len(vv) == 0 && isM32(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xab)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMSUB213SS {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xab)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMSUB213SS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xab)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMSUB213SS")
    }
    return p
}

// VFMSUB231PD performs "Fused Multiply-Subtract of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VFMSUB231PD
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFMSUB231PD m128/m64bcst, xmm, xmm{k}{z}
//    * VFMSUB231PD xmm, xmm, xmm{k}{z}
//    * VFMSUB231PD m256/m64bcst, ymm, ymm{k}{z}
//    * VFMSUB231PD ymm, ymm, ymm{k}{z}
//    * VFMSUB231PD m512/m64bcst, zmm, zmm{k}{z}
//    * VFMSUB231PD xmm, xmm, xmm
//    * VFMSUB231PD m128, xmm, xmm
//    * VFMSUB231PD ymm, ymm, ymm
//    * VFMSUB231PD m256, ymm, ymm
//    * VFMSUB231PD {er}, zmm, zmm, zmm{k}{z}
//    * VFMSUB231PD zmm, zmm, zmm{k}{z}
//
func (self *Program) VFMSUB231PD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMSUB231PD takes 3 or 4 operands")
    }
    // VFMSUB231PD m128/m64bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xba)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFMSUB231PD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xba)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUB231PD m256/m64bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xba)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFMSUB231PD ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xba)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUB231PD m512/m64bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xba)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFMSUB231PD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[1]) << 3))
            m.emit(0xba)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUB231PD m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xba)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMSUB231PD ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit(0xba)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUB231PD m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x85, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xba)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMSUB231PD {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xba)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMSUB231PD zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xba)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMSUB231PD")
    }
    return p
}

// VFMSUB231PS performs "Fused Multiply-Subtract of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VFMSUB231PS
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFMSUB231PS m128/m32bcst, xmm, xmm{k}{z}
//    * VFMSUB231PS xmm, xmm, xmm{k}{z}
//    * VFMSUB231PS m256/m32bcst, ymm, ymm{k}{z}
//    * VFMSUB231PS ymm, ymm, ymm{k}{z}
//    * VFMSUB231PS m512/m32bcst, zmm, zmm{k}{z}
//    * VFMSUB231PS xmm, xmm, xmm
//    * VFMSUB231PS m128, xmm, xmm
//    * VFMSUB231PS ymm, ymm, ymm
//    * VFMSUB231PS m256, ymm, ymm
//    * VFMSUB231PS {er}, zmm, zmm, zmm{k}{z}
//    * VFMSUB231PS zmm, zmm, zmm{k}{z}
//
func (self *Program) VFMSUB231PS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMSUB231PS takes 3 or 4 operands")
    }
    // VFMSUB231PS m128/m32bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xba)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFMSUB231PS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xba)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUB231PS m256/m32bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xba)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFMSUB231PS ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xba)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUB231PS m512/m32bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xba)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFMSUB231PS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0xba)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUB231PS m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xba)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMSUB231PS ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0xba)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUB231PS m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xba)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMSUB231PS {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xba)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMSUB231PS zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xba)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMSUB231PS")
    }
    return p
}

// VFMSUB231SD performs "Fused Multiply-Subtract of Scalar Double-Precision Floating-Point Values".
//
// Mnemonic        : VFMSUB231SD
// ISA extensions  : AVX512F, FMA3
// Supported forms : (5 forms)
//
//    * VFMSUB231SD m64, xmm, xmm{k}{z}
//    * VFMSUB231SD xmm, xmm, xmm
//    * VFMSUB231SD m64, xmm, xmm
//    * VFMSUB231SD {er}, xmm, xmm, xmm{k}{z}
//    * VFMSUB231SD xmm, xmm, xmm{k}{z}
//
func (self *Program) VFMSUB231SD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMSUB231SD takes 3 or 4 operands")
    }
    // VFMSUB231SD m64, xmm, xmm{k}{z}
    if len(vv) == 0 && isM64(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xbb)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    // VFMSUB231SD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[1]) << 3))
            m.emit(0xbb)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUB231SD m64, xmm, xmm
    if len(vv) == 0 && isM64(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xbb)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMSUB231SD {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xbb)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMSUB231SD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xbb)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMSUB231SD")
    }
    return p
}

// VFMSUB231SS performs "Fused Multiply-Subtract of Scalar Single-Precision Floating-Point Values".
//
// Mnemonic        : VFMSUB231SS
// ISA extensions  : AVX512F, FMA3
// Supported forms : (5 forms)
//
//    * VFMSUB231SS m32, xmm, xmm{k}{z}
//    * VFMSUB231SS xmm, xmm, xmm
//    * VFMSUB231SS m32, xmm, xmm
//    * VFMSUB231SS {er}, xmm, xmm, xmm{k}{z}
//    * VFMSUB231SS xmm, xmm, xmm{k}{z}
//
func (self *Program) VFMSUB231SS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMSUB231SS takes 3 or 4 operands")
    }
    // VFMSUB231SS m32, xmm, xmm{k}{z}
    if len(vv) == 0 && isM32(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xbb)
            m.mrsd(lcode(v[2]), addr(v[0]), 4)
        })
    }
    // VFMSUB231SS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0xbb)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUB231SS m32, xmm, xmm
    if len(vv) == 0 && isM32(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xbb)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMSUB231SS {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xbb)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMSUB231SS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xbb)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMSUB231SS")
    }
    return p
}

// VFMSUBADD132PD performs "Fused Multiply-Alternating Subtract/Add of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VFMSUBADD132PD
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFMSUBADD132PD m128/m64bcst, xmm, xmm{k}{z}
//    * VFMSUBADD132PD xmm, xmm, xmm{k}{z}
//    * VFMSUBADD132PD m256/m64bcst, ymm, ymm{k}{z}
//    * VFMSUBADD132PD ymm, ymm, ymm{k}{z}
//    * VFMSUBADD132PD m512/m64bcst, zmm, zmm{k}{z}
//    * VFMSUBADD132PD xmm, xmm, xmm
//    * VFMSUBADD132PD m128, xmm, xmm
//    * VFMSUBADD132PD ymm, ymm, ymm
//    * VFMSUBADD132PD m256, ymm, ymm
//    * VFMSUBADD132PD {er}, zmm, zmm, zmm{k}{z}
//    * VFMSUBADD132PD zmm, zmm, zmm{k}{z}
//
func (self *Program) VFMSUBADD132PD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMSUBADD132PD takes 3 or 4 operands")
    }
    // VFMSUBADD132PD m128/m64bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x97)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFMSUBADD132PD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x97)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUBADD132PD m256/m64bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x97)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFMSUBADD132PD ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x97)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUBADD132PD m512/m64bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x97)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFMSUBADD132PD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[1]) << 3))
            m.emit(0x97)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUBADD132PD m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x97)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMSUBADD132PD ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit(0x97)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUBADD132PD m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x85, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x97)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMSUBADD132PD {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x97)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMSUBADD132PD zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x97)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMSUBADD132PD")
    }
    return p
}

// VFMSUBADD132PS performs "Fused Multiply-Alternating Subtract/Add of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VFMSUBADD132PS
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFMSUBADD132PS m128/m32bcst, xmm, xmm{k}{z}
//    * VFMSUBADD132PS xmm, xmm, xmm{k}{z}
//    * VFMSUBADD132PS m256/m32bcst, ymm, ymm{k}{z}
//    * VFMSUBADD132PS ymm, ymm, ymm{k}{z}
//    * VFMSUBADD132PS m512/m32bcst, zmm, zmm{k}{z}
//    * VFMSUBADD132PS xmm, xmm, xmm
//    * VFMSUBADD132PS m128, xmm, xmm
//    * VFMSUBADD132PS ymm, ymm, ymm
//    * VFMSUBADD132PS m256, ymm, ymm
//    * VFMSUBADD132PS {er}, zmm, zmm, zmm{k}{z}
//    * VFMSUBADD132PS zmm, zmm, zmm{k}{z}
//
func (self *Program) VFMSUBADD132PS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMSUBADD132PS takes 3 or 4 operands")
    }
    // VFMSUBADD132PS m128/m32bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x97)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFMSUBADD132PS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x97)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUBADD132PS m256/m32bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x97)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFMSUBADD132PS ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x97)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUBADD132PS m512/m32bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x97)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFMSUBADD132PS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x97)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUBADD132PS m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x97)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMSUBADD132PS ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x97)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUBADD132PS m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x97)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMSUBADD132PS {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x97)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMSUBADD132PS zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x97)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMSUBADD132PS")
    }
    return p
}

// VFMSUBADD213PD performs "Fused Multiply-Alternating Subtract/Add of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VFMSUBADD213PD
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFMSUBADD213PD m128/m64bcst, xmm, xmm{k}{z}
//    * VFMSUBADD213PD xmm, xmm, xmm{k}{z}
//    * VFMSUBADD213PD m256/m64bcst, ymm, ymm{k}{z}
//    * VFMSUBADD213PD ymm, ymm, ymm{k}{z}
//    * VFMSUBADD213PD m512/m64bcst, zmm, zmm{k}{z}
//    * VFMSUBADD213PD xmm, xmm, xmm
//    * VFMSUBADD213PD m128, xmm, xmm
//    * VFMSUBADD213PD ymm, ymm, ymm
//    * VFMSUBADD213PD m256, ymm, ymm
//    * VFMSUBADD213PD {er}, zmm, zmm, zmm{k}{z}
//    * VFMSUBADD213PD zmm, zmm, zmm{k}{z}
//
func (self *Program) VFMSUBADD213PD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMSUBADD213PD takes 3 or 4 operands")
    }
    // VFMSUBADD213PD m128/m64bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xa7)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFMSUBADD213PD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xa7)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUBADD213PD m256/m64bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xa7)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFMSUBADD213PD ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xa7)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUBADD213PD m512/m64bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xa7)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFMSUBADD213PD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[1]) << 3))
            m.emit(0xa7)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUBADD213PD m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xa7)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMSUBADD213PD ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit(0xa7)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUBADD213PD m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x85, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xa7)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMSUBADD213PD {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xa7)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMSUBADD213PD zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xa7)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMSUBADD213PD")
    }
    return p
}

// VFMSUBADD213PS performs "Fused Multiply-Alternating Subtract/Add of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VFMSUBADD213PS
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFMSUBADD213PS m128/m32bcst, xmm, xmm{k}{z}
//    * VFMSUBADD213PS xmm, xmm, xmm{k}{z}
//    * VFMSUBADD213PS m256/m32bcst, ymm, ymm{k}{z}
//    * VFMSUBADD213PS ymm, ymm, ymm{k}{z}
//    * VFMSUBADD213PS m512/m32bcst, zmm, zmm{k}{z}
//    * VFMSUBADD213PS xmm, xmm, xmm
//    * VFMSUBADD213PS m128, xmm, xmm
//    * VFMSUBADD213PS ymm, ymm, ymm
//    * VFMSUBADD213PS m256, ymm, ymm
//    * VFMSUBADD213PS {er}, zmm, zmm, zmm{k}{z}
//    * VFMSUBADD213PS zmm, zmm, zmm{k}{z}
//
func (self *Program) VFMSUBADD213PS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMSUBADD213PS takes 3 or 4 operands")
    }
    // VFMSUBADD213PS m128/m32bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xa7)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFMSUBADD213PS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xa7)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUBADD213PS m256/m32bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xa7)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFMSUBADD213PS ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xa7)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUBADD213PS m512/m32bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xa7)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFMSUBADD213PS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0xa7)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUBADD213PS m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xa7)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMSUBADD213PS ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0xa7)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUBADD213PS m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xa7)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMSUBADD213PS {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xa7)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMSUBADD213PS zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xa7)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMSUBADD213PS")
    }
    return p
}

// VFMSUBADD231PD performs "Fused Multiply-Alternating Subtract/Add of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VFMSUBADD231PD
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFMSUBADD231PD m128/m64bcst, xmm, xmm{k}{z}
//    * VFMSUBADD231PD xmm, xmm, xmm{k}{z}
//    * VFMSUBADD231PD m256/m64bcst, ymm, ymm{k}{z}
//    * VFMSUBADD231PD ymm, ymm, ymm{k}{z}
//    * VFMSUBADD231PD m512/m64bcst, zmm, zmm{k}{z}
//    * VFMSUBADD231PD xmm, xmm, xmm
//    * VFMSUBADD231PD m128, xmm, xmm
//    * VFMSUBADD231PD ymm, ymm, ymm
//    * VFMSUBADD231PD m256, ymm, ymm
//    * VFMSUBADD231PD {er}, zmm, zmm, zmm{k}{z}
//    * VFMSUBADD231PD zmm, zmm, zmm{k}{z}
//
func (self *Program) VFMSUBADD231PD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMSUBADD231PD takes 3 or 4 operands")
    }
    // VFMSUBADD231PD m128/m64bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xb7)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFMSUBADD231PD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xb7)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUBADD231PD m256/m64bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xb7)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFMSUBADD231PD ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xb7)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUBADD231PD m512/m64bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xb7)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFMSUBADD231PD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[1]) << 3))
            m.emit(0xb7)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUBADD231PD m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xb7)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMSUBADD231PD ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit(0xb7)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUBADD231PD m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x85, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xb7)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMSUBADD231PD {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xb7)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMSUBADD231PD zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xb7)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMSUBADD231PD")
    }
    return p
}

// VFMSUBADD231PS performs "Fused Multiply-Alternating Subtract/Add of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VFMSUBADD231PS
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFMSUBADD231PS m128/m32bcst, xmm, xmm{k}{z}
//    * VFMSUBADD231PS xmm, xmm, xmm{k}{z}
//    * VFMSUBADD231PS m256/m32bcst, ymm, ymm{k}{z}
//    * VFMSUBADD231PS ymm, ymm, ymm{k}{z}
//    * VFMSUBADD231PS m512/m32bcst, zmm, zmm{k}{z}
//    * VFMSUBADD231PS xmm, xmm, xmm
//    * VFMSUBADD231PS m128, xmm, xmm
//    * VFMSUBADD231PS ymm, ymm, ymm
//    * VFMSUBADD231PS m256, ymm, ymm
//    * VFMSUBADD231PS {er}, zmm, zmm, zmm{k}{z}
//    * VFMSUBADD231PS zmm, zmm, zmm{k}{z}
//
func (self *Program) VFMSUBADD231PS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFMSUBADD231PS takes 3 or 4 operands")
    }
    // VFMSUBADD231PS m128/m32bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xb7)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFMSUBADD231PS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xb7)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUBADD231PS m256/m32bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xb7)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFMSUBADD231PS ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xb7)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUBADD231PS m512/m32bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xb7)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFMSUBADD231PS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0xb7)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUBADD231PS m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xb7)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMSUBADD231PS ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0xb7)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFMSUBADD231PS m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xb7)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFMSUBADD231PS {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xb7)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFMSUBADD231PS zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xb7)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMSUBADD231PS")
    }
    return p
}

// VFMSUBADDPD performs "Fused Multiply-Alternating Subtract/Add of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VFMSUBADDPD
// ISA extensions  : FMA4
// Supported forms : (6 forms)
//
//    * VFMSUBADDPD xmm, xmm, xmm, xmm
//    * VFMSUBADDPD m128, xmm, xmm, xmm
//    * VFMSUBADDPD xmm, m128, xmm, xmm
//    * VFMSUBADDPD ymm, ymm, ymm, ymm
//    * VFMSUBADDPD m256, ymm, ymm, ymm
//    * VFMSUBADDPD ymm, m256, ymm, ymm
//
func (self *Program) VFMSUBADDPD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VFMSUBADDPD xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[2]) << 3))
            m.emit(0x5f)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x5f)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFMSUBADDPD m128, xmm, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x81, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0x5f)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VFMSUBADDPD xmm, m128, xmm, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x5f)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFMSUBADDPD ymm, ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit(0x5f)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit(0x5f)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFMSUBADDPD m256, ymm, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x85, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0x5f)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VFMSUBADDPD ymm, m256, ymm, ymm
    if isYMM(v0) && isM256(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x5f)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMSUBADDPD")
    }
    return p
}

// VFMSUBADDPS performs "Fused Multiply-Alternating Subtract/Add of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VFMSUBADDPS
// ISA extensions  : FMA4
// Supported forms : (6 forms)
//
//    * VFMSUBADDPS xmm, xmm, xmm, xmm
//    * VFMSUBADDPS m128, xmm, xmm, xmm
//    * VFMSUBADDPS xmm, m128, xmm, xmm
//    * VFMSUBADDPS ymm, ymm, ymm, ymm
//    * VFMSUBADDPS m256, ymm, ymm, ymm
//    * VFMSUBADDPS ymm, m256, ymm, ymm
//
func (self *Program) VFMSUBADDPS(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VFMSUBADDPS xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[2]) << 3))
            m.emit(0x5e)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x5e)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFMSUBADDPS m128, xmm, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x81, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0x5e)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VFMSUBADDPS xmm, m128, xmm, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x5e)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFMSUBADDPS ymm, ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit(0x5e)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit(0x5e)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFMSUBADDPS m256, ymm, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x85, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0x5e)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VFMSUBADDPS ymm, m256, ymm, ymm
    if isYMM(v0) && isM256(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x5e)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMSUBADDPS")
    }
    return p
}

// VFMSUBPD performs "Fused Multiply-Subtract of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VFMSUBPD
// ISA extensions  : FMA4
// Supported forms : (6 forms)
//
//    * VFMSUBPD xmm, xmm, xmm, xmm
//    * VFMSUBPD m128, xmm, xmm, xmm
//    * VFMSUBPD xmm, m128, xmm, xmm
//    * VFMSUBPD ymm, ymm, ymm, ymm
//    * VFMSUBPD m256, ymm, ymm, ymm
//    * VFMSUBPD ymm, m256, ymm, ymm
//
func (self *Program) VFMSUBPD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VFMSUBPD xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[2]) << 3))
            m.emit(0x6d)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x6d)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFMSUBPD m128, xmm, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x81, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0x6d)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VFMSUBPD xmm, m128, xmm, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x6d)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFMSUBPD ymm, ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit(0x6d)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit(0x6d)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFMSUBPD m256, ymm, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x85, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0x6d)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VFMSUBPD ymm, m256, ymm, ymm
    if isYMM(v0) && isM256(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x6d)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMSUBPD")
    }
    return p
}

// VFMSUBPS performs "Fused Multiply-Subtract of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VFMSUBPS
// ISA extensions  : FMA4
// Supported forms : (6 forms)
//
//    * VFMSUBPS xmm, xmm, xmm, xmm
//    * VFMSUBPS m128, xmm, xmm, xmm
//    * VFMSUBPS xmm, m128, xmm, xmm
//    * VFMSUBPS ymm, ymm, ymm, ymm
//    * VFMSUBPS m256, ymm, ymm, ymm
//    * VFMSUBPS ymm, m256, ymm, ymm
//
func (self *Program) VFMSUBPS(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VFMSUBPS xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[2]) << 3))
            m.emit(0x6c)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x6c)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFMSUBPS m128, xmm, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x81, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0x6c)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VFMSUBPS xmm, m128, xmm, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x6c)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFMSUBPS ymm, ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit(0x6c)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit(0x6c)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFMSUBPS m256, ymm, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x85, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0x6c)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VFMSUBPS ymm, m256, ymm, ymm
    if isYMM(v0) && isM256(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x6c)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMSUBPS")
    }
    return p
}

// VFMSUBSD performs "Fused Multiply-Subtract of Scalar Double-Precision Floating-Point Values".
//
// Mnemonic        : VFMSUBSD
// ISA extensions  : FMA4
// Supported forms : (3 forms)
//
//    * VFMSUBSD xmm, xmm, xmm, xmm
//    * VFMSUBSD m64, xmm, xmm, xmm
//    * VFMSUBSD xmm, m64, xmm, xmm
//
func (self *Program) VFMSUBSD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VFMSUBSD xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[2]) << 3))
            m.emit(0x6f)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x6f)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFMSUBSD m64, xmm, xmm, xmm
    if isM64(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x81, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0x6f)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VFMSUBSD xmm, m64, xmm, xmm
    if isXMM(v0) && isM64(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x6f)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMSUBSD")
    }
    return p
}

// VFMSUBSS performs "Fused Multiply-Subtract of Scalar Single-Precision Floating-Point Values".
//
// Mnemonic        : VFMSUBSS
// ISA extensions  : FMA4
// Supported forms : (3 forms)
//
//    * VFMSUBSS xmm, xmm, xmm, xmm
//    * VFMSUBSS m32, xmm, xmm, xmm
//    * VFMSUBSS xmm, m32, xmm, xmm
//
func (self *Program) VFMSUBSS(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VFMSUBSS xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[2]) << 3))
            m.emit(0x6e)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x6e)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFMSUBSS m32, xmm, xmm, xmm
    if isM32(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x81, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0x6e)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VFMSUBSS xmm, m32, xmm, xmm
    if isXMM(v0) && isM32(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x6e)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFMSUBSS")
    }
    return p
}

// VFNMADD132PD performs "Fused Negative Multiply-Add of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VFNMADD132PD
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFNMADD132PD m128/m64bcst, xmm, xmm{k}{z}
//    * VFNMADD132PD xmm, xmm, xmm{k}{z}
//    * VFNMADD132PD m256/m64bcst, ymm, ymm{k}{z}
//    * VFNMADD132PD ymm, ymm, ymm{k}{z}
//    * VFNMADD132PD m512/m64bcst, zmm, zmm{k}{z}
//    * VFNMADD132PD xmm, xmm, xmm
//    * VFNMADD132PD m128, xmm, xmm
//    * VFNMADD132PD ymm, ymm, ymm
//    * VFNMADD132PD m256, ymm, ymm
//    * VFNMADD132PD {er}, zmm, zmm, zmm{k}{z}
//    * VFNMADD132PD zmm, zmm, zmm{k}{z}
//
func (self *Program) VFNMADD132PD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFNMADD132PD takes 3 or 4 operands")
    }
    // VFNMADD132PD m128/m64bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x9c)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFNMADD132PD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x9c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMADD132PD m256/m64bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x9c)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFNMADD132PD ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x9c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMADD132PD m512/m64bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x9c)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFNMADD132PD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[1]) << 3))
            m.emit(0x9c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMADD132PD m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x9c)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMADD132PD ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit(0x9c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMADD132PD m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x85, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x9c)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMADD132PD {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x9c)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFNMADD132PD zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x9c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFNMADD132PD")
    }
    return p
}

// VFNMADD132PS performs "Fused Negative Multiply-Add of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VFNMADD132PS
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFNMADD132PS m128/m32bcst, xmm, xmm{k}{z}
//    * VFNMADD132PS xmm, xmm, xmm{k}{z}
//    * VFNMADD132PS m256/m32bcst, ymm, ymm{k}{z}
//    * VFNMADD132PS ymm, ymm, ymm{k}{z}
//    * VFNMADD132PS m512/m32bcst, zmm, zmm{k}{z}
//    * VFNMADD132PS xmm, xmm, xmm
//    * VFNMADD132PS m128, xmm, xmm
//    * VFNMADD132PS ymm, ymm, ymm
//    * VFNMADD132PS m256, ymm, ymm
//    * VFNMADD132PS {er}, zmm, zmm, zmm{k}{z}
//    * VFNMADD132PS zmm, zmm, zmm{k}{z}
//
func (self *Program) VFNMADD132PS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFNMADD132PS takes 3 or 4 operands")
    }
    // VFNMADD132PS m128/m32bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x9c)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFNMADD132PS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x9c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMADD132PS m256/m32bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x9c)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFNMADD132PS ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x9c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMADD132PS m512/m32bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x9c)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFNMADD132PS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x9c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMADD132PS m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x9c)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMADD132PS ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x9c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMADD132PS m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x9c)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMADD132PS {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x9c)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFNMADD132PS zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x9c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFNMADD132PS")
    }
    return p
}

// VFNMADD132SD performs "Fused Negative Multiply-Add of Scalar Double-Precision Floating-Point Values".
//
// Mnemonic        : VFNMADD132SD
// ISA extensions  : AVX512F, FMA3
// Supported forms : (5 forms)
//
//    * VFNMADD132SD m64, xmm, xmm{k}{z}
//    * VFNMADD132SD xmm, xmm, xmm
//    * VFNMADD132SD m64, xmm, xmm
//    * VFNMADD132SD {er}, xmm, xmm, xmm{k}{z}
//    * VFNMADD132SD xmm, xmm, xmm{k}{z}
//
func (self *Program) VFNMADD132SD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFNMADD132SD takes 3 or 4 operands")
    }
    // VFNMADD132SD m64, xmm, xmm{k}{z}
    if len(vv) == 0 && isM64(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x9d)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    // VFNMADD132SD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[1]) << 3))
            m.emit(0x9d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMADD132SD m64, xmm, xmm
    if len(vv) == 0 && isM64(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x9d)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMADD132SD {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x9d)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFNMADD132SD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x9d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFNMADD132SD")
    }
    return p
}

// VFNMADD132SS performs "Fused Negative Multiply-Add of Scalar Single-Precision Floating-Point Values".
//
// Mnemonic        : VFNMADD132SS
// ISA extensions  : AVX512F, FMA3
// Supported forms : (5 forms)
//
//    * VFNMADD132SS m32, xmm, xmm{k}{z}
//    * VFNMADD132SS xmm, xmm, xmm
//    * VFNMADD132SS m32, xmm, xmm
//    * VFNMADD132SS {er}, xmm, xmm, xmm{k}{z}
//    * VFNMADD132SS xmm, xmm, xmm{k}{z}
//
func (self *Program) VFNMADD132SS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFNMADD132SS takes 3 or 4 operands")
    }
    // VFNMADD132SS m32, xmm, xmm{k}{z}
    if len(vv) == 0 && isM32(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x9d)
            m.mrsd(lcode(v[2]), addr(v[0]), 4)
        })
    }
    // VFNMADD132SS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x9d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMADD132SS m32, xmm, xmm
    if len(vv) == 0 && isM32(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x9d)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMADD132SS {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x9d)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFNMADD132SS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x9d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFNMADD132SS")
    }
    return p
}

// VFNMADD213PD performs "Fused Negative Multiply-Add of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VFNMADD213PD
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFNMADD213PD m128/m64bcst, xmm, xmm{k}{z}
//    * VFNMADD213PD xmm, xmm, xmm{k}{z}
//    * VFNMADD213PD m256/m64bcst, ymm, ymm{k}{z}
//    * VFNMADD213PD ymm, ymm, ymm{k}{z}
//    * VFNMADD213PD m512/m64bcst, zmm, zmm{k}{z}
//    * VFNMADD213PD xmm, xmm, xmm
//    * VFNMADD213PD m128, xmm, xmm
//    * VFNMADD213PD ymm, ymm, ymm
//    * VFNMADD213PD m256, ymm, ymm
//    * VFNMADD213PD {er}, zmm, zmm, zmm{k}{z}
//    * VFNMADD213PD zmm, zmm, zmm{k}{z}
//
func (self *Program) VFNMADD213PD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFNMADD213PD takes 3 or 4 operands")
    }
    // VFNMADD213PD m128/m64bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xac)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFNMADD213PD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xac)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMADD213PD m256/m64bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xac)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFNMADD213PD ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xac)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMADD213PD m512/m64bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xac)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFNMADD213PD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[1]) << 3))
            m.emit(0xac)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMADD213PD m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xac)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMADD213PD ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit(0xac)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMADD213PD m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x85, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xac)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMADD213PD {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xac)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFNMADD213PD zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xac)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFNMADD213PD")
    }
    return p
}

// VFNMADD213PS performs "Fused Negative Multiply-Add of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VFNMADD213PS
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFNMADD213PS m128/m32bcst, xmm, xmm{k}{z}
//    * VFNMADD213PS xmm, xmm, xmm{k}{z}
//    * VFNMADD213PS m256/m32bcst, ymm, ymm{k}{z}
//    * VFNMADD213PS ymm, ymm, ymm{k}{z}
//    * VFNMADD213PS m512/m32bcst, zmm, zmm{k}{z}
//    * VFNMADD213PS xmm, xmm, xmm
//    * VFNMADD213PS m128, xmm, xmm
//    * VFNMADD213PS ymm, ymm, ymm
//    * VFNMADD213PS m256, ymm, ymm
//    * VFNMADD213PS {er}, zmm, zmm, zmm{k}{z}
//    * VFNMADD213PS zmm, zmm, zmm{k}{z}
//
func (self *Program) VFNMADD213PS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFNMADD213PS takes 3 or 4 operands")
    }
    // VFNMADD213PS m128/m32bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xac)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFNMADD213PS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xac)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMADD213PS m256/m32bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xac)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFNMADD213PS ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xac)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMADD213PS m512/m32bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xac)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFNMADD213PS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0xac)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMADD213PS m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xac)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMADD213PS ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0xac)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMADD213PS m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xac)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMADD213PS {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xac)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFNMADD213PS zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xac)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFNMADD213PS")
    }
    return p
}

// VFNMADD213SD performs "Fused Negative Multiply-Add of Scalar Double-Precision Floating-Point Values".
//
// Mnemonic        : VFNMADD213SD
// ISA extensions  : AVX512F, FMA3
// Supported forms : (5 forms)
//
//    * VFNMADD213SD m64, xmm, xmm{k}{z}
//    * VFNMADD213SD xmm, xmm, xmm
//    * VFNMADD213SD m64, xmm, xmm
//    * VFNMADD213SD {er}, xmm, xmm, xmm{k}{z}
//    * VFNMADD213SD xmm, xmm, xmm{k}{z}
//
func (self *Program) VFNMADD213SD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFNMADD213SD takes 3 or 4 operands")
    }
    // VFNMADD213SD m64, xmm, xmm{k}{z}
    if len(vv) == 0 && isM64(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xad)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    // VFNMADD213SD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[1]) << 3))
            m.emit(0xad)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMADD213SD m64, xmm, xmm
    if len(vv) == 0 && isM64(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xad)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMADD213SD {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xad)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFNMADD213SD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xad)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFNMADD213SD")
    }
    return p
}

// VFNMADD213SS performs "Fused Negative Multiply-Add of Scalar Single-Precision Floating-Point Values".
//
// Mnemonic        : VFNMADD213SS
// ISA extensions  : AVX512F, FMA3
// Supported forms : (5 forms)
//
//    * VFNMADD213SS m32, xmm, xmm{k}{z}
//    * VFNMADD213SS xmm, xmm, xmm
//    * VFNMADD213SS m32, xmm, xmm
//    * VFNMADD213SS {er}, xmm, xmm, xmm{k}{z}
//    * VFNMADD213SS xmm, xmm, xmm{k}{z}
//
func (self *Program) VFNMADD213SS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFNMADD213SS takes 3 or 4 operands")
    }
    // VFNMADD213SS m32, xmm, xmm{k}{z}
    if len(vv) == 0 && isM32(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xad)
            m.mrsd(lcode(v[2]), addr(v[0]), 4)
        })
    }
    // VFNMADD213SS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0xad)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMADD213SS m32, xmm, xmm
    if len(vv) == 0 && isM32(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xad)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMADD213SS {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xad)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFNMADD213SS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xad)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFNMADD213SS")
    }
    return p
}

// VFNMADD231PD performs "Fused Negative Multiply-Add of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VFNMADD231PD
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFNMADD231PD m128/m64bcst, xmm, xmm{k}{z}
//    * VFNMADD231PD xmm, xmm, xmm{k}{z}
//    * VFNMADD231PD m256/m64bcst, ymm, ymm{k}{z}
//    * VFNMADD231PD ymm, ymm, ymm{k}{z}
//    * VFNMADD231PD m512/m64bcst, zmm, zmm{k}{z}
//    * VFNMADD231PD xmm, xmm, xmm
//    * VFNMADD231PD m128, xmm, xmm
//    * VFNMADD231PD ymm, ymm, ymm
//    * VFNMADD231PD m256, ymm, ymm
//    * VFNMADD231PD {er}, zmm, zmm, zmm{k}{z}
//    * VFNMADD231PD zmm, zmm, zmm{k}{z}
//
func (self *Program) VFNMADD231PD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFNMADD231PD takes 3 or 4 operands")
    }
    // VFNMADD231PD m128/m64bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xbc)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFNMADD231PD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xbc)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMADD231PD m256/m64bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xbc)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFNMADD231PD ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xbc)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMADD231PD m512/m64bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xbc)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFNMADD231PD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[1]) << 3))
            m.emit(0xbc)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMADD231PD m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xbc)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMADD231PD ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit(0xbc)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMADD231PD m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x85, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xbc)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMADD231PD {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xbc)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFNMADD231PD zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xbc)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFNMADD231PD")
    }
    return p
}

// VFNMADD231PS performs "Fused Negative Multiply-Add of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VFNMADD231PS
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFNMADD231PS m128/m32bcst, xmm, xmm{k}{z}
//    * VFNMADD231PS xmm, xmm, xmm{k}{z}
//    * VFNMADD231PS m256/m32bcst, ymm, ymm{k}{z}
//    * VFNMADD231PS ymm, ymm, ymm{k}{z}
//    * VFNMADD231PS m512/m32bcst, zmm, zmm{k}{z}
//    * VFNMADD231PS xmm, xmm, xmm
//    * VFNMADD231PS m128, xmm, xmm
//    * VFNMADD231PS ymm, ymm, ymm
//    * VFNMADD231PS m256, ymm, ymm
//    * VFNMADD231PS {er}, zmm, zmm, zmm{k}{z}
//    * VFNMADD231PS zmm, zmm, zmm{k}{z}
//
func (self *Program) VFNMADD231PS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFNMADD231PS takes 3 or 4 operands")
    }
    // VFNMADD231PS m128/m32bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xbc)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFNMADD231PS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xbc)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMADD231PS m256/m32bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xbc)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFNMADD231PS ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xbc)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMADD231PS m512/m32bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xbc)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFNMADD231PS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0xbc)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMADD231PS m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xbc)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMADD231PS ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0xbc)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMADD231PS m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xbc)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMADD231PS {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xbc)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFNMADD231PS zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xbc)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFNMADD231PS")
    }
    return p
}

// VFNMADD231SD performs "Fused Negative Multiply-Add of Scalar Double-Precision Floating-Point Values".
//
// Mnemonic        : VFNMADD231SD
// ISA extensions  : AVX512F, FMA3
// Supported forms : (5 forms)
//
//    * VFNMADD231SD m64, xmm, xmm{k}{z}
//    * VFNMADD231SD xmm, xmm, xmm
//    * VFNMADD231SD m64, xmm, xmm
//    * VFNMADD231SD {er}, xmm, xmm, xmm{k}{z}
//    * VFNMADD231SD xmm, xmm, xmm{k}{z}
//
func (self *Program) VFNMADD231SD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFNMADD231SD takes 3 or 4 operands")
    }
    // VFNMADD231SD m64, xmm, xmm{k}{z}
    if len(vv) == 0 && isM64(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xbd)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    // VFNMADD231SD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[1]) << 3))
            m.emit(0xbd)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMADD231SD m64, xmm, xmm
    if len(vv) == 0 && isM64(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xbd)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMADD231SD {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xbd)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFNMADD231SD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xbd)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFNMADD231SD")
    }
    return p
}

// VFNMADD231SS performs "Fused Negative Multiply-Add of Scalar Single-Precision Floating-Point Values".
//
// Mnemonic        : VFNMADD231SS
// ISA extensions  : AVX512F, FMA3
// Supported forms : (5 forms)
//
//    * VFNMADD231SS m32, xmm, xmm{k}{z}
//    * VFNMADD231SS xmm, xmm, xmm
//    * VFNMADD231SS m32, xmm, xmm
//    * VFNMADD231SS {er}, xmm, xmm, xmm{k}{z}
//    * VFNMADD231SS xmm, xmm, xmm{k}{z}
//
func (self *Program) VFNMADD231SS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFNMADD231SS takes 3 or 4 operands")
    }
    // VFNMADD231SS m32, xmm, xmm{k}{z}
    if len(vv) == 0 && isM32(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xbd)
            m.mrsd(lcode(v[2]), addr(v[0]), 4)
        })
    }
    // VFNMADD231SS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0xbd)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMADD231SS m32, xmm, xmm
    if len(vv) == 0 && isM32(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xbd)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMADD231SS {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xbd)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFNMADD231SS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xbd)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFNMADD231SS")
    }
    return p
}

// VFNMADDPD performs "Fused Negative Multiply-Add of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VFNMADDPD
// ISA extensions  : FMA4
// Supported forms : (6 forms)
//
//    * VFNMADDPD xmm, xmm, xmm, xmm
//    * VFNMADDPD m128, xmm, xmm, xmm
//    * VFNMADDPD xmm, m128, xmm, xmm
//    * VFNMADDPD ymm, ymm, ymm, ymm
//    * VFNMADDPD m256, ymm, ymm, ymm
//    * VFNMADDPD ymm, m256, ymm, ymm
//
func (self *Program) VFNMADDPD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VFNMADDPD xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[2]) << 3))
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFNMADDPD m128, xmm, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x81, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0x79)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VFNMADDPD xmm, m128, xmm, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x79)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFNMADDPD ymm, ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFNMADDPD m256, ymm, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x85, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0x79)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VFNMADDPD ymm, m256, ymm, ymm
    if isYMM(v0) && isM256(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x79)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFNMADDPD")
    }
    return p
}

// VFNMADDPS performs "Fused Negative Multiply-Add of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VFNMADDPS
// ISA extensions  : FMA4
// Supported forms : (6 forms)
//
//    * VFNMADDPS xmm, xmm, xmm, xmm
//    * VFNMADDPS m128, xmm, xmm, xmm
//    * VFNMADDPS xmm, m128, xmm, xmm
//    * VFNMADDPS ymm, ymm, ymm, ymm
//    * VFNMADDPS m256, ymm, ymm, ymm
//    * VFNMADDPS ymm, m256, ymm, ymm
//
func (self *Program) VFNMADDPS(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VFNMADDPS xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[2]) << 3))
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFNMADDPS m128, xmm, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x81, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0x78)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VFNMADDPS xmm, m128, xmm, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x78)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFNMADDPS ymm, ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFNMADDPS m256, ymm, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x85, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0x78)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VFNMADDPS ymm, m256, ymm, ymm
    if isYMM(v0) && isM256(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x78)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFNMADDPS")
    }
    return p
}

// VFNMADDSD performs "Fused Negative Multiply-Add of Scalar Double-Precision Floating-Point Values".
//
// Mnemonic        : VFNMADDSD
// ISA extensions  : FMA4
// Supported forms : (3 forms)
//
//    * VFNMADDSD xmm, xmm, xmm, xmm
//    * VFNMADDSD m64, xmm, xmm, xmm
//    * VFNMADDSD xmm, m64, xmm, xmm
//
func (self *Program) VFNMADDSD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VFNMADDSD xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[2]) << 3))
            m.emit(0x7b)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x7b)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFNMADDSD m64, xmm, xmm, xmm
    if isM64(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x81, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0x7b)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VFNMADDSD xmm, m64, xmm, xmm
    if isXMM(v0) && isM64(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x7b)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFNMADDSD")
    }
    return p
}

// VFNMADDSS performs "Fused Negative Multiply-Add of Scalar Single-Precision Floating-Point Values".
//
// Mnemonic        : VFNMADDSS
// ISA extensions  : FMA4
// Supported forms : (3 forms)
//
//    * VFNMADDSS xmm, xmm, xmm, xmm
//    * VFNMADDSS m32, xmm, xmm, xmm
//    * VFNMADDSS xmm, m32, xmm, xmm
//
func (self *Program) VFNMADDSS(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VFNMADDSS xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[2]) << 3))
            m.emit(0x7a)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x7a)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFNMADDSS m32, xmm, xmm, xmm
    if isM32(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x81, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0x7a)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VFNMADDSS xmm, m32, xmm, xmm
    if isXMM(v0) && isM32(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x7a)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFNMADDSS")
    }
    return p
}

// VFNMSUB132PD performs "Fused Negative Multiply-Subtract of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VFNMSUB132PD
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFNMSUB132PD m128/m64bcst, xmm, xmm{k}{z}
//    * VFNMSUB132PD xmm, xmm, xmm{k}{z}
//    * VFNMSUB132PD m256/m64bcst, ymm, ymm{k}{z}
//    * VFNMSUB132PD ymm, ymm, ymm{k}{z}
//    * VFNMSUB132PD m512/m64bcst, zmm, zmm{k}{z}
//    * VFNMSUB132PD xmm, xmm, xmm
//    * VFNMSUB132PD m128, xmm, xmm
//    * VFNMSUB132PD ymm, ymm, ymm
//    * VFNMSUB132PD m256, ymm, ymm
//    * VFNMSUB132PD {er}, zmm, zmm, zmm{k}{z}
//    * VFNMSUB132PD zmm, zmm, zmm{k}{z}
//
func (self *Program) VFNMSUB132PD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFNMSUB132PD takes 3 or 4 operands")
    }
    // VFNMSUB132PD m128/m64bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x9e)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFNMSUB132PD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x9e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMSUB132PD m256/m64bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x9e)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFNMSUB132PD ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x9e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMSUB132PD m512/m64bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x9e)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFNMSUB132PD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[1]) << 3))
            m.emit(0x9e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMSUB132PD m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x9e)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMSUB132PD ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit(0x9e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMSUB132PD m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x85, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x9e)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMSUB132PD {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x9e)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFNMSUB132PD zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x9e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFNMSUB132PD")
    }
    return p
}

// VFNMSUB132PS performs "Fused Negative Multiply-Subtract of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VFNMSUB132PS
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFNMSUB132PS m128/m32bcst, xmm, xmm{k}{z}
//    * VFNMSUB132PS xmm, xmm, xmm{k}{z}
//    * VFNMSUB132PS m256/m32bcst, ymm, ymm{k}{z}
//    * VFNMSUB132PS ymm, ymm, ymm{k}{z}
//    * VFNMSUB132PS m512/m32bcst, zmm, zmm{k}{z}
//    * VFNMSUB132PS xmm, xmm, xmm
//    * VFNMSUB132PS m128, xmm, xmm
//    * VFNMSUB132PS ymm, ymm, ymm
//    * VFNMSUB132PS m256, ymm, ymm
//    * VFNMSUB132PS {er}, zmm, zmm, zmm{k}{z}
//    * VFNMSUB132PS zmm, zmm, zmm{k}{z}
//
func (self *Program) VFNMSUB132PS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFNMSUB132PS takes 3 or 4 operands")
    }
    // VFNMSUB132PS m128/m32bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x9e)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFNMSUB132PS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x9e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMSUB132PS m256/m32bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x9e)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFNMSUB132PS ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x9e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMSUB132PS m512/m32bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x9e)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFNMSUB132PS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x9e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMSUB132PS m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x9e)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMSUB132PS ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x9e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMSUB132PS m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x9e)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMSUB132PS {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x9e)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFNMSUB132PS zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x9e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFNMSUB132PS")
    }
    return p
}

// VFNMSUB132SD performs "Fused Negative Multiply-Subtract of Scalar Double-Precision Floating-Point Values".
//
// Mnemonic        : VFNMSUB132SD
// ISA extensions  : AVX512F, FMA3
// Supported forms : (5 forms)
//
//    * VFNMSUB132SD m64, xmm, xmm{k}{z}
//    * VFNMSUB132SD xmm, xmm, xmm
//    * VFNMSUB132SD m64, xmm, xmm
//    * VFNMSUB132SD {er}, xmm, xmm, xmm{k}{z}
//    * VFNMSUB132SD xmm, xmm, xmm{k}{z}
//
func (self *Program) VFNMSUB132SD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFNMSUB132SD takes 3 or 4 operands")
    }
    // VFNMSUB132SD m64, xmm, xmm{k}{z}
    if len(vv) == 0 && isM64(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x9f)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    // VFNMSUB132SD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[1]) << 3))
            m.emit(0x9f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMSUB132SD m64, xmm, xmm
    if len(vv) == 0 && isM64(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x9f)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMSUB132SD {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x9f)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFNMSUB132SD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x9f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFNMSUB132SD")
    }
    return p
}

// VFNMSUB132SS performs "Fused Negative Multiply-Subtract of Scalar Single-Precision Floating-Point Values".
//
// Mnemonic        : VFNMSUB132SS
// ISA extensions  : AVX512F, FMA3
// Supported forms : (5 forms)
//
//    * VFNMSUB132SS m32, xmm, xmm{k}{z}
//    * VFNMSUB132SS xmm, xmm, xmm
//    * VFNMSUB132SS m32, xmm, xmm
//    * VFNMSUB132SS {er}, xmm, xmm, xmm{k}{z}
//    * VFNMSUB132SS xmm, xmm, xmm{k}{z}
//
func (self *Program) VFNMSUB132SS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFNMSUB132SS takes 3 or 4 operands")
    }
    // VFNMSUB132SS m32, xmm, xmm{k}{z}
    if len(vv) == 0 && isM32(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x9f)
            m.mrsd(lcode(v[2]), addr(v[0]), 4)
        })
    }
    // VFNMSUB132SS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x9f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMSUB132SS m32, xmm, xmm
    if len(vv) == 0 && isM32(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x9f)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMSUB132SS {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x9f)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFNMSUB132SS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x9f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFNMSUB132SS")
    }
    return p
}

// VFNMSUB213PD performs "Fused Negative Multiply-Subtract of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VFNMSUB213PD
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFNMSUB213PD m128/m64bcst, xmm, xmm{k}{z}
//    * VFNMSUB213PD xmm, xmm, xmm{k}{z}
//    * VFNMSUB213PD m256/m64bcst, ymm, ymm{k}{z}
//    * VFNMSUB213PD ymm, ymm, ymm{k}{z}
//    * VFNMSUB213PD m512/m64bcst, zmm, zmm{k}{z}
//    * VFNMSUB213PD xmm, xmm, xmm
//    * VFNMSUB213PD m128, xmm, xmm
//    * VFNMSUB213PD ymm, ymm, ymm
//    * VFNMSUB213PD m256, ymm, ymm
//    * VFNMSUB213PD {er}, zmm, zmm, zmm{k}{z}
//    * VFNMSUB213PD zmm, zmm, zmm{k}{z}
//
func (self *Program) VFNMSUB213PD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFNMSUB213PD takes 3 or 4 operands")
    }
    // VFNMSUB213PD m128/m64bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xae)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFNMSUB213PD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xae)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMSUB213PD m256/m64bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xae)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFNMSUB213PD ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xae)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMSUB213PD m512/m64bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xae)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFNMSUB213PD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[1]) << 3))
            m.emit(0xae)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMSUB213PD m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xae)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMSUB213PD ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit(0xae)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMSUB213PD m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x85, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xae)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMSUB213PD {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xae)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFNMSUB213PD zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xae)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFNMSUB213PD")
    }
    return p
}

// VFNMSUB213PS performs "Fused Negative Multiply-Subtract of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VFNMSUB213PS
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFNMSUB213PS m128/m32bcst, xmm, xmm{k}{z}
//    * VFNMSUB213PS xmm, xmm, xmm{k}{z}
//    * VFNMSUB213PS m256/m32bcst, ymm, ymm{k}{z}
//    * VFNMSUB213PS ymm, ymm, ymm{k}{z}
//    * VFNMSUB213PS m512/m32bcst, zmm, zmm{k}{z}
//    * VFNMSUB213PS xmm, xmm, xmm
//    * VFNMSUB213PS m128, xmm, xmm
//    * VFNMSUB213PS ymm, ymm, ymm
//    * VFNMSUB213PS m256, ymm, ymm
//    * VFNMSUB213PS {er}, zmm, zmm, zmm{k}{z}
//    * VFNMSUB213PS zmm, zmm, zmm{k}{z}
//
func (self *Program) VFNMSUB213PS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFNMSUB213PS takes 3 or 4 operands")
    }
    // VFNMSUB213PS m128/m32bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xae)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFNMSUB213PS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xae)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMSUB213PS m256/m32bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xae)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFNMSUB213PS ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xae)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMSUB213PS m512/m32bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xae)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFNMSUB213PS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0xae)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMSUB213PS m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xae)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMSUB213PS ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0xae)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMSUB213PS m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xae)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMSUB213PS {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xae)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFNMSUB213PS zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xae)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFNMSUB213PS")
    }
    return p
}

// VFNMSUB213SD performs "Fused Negative Multiply-Subtract of Scalar Double-Precision Floating-Point Values".
//
// Mnemonic        : VFNMSUB213SD
// ISA extensions  : AVX512F, FMA3
// Supported forms : (5 forms)
//
//    * VFNMSUB213SD m64, xmm, xmm{k}{z}
//    * VFNMSUB213SD xmm, xmm, xmm
//    * VFNMSUB213SD m64, xmm, xmm
//    * VFNMSUB213SD {er}, xmm, xmm, xmm{k}{z}
//    * VFNMSUB213SD xmm, xmm, xmm{k}{z}
//
func (self *Program) VFNMSUB213SD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFNMSUB213SD takes 3 or 4 operands")
    }
    // VFNMSUB213SD m64, xmm, xmm{k}{z}
    if len(vv) == 0 && isM64(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xaf)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    // VFNMSUB213SD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[1]) << 3))
            m.emit(0xaf)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMSUB213SD m64, xmm, xmm
    if len(vv) == 0 && isM64(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xaf)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMSUB213SD {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xaf)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFNMSUB213SD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xaf)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFNMSUB213SD")
    }
    return p
}

// VFNMSUB213SS performs "Fused Negative Multiply-Subtract of Scalar Single-Precision Floating-Point Values".
//
// Mnemonic        : VFNMSUB213SS
// ISA extensions  : AVX512F, FMA3
// Supported forms : (5 forms)
//
//    * VFNMSUB213SS m32, xmm, xmm{k}{z}
//    * VFNMSUB213SS xmm, xmm, xmm
//    * VFNMSUB213SS m32, xmm, xmm
//    * VFNMSUB213SS {er}, xmm, xmm, xmm{k}{z}
//    * VFNMSUB213SS xmm, xmm, xmm{k}{z}
//
func (self *Program) VFNMSUB213SS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFNMSUB213SS takes 3 or 4 operands")
    }
    // VFNMSUB213SS m32, xmm, xmm{k}{z}
    if len(vv) == 0 && isM32(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xaf)
            m.mrsd(lcode(v[2]), addr(v[0]), 4)
        })
    }
    // VFNMSUB213SS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0xaf)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMSUB213SS m32, xmm, xmm
    if len(vv) == 0 && isM32(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xaf)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMSUB213SS {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xaf)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFNMSUB213SS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xaf)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFNMSUB213SS")
    }
    return p
}

// VFNMSUB231PD performs "Fused Negative Multiply-Subtract of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VFNMSUB231PD
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFNMSUB231PD m128/m64bcst, xmm, xmm{k}{z}
//    * VFNMSUB231PD xmm, xmm, xmm{k}{z}
//    * VFNMSUB231PD m256/m64bcst, ymm, ymm{k}{z}
//    * VFNMSUB231PD ymm, ymm, ymm{k}{z}
//    * VFNMSUB231PD m512/m64bcst, zmm, zmm{k}{z}
//    * VFNMSUB231PD xmm, xmm, xmm
//    * VFNMSUB231PD m128, xmm, xmm
//    * VFNMSUB231PD ymm, ymm, ymm
//    * VFNMSUB231PD m256, ymm, ymm
//    * VFNMSUB231PD {er}, zmm, zmm, zmm{k}{z}
//    * VFNMSUB231PD zmm, zmm, zmm{k}{z}
//
func (self *Program) VFNMSUB231PD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFNMSUB231PD takes 3 or 4 operands")
    }
    // VFNMSUB231PD m128/m64bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xbe)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFNMSUB231PD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xbe)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMSUB231PD m256/m64bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xbe)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFNMSUB231PD ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xbe)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMSUB231PD m512/m64bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xbe)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFNMSUB231PD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[1]) << 3))
            m.emit(0xbe)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMSUB231PD m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xbe)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMSUB231PD ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit(0xbe)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMSUB231PD m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x85, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xbe)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMSUB231PD {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xbe)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFNMSUB231PD zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xbe)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFNMSUB231PD")
    }
    return p
}

// VFNMSUB231PS performs "Fused Negative Multiply-Subtract of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VFNMSUB231PS
// ISA extensions  : AVX512F, AVX512VL, FMA3
// Supported forms : (11 forms)
//
//    * VFNMSUB231PS m128/m32bcst, xmm, xmm{k}{z}
//    * VFNMSUB231PS xmm, xmm, xmm{k}{z}
//    * VFNMSUB231PS m256/m32bcst, ymm, ymm{k}{z}
//    * VFNMSUB231PS ymm, ymm, ymm{k}{z}
//    * VFNMSUB231PS m512/m32bcst, zmm, zmm{k}{z}
//    * VFNMSUB231PS xmm, xmm, xmm
//    * VFNMSUB231PS m128, xmm, xmm
//    * VFNMSUB231PS ymm, ymm, ymm
//    * VFNMSUB231PS m256, ymm, ymm
//    * VFNMSUB231PS {er}, zmm, zmm, zmm{k}{z}
//    * VFNMSUB231PS zmm, zmm, zmm{k}{z}
//
func (self *Program) VFNMSUB231PS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFNMSUB231PS takes 3 or 4 operands")
    }
    // VFNMSUB231PS m128/m32bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xbe)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VFNMSUB231PS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xbe)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMSUB231PS m256/m32bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xbe)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VFNMSUB231PS ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xbe)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMSUB231PS m512/m32bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xbe)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VFNMSUB231PS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0xbe)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMSUB231PS m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xbe)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMSUB231PS ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0xbe)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMSUB231PS m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xbe)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMSUB231PS {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xbe)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFNMSUB231PS zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xbe)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFNMSUB231PS")
    }
    return p
}

// VFNMSUB231SD performs "Fused Negative Multiply-Subtract of Scalar Double-Precision Floating-Point Values".
//
// Mnemonic        : VFNMSUB231SD
// ISA extensions  : AVX512F, FMA3
// Supported forms : (5 forms)
//
//    * VFNMSUB231SD m64, xmm, xmm{k}{z}
//    * VFNMSUB231SD xmm, xmm, xmm
//    * VFNMSUB231SD m64, xmm, xmm
//    * VFNMSUB231SD {er}, xmm, xmm, xmm{k}{z}
//    * VFNMSUB231SD xmm, xmm, xmm{k}{z}
//
func (self *Program) VFNMSUB231SD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFNMSUB231SD takes 3 or 4 operands")
    }
    // VFNMSUB231SD m64, xmm, xmm{k}{z}
    if len(vv) == 0 && isM64(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xbf)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    // VFNMSUB231SD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[1]) << 3))
            m.emit(0xbf)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMSUB231SD m64, xmm, xmm
    if len(vv) == 0 && isM64(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xbf)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMSUB231SD {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xbf)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFNMSUB231SD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xbf)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFNMSUB231SD")
    }
    return p
}

// VFNMSUB231SS performs "Fused Negative Multiply-Subtract of Scalar Single-Precision Floating-Point Values".
//
// Mnemonic        : VFNMSUB231SS
// ISA extensions  : AVX512F, FMA3
// Supported forms : (5 forms)
//
//    * VFNMSUB231SS m32, xmm, xmm{k}{z}
//    * VFNMSUB231SS xmm, xmm, xmm
//    * VFNMSUB231SS m32, xmm, xmm
//    * VFNMSUB231SS {er}, xmm, xmm, xmm{k}{z}
//    * VFNMSUB231SS xmm, xmm, xmm{k}{z}
//
func (self *Program) VFNMSUB231SS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VFNMSUB231SS takes 3 or 4 operands")
    }
    // VFNMSUB231SS m32, xmm, xmm{k}{z}
    if len(vv) == 0 && isM32(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xbf)
            m.mrsd(lcode(v[2]), addr(v[0]), 4)
        })
    }
    // VFNMSUB231SS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0xbf)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VFNMSUB231SS m32, xmm, xmm
    if len(vv) == 0 && isM32(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_FMA3)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xbf)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VFNMSUB231SS {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xbf)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VFNMSUB231SS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xbf)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFNMSUB231SS")
    }
    return p
}

// VFNMSUBPD performs "Fused Negative Multiply-Subtract of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VFNMSUBPD
// ISA extensions  : FMA4
// Supported forms : (6 forms)
//
//    * VFNMSUBPD xmm, xmm, xmm, xmm
//    * VFNMSUBPD m128, xmm, xmm, xmm
//    * VFNMSUBPD xmm, m128, xmm, xmm
//    * VFNMSUBPD ymm, ymm, ymm, ymm
//    * VFNMSUBPD m256, ymm, ymm, ymm
//    * VFNMSUBPD ymm, m256, ymm, ymm
//
func (self *Program) VFNMSUBPD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VFNMSUBPD xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[2]) << 3))
            m.emit(0x7d)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x7d)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFNMSUBPD m128, xmm, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x81, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0x7d)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VFNMSUBPD xmm, m128, xmm, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x7d)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFNMSUBPD ymm, ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit(0x7d)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit(0x7d)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFNMSUBPD m256, ymm, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x85, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0x7d)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VFNMSUBPD ymm, m256, ymm, ymm
    if isYMM(v0) && isM256(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x7d)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFNMSUBPD")
    }
    return p
}

// VFNMSUBPS performs "Fused Negative Multiply-Subtract of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VFNMSUBPS
// ISA extensions  : FMA4
// Supported forms : (6 forms)
//
//    * VFNMSUBPS xmm, xmm, xmm, xmm
//    * VFNMSUBPS m128, xmm, xmm, xmm
//    * VFNMSUBPS xmm, m128, xmm, xmm
//    * VFNMSUBPS ymm, ymm, ymm, ymm
//    * VFNMSUBPS m256, ymm, ymm, ymm
//    * VFNMSUBPS ymm, m256, ymm, ymm
//
func (self *Program) VFNMSUBPS(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VFNMSUBPS xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[2]) << 3))
            m.emit(0x7c)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x7c)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFNMSUBPS m128, xmm, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x81, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0x7c)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VFNMSUBPS xmm, m128, xmm, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x7c)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFNMSUBPS ymm, ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit(0x7c)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit(0x7c)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFNMSUBPS m256, ymm, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x85, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0x7c)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VFNMSUBPS ymm, m256, ymm, ymm
    if isYMM(v0) && isM256(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x7c)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFNMSUBPS")
    }
    return p
}

// VFNMSUBSD performs "Fused Negative Multiply-Subtract of Scalar Double-Precision Floating-Point Values".
//
// Mnemonic        : VFNMSUBSD
// ISA extensions  : FMA4
// Supported forms : (3 forms)
//
//    * VFNMSUBSD xmm, xmm, xmm, xmm
//    * VFNMSUBSD m64, xmm, xmm, xmm
//    * VFNMSUBSD xmm, m64, xmm, xmm
//
func (self *Program) VFNMSUBSD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VFNMSUBSD xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[2]) << 3))
            m.emit(0x7f)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x7f)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFNMSUBSD m64, xmm, xmm, xmm
    if isM64(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x81, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0x7f)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VFNMSUBSD xmm, m64, xmm, xmm
    if isXMM(v0) && isM64(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x7f)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFNMSUBSD")
    }
    return p
}

// VFNMSUBSS performs "Fused Negative Multiply-Subtract of Scalar Single-Precision Floating-Point Values".
//
// Mnemonic        : VFNMSUBSS
// ISA extensions  : FMA4
// Supported forms : (3 forms)
//
//    * VFNMSUBSS xmm, xmm, xmm, xmm
//    * VFNMSUBSS m32, xmm, xmm, xmm
//    * VFNMSUBSS xmm, m32, xmm, xmm
//
func (self *Program) VFNMSUBSS(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VFNMSUBSS xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[2]) << 3))
            m.emit(0x7e)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x7e)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VFNMSUBSS m32, xmm, xmm, xmm
    if isM32(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x81, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0x7e)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VFNMSUBSS xmm, m32, xmm, xmm
    if isXMM(v0) && isM32(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_FMA4)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x7e)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFNMSUBSS")
    }
    return p
}

// VFPCLASSPD performs "Test Class of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VFPCLASSPD
// ISA extensions  : AVX512DQ, AVX512VL
// Supported forms : (6 forms)
//
//    * VFPCLASSPD imm8, m128/m64bcst, k{k}
//    * VFPCLASSPD imm8, m256/m64bcst, k{k}
//    * VFPCLASSPD imm8, m512/m64bcst, k{k}
//    * VFPCLASSPD imm8, xmm, k{k}
//    * VFPCLASSPD imm8, ymm, k{k}
//    * VFPCLASSPD imm8, zmm, k{k}
//
func (self *Program) VFPCLASSPD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VFPCLASSPD imm8, m128/m64bcst, k{k}
    if isImm8(v0) && isM128M64bcst(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b00, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), 0, bcode(v[1]))
            m.emit(0x66)
            m.mrsd(lcode(v[2]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VFPCLASSPD imm8, m256/m64bcst, k{k}
    if isImm8(v0) && isM256M64bcst(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b01, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), 0, bcode(v[1]))
            m.emit(0x66)
            m.mrsd(lcode(v[2]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VFPCLASSPD imm8, m512/m64bcst, k{k}
    if isImm8(v0) && isM512M64bcst(v1) && isKk(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b10, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), 0, bcode(v[1]))
            m.emit(0x66)
            m.mrsd(lcode(v[2]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VFPCLASSPD imm8, xmm, k{k}
    if isImm8(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd)
            m.emit(kcode(v[2]) | 0x08)
            m.emit(0x66)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VFPCLASSPD imm8, ymm, k{k}
    if isImm8(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd)
            m.emit(kcode(v[2]) | 0x28)
            m.emit(0x66)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VFPCLASSPD imm8, zmm, k{k}
    if isImm8(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd)
            m.emit(kcode(v[2]) | 0x48)
            m.emit(0x66)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFPCLASSPD")
    }
    return p
}

// VFPCLASSPS performs "Test Class of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VFPCLASSPS
// ISA extensions  : AVX512DQ, AVX512VL
// Supported forms : (6 forms)
//
//    * VFPCLASSPS imm8, m128/m32bcst, k{k}
//    * VFPCLASSPS imm8, m256/m32bcst, k{k}
//    * VFPCLASSPS imm8, m512/m32bcst, k{k}
//    * VFPCLASSPS imm8, xmm, k{k}
//    * VFPCLASSPS imm8, ymm, k{k}
//    * VFPCLASSPS imm8, zmm, k{k}
//
func (self *Program) VFPCLASSPS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VFPCLASSPS imm8, m128/m32bcst, k{k}
    if isImm8(v0) && isM128M32bcst(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b00, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), 0, bcode(v[1]))
            m.emit(0x66)
            m.mrsd(lcode(v[2]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VFPCLASSPS imm8, m256/m32bcst, k{k}
    if isImm8(v0) && isM256M32bcst(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b01, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), 0, bcode(v[1]))
            m.emit(0x66)
            m.mrsd(lcode(v[2]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VFPCLASSPS imm8, m512/m32bcst, k{k}
    if isImm8(v0) && isM512M32bcst(v1) && isKk(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b10, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), 0, bcode(v[1]))
            m.emit(0x66)
            m.mrsd(lcode(v[2]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VFPCLASSPS imm8, xmm, k{k}
    if isImm8(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d)
            m.emit(kcode(v[2]) | 0x08)
            m.emit(0x66)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VFPCLASSPS imm8, ymm, k{k}
    if isImm8(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d)
            m.emit(kcode(v[2]) | 0x28)
            m.emit(0x66)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VFPCLASSPS imm8, zmm, k{k}
    if isImm8(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d)
            m.emit(kcode(v[2]) | 0x48)
            m.emit(0x66)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFPCLASSPS")
    }
    return p
}

// VFPCLASSSD performs "Test Class of Scalar Double-Precision Floating-Point Value".
//
// Mnemonic        : VFPCLASSSD
// ISA extensions  : AVX512DQ
// Supported forms : (2 forms)
//
//    * VFPCLASSSD imm8, xmm, k{k}
//    * VFPCLASSSD imm8, m64, k{k}
//
func (self *Program) VFPCLASSSD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VFPCLASSSD imm8, xmm, k{k}
    if isImm8(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd)
            m.emit(kcode(v[2]) | 0x08)
            m.emit(0x67)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VFPCLASSSD imm8, m64, k{k}
    if isImm8(v0) && isM64(v1) && isKk(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b00, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), 0, 0)
            m.emit(0x67)
            m.mrsd(lcode(v[2]), addr(v[1]), 8)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFPCLASSSD")
    }
    return p
}

// VFPCLASSSS performs "Test Class of Scalar Single-Precision Floating-Point Value".
//
// Mnemonic        : VFPCLASSSS
// ISA extensions  : AVX512DQ
// Supported forms : (2 forms)
//
//    * VFPCLASSSS imm8, xmm, k{k}
//    * VFPCLASSSS imm8, m32, k{k}
//
func (self *Program) VFPCLASSSS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VFPCLASSSS imm8, xmm, k{k}
    if isImm8(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d)
            m.emit(kcode(v[2]) | 0x08)
            m.emit(0x67)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VFPCLASSSS imm8, m32, k{k}
    if isImm8(v0) && isM32(v1) && isKk(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b00, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), 0, 0)
            m.emit(0x67)
            m.mrsd(lcode(v[2]), addr(v[1]), 4)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFPCLASSSS")
    }
    return p
}

// VFRCZPD performs "Extract Fraction Packed Double-Precision Floating-Point".
//
// Mnemonic        : VFRCZPD
// ISA extensions  : XOP
// Supported forms : (4 forms)
//
//    * VFRCZPD xmm, xmm
//    * VFRCZPD m128, xmm
//    * VFRCZPD ymm, ymm
//    * VFRCZPD m256, ymm
//
func (self *Program) VFRCZPD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VFRCZPD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x78)
            m.emit(0x81)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VFRCZPD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x81)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VFRCZPD ymm, ymm
    if isYMM(v0) && isYMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7c)
            m.emit(0x81)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VFRCZPD m256, ymm
    if isM256(v0) && isYMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x04, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x81)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFRCZPD")
    }
    return p
}

// VFRCZPS performs "Extract Fraction Packed Single-Precision Floating-Point".
//
// Mnemonic        : VFRCZPS
// ISA extensions  : XOP
// Supported forms : (4 forms)
//
//    * VFRCZPS xmm, xmm
//    * VFRCZPS m128, xmm
//    * VFRCZPS ymm, ymm
//    * VFRCZPS m256, ymm
//
func (self *Program) VFRCZPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VFRCZPS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x78)
            m.emit(0x80)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VFRCZPS m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x80)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VFRCZPS ymm, ymm
    if isYMM(v0) && isYMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7c)
            m.emit(0x80)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VFRCZPS m256, ymm
    if isM256(v0) && isYMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x04, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x80)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFRCZPS")
    }
    return p
}

// VFRCZSD performs "Extract Fraction Scalar Double-Precision Floating-Point".
//
// Mnemonic        : VFRCZSD
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VFRCZSD xmm, xmm
//    * VFRCZSD m64, xmm
//
func (self *Program) VFRCZSD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VFRCZSD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x78)
            m.emit(0x83)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VFRCZSD m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x83)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFRCZSD")
    }
    return p
}

// VFRCZSS performs "Extract Fraction Scalar Single-Precision Floating Point".
//
// Mnemonic        : VFRCZSS
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VFRCZSS xmm, xmm
//    * VFRCZSS m32, xmm
//
func (self *Program) VFRCZSS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VFRCZSS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x78)
            m.emit(0x82)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VFRCZSS m32, xmm
    if isM32(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x82)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VFRCZSS")
    }
    return p
}

// VGATHERDPD performs "Gather Packed Double-Precision Floating-Point Values Using Signed Doubleword Indices".
//
// Mnemonic        : VGATHERDPD
// ISA extensions  : AVX2, AVX512F, AVX512VL
// Supported forms : (5 forms)
//
//    * VGATHERDPD vm32x, xmm{k}
//    * VGATHERDPD vm32x, ymm{k}
//    * VGATHERDPD vm32y, zmm{k}
//    * VGATHERDPD xmm, vm32x, xmm
//    * VGATHERDPD ymm, vm32x, ymm
//
func (self *Program) VGATHERDPD(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VGATHERDPD takes 2 or 3 operands")
    }
    // VGATHERDPD vm32x, xmm{k}
    if len(vv) == 0 && isEVEXVMX(v0) && isXMMk(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), 0, 0)
            m.emit(0x92)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VGATHERDPD vm32x, ymm{k}
    if len(vv) == 0 && isEVEXVMX(v0) && isYMMk(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), 0, 0)
            m.emit(0x92)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VGATHERDPD vm32y, zmm{k}
    if len(vv) == 0 && isEVEXVMY(v0) && isZMMk(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), 0, 0)
            m.emit(0x92)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VGATHERDPD xmm, vm32x, xmm
    if len(vv) == 1 && isXMM(v0) && isVMX(v1) && isXMM(vv[0]) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0x92)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    // VGATHERDPD ymm, vm32x, ymm
    if len(vv) == 1 && isYMM(v0) && isVMX(v1) && isYMM(vv[0]) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x85, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0x92)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VGATHERDPD")
    }
    return p
}

// VGATHERDPS performs "Gather Packed Single-Precision Floating-Point Values Using Signed Doubleword Indices".
//
// Mnemonic        : VGATHERDPS
// ISA extensions  : AVX2, AVX512F, AVX512VL
// Supported forms : (5 forms)
//
//    * VGATHERDPS vm32x, xmm{k}
//    * VGATHERDPS vm32y, ymm{k}
//    * VGATHERDPS vm32z, zmm{k}
//    * VGATHERDPS xmm, vm32x, xmm
//    * VGATHERDPS ymm, vm32y, ymm
//
func (self *Program) VGATHERDPS(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VGATHERDPS takes 2 or 3 operands")
    }
    // VGATHERDPS vm32x, xmm{k}
    if len(vv) == 0 && isEVEXVMX(v0) && isXMMk(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), 0, 0)
            m.emit(0x92)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VGATHERDPS vm32y, ymm{k}
    if len(vv) == 0 && isEVEXVMY(v0) && isYMMk(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), 0, 0)
            m.emit(0x92)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VGATHERDPS vm32z, zmm{k}
    if len(vv) == 0 && isVMZ(v0) && isZMMk(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), 0, 0)
            m.emit(0x92)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VGATHERDPS xmm, vm32x, xmm
    if len(vv) == 1 && isXMM(v0) && isVMX(v1) && isXMM(vv[0]) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0x92)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    // VGATHERDPS ymm, vm32y, ymm
    if len(vv) == 1 && isYMM(v0) && isVMY(v1) && isYMM(vv[0]) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0x92)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VGATHERDPS")
    }
    return p
}

// VGATHERPF0DPD performs "Sparse Prefetch Packed Double-Precision Floating-Point Data Values with Signed Doubleword Indices Using T0 Hint".
//
// Mnemonic        : VGATHERPF0DPD
// ISA extensions  : AVX512PF
// Supported forms : (1 form)
//
//    * VGATHERPF0DPD vm32y{k}
//
func (self *Program) VGATHERPF0DPD(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // VGATHERPF0DPD vm32y{k}
    if isVMYk(v0) {
        self.require(ISA_AVX512PF)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, 0, addr(v[0]), 0, kcode(v[0]), 0, 0)
            m.emit(0xc6)
            m.mrsd(1, addr(v[0]), 8)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VGATHERPF0DPD")
    }
    return p
}

// VGATHERPF0DPS performs "Sparse Prefetch Packed Single-Precision Floating-Point Data Values with Signed Doubleword Indices Using T0 Hint".
//
// Mnemonic        : VGATHERPF0DPS
// ISA extensions  : AVX512PF
// Supported forms : (1 form)
//
//    * VGATHERPF0DPS vm32z{k}
//
func (self *Program) VGATHERPF0DPS(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // VGATHERPF0DPS vm32z{k}
    if isVMZk(v0) {
        self.require(ISA_AVX512PF)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, 0, addr(v[0]), 0, kcode(v[0]), 0, 0)
            m.emit(0xc6)
            m.mrsd(1, addr(v[0]), 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VGATHERPF0DPS")
    }
    return p
}

// VGATHERPF0QPD performs "Sparse Prefetch Packed Double-Precision Floating-Point Data Values with Signed Quadword Indices Using T0 Hint".
//
// Mnemonic        : VGATHERPF0QPD
// ISA extensions  : AVX512PF
// Supported forms : (1 form)
//
//    * VGATHERPF0QPD vm64z{k}
//
func (self *Program) VGATHERPF0QPD(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // VGATHERPF0QPD vm64z{k}
    if isVMZk(v0) {
        self.require(ISA_AVX512PF)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, 0, addr(v[0]), 0, kcode(v[0]), 0, 0)
            m.emit(0xc7)
            m.mrsd(1, addr(v[0]), 8)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VGATHERPF0QPD")
    }
    return p
}

// VGATHERPF0QPS performs "Sparse Prefetch Packed Single-Precision Floating-Point Data Values with Signed Quadword Indices Using T0 Hint".
//
// Mnemonic        : VGATHERPF0QPS
// ISA extensions  : AVX512PF
// Supported forms : (1 form)
//
//    * VGATHERPF0QPS vm64z{k}
//
func (self *Program) VGATHERPF0QPS(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // VGATHERPF0QPS vm64z{k}
    if isVMZk(v0) {
        self.require(ISA_AVX512PF)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, 0, addr(v[0]), 0, kcode(v[0]), 0, 0)
            m.emit(0xc7)
            m.mrsd(1, addr(v[0]), 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VGATHERPF0QPS")
    }
    return p
}

// VGATHERPF1DPD performs "Sparse Prefetch Packed Double-Precision Floating-Point Data Values with Signed Doubleword Indices Using T1 Hint".
//
// Mnemonic        : VGATHERPF1DPD
// ISA extensions  : AVX512PF
// Supported forms : (1 form)
//
//    * VGATHERPF1DPD vm32y{k}
//
func (self *Program) VGATHERPF1DPD(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // VGATHERPF1DPD vm32y{k}
    if isVMYk(v0) {
        self.require(ISA_AVX512PF)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, 0, addr(v[0]), 0, kcode(v[0]), 0, 0)
            m.emit(0xc6)
            m.mrsd(2, addr(v[0]), 8)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VGATHERPF1DPD")
    }
    return p
}

// VGATHERPF1DPS performs "Sparse Prefetch Packed Single-Precision Floating-Point Data Values with Signed Doubleword Indices Using T1 Hint".
//
// Mnemonic        : VGATHERPF1DPS
// ISA extensions  : AVX512PF
// Supported forms : (1 form)
//
//    * VGATHERPF1DPS vm32z{k}
//
func (self *Program) VGATHERPF1DPS(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // VGATHERPF1DPS vm32z{k}
    if isVMZk(v0) {
        self.require(ISA_AVX512PF)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, 0, addr(v[0]), 0, kcode(v[0]), 0, 0)
            m.emit(0xc6)
            m.mrsd(2, addr(v[0]), 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VGATHERPF1DPS")
    }
    return p
}

// VGATHERPF1QPD performs "Sparse Prefetch Packed Double-Precision Floating-Point Data Values with Signed Quadword Indices Using T1 Hint".
//
// Mnemonic        : VGATHERPF1QPD
// ISA extensions  : AVX512PF
// Supported forms : (1 form)
//
//    * VGATHERPF1QPD vm64z{k}
//
func (self *Program) VGATHERPF1QPD(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // VGATHERPF1QPD vm64z{k}
    if isVMZk(v0) {
        self.require(ISA_AVX512PF)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, 0, addr(v[0]), 0, kcode(v[0]), 0, 0)
            m.emit(0xc7)
            m.mrsd(2, addr(v[0]), 8)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VGATHERPF1QPD")
    }
    return p
}

// VGATHERPF1QPS performs "Sparse Prefetch Packed Single-Precision Floating-Point Data Values with Signed Quadword Indices Using T1 Hint".
//
// Mnemonic        : VGATHERPF1QPS
// ISA extensions  : AVX512PF
// Supported forms : (1 form)
//
//    * VGATHERPF1QPS vm64z{k}
//
func (self *Program) VGATHERPF1QPS(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // VGATHERPF1QPS vm64z{k}
    if isVMZk(v0) {
        self.require(ISA_AVX512PF)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, 0, addr(v[0]), 0, kcode(v[0]), 0, 0)
            m.emit(0xc7)
            m.mrsd(2, addr(v[0]), 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VGATHERPF1QPS")
    }
    return p
}

// VGATHERQPD performs "Gather Packed Double-Precision Floating-Point Values Using Signed Quadword Indices".
//
// Mnemonic        : VGATHERQPD
// ISA extensions  : AVX2, AVX512F, AVX512VL
// Supported forms : (5 forms)
//
//    * VGATHERQPD vm64x, xmm{k}
//    * VGATHERQPD vm64y, ymm{k}
//    * VGATHERQPD vm64z, zmm{k}
//    * VGATHERQPD xmm, vm64x, xmm
//    * VGATHERQPD ymm, vm64y, ymm
//
func (self *Program) VGATHERQPD(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VGATHERQPD takes 2 or 3 operands")
    }
    // VGATHERQPD vm64x, xmm{k}
    if len(vv) == 0 && isEVEXVMX(v0) && isXMMk(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), 0, 0)
            m.emit(0x93)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VGATHERQPD vm64y, ymm{k}
    if len(vv) == 0 && isEVEXVMY(v0) && isYMMk(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), 0, 0)
            m.emit(0x93)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VGATHERQPD vm64z, zmm{k}
    if len(vv) == 0 && isVMZ(v0) && isZMMk(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), 0, 0)
            m.emit(0x93)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VGATHERQPD xmm, vm64x, xmm
    if len(vv) == 1 && isXMM(v0) && isVMX(v1) && isXMM(vv[0]) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0x93)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    // VGATHERQPD ymm, vm64y, ymm
    if len(vv) == 1 && isYMM(v0) && isVMY(v1) && isYMM(vv[0]) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x85, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0x93)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VGATHERQPD")
    }
    return p
}

// VGATHERQPS performs "Gather Packed Single-Precision Floating-Point Values Using Signed Quadword Indices".
//
// Mnemonic        : VGATHERQPS
// ISA extensions  : AVX2, AVX512F, AVX512VL
// Supported forms : (5 forms)
//
//    * VGATHERQPS vm64x, xmm{k}
//    * VGATHERQPS vm64y, xmm{k}
//    * VGATHERQPS vm64z, ymm{k}
//    * VGATHERQPS xmm, vm64x, xmm
//    * VGATHERQPS xmm, vm64y, xmm
//
func (self *Program) VGATHERQPS(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VGATHERQPS takes 2 or 3 operands")
    }
    // VGATHERQPS vm64x, xmm{k}
    if len(vv) == 0 && isEVEXVMX(v0) && isXMMk(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), 0, 0)
            m.emit(0x93)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VGATHERQPS vm64y, xmm{k}
    if len(vv) == 0 && isEVEXVMY(v0) && isXMMk(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), 0, 0)
            m.emit(0x93)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VGATHERQPS vm64z, ymm{k}
    if len(vv) == 0 && isVMZ(v0) && isYMMk(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), 0, 0)
            m.emit(0x93)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VGATHERQPS xmm, vm64x, xmm
    if len(vv) == 1 && isXMM(v0) && isVMX(v1) && isXMM(vv[0]) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0x93)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    // VGATHERQPS xmm, vm64y, xmm
    if len(vv) == 1 && isXMM(v0) && isVMY(v1) && isXMM(vv[0]) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0x93)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VGATHERQPS")
    }
    return p
}

// VGETEXPPD performs "Extract Exponents of Packed Double-Precision Floating-Point Values as Double-Precision Floating-Point Values".
//
// Mnemonic        : VGETEXPPD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (7 forms)
//
//    * VGETEXPPD m128/m64bcst, xmm{k}{z}
//    * VGETEXPPD m256/m64bcst, ymm{k}{z}
//    * VGETEXPPD m512/m64bcst, zmm{k}{z}
//    * VGETEXPPD xmm, xmm{k}{z}
//    * VGETEXPPD ymm, ymm{k}{z}
//    * VGETEXPPD {sae}, zmm, zmm{k}{z}
//    * VGETEXPPD zmm, zmm{k}{z}
//
func (self *Program) VGETEXPPD(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VGETEXPPD takes 2 or 3 operands")
    }
    // VGETEXPPD m128/m64bcst, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x42)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VGETEXPPD m256/m64bcst, ymm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x42)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VGETEXPPD m512/m64bcst, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x42)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VGETEXPPD xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x42)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VGETEXPPD ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x42)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VGETEXPPD {sae}, zmm, zmm{k}{z}
    if len(vv) == 1 && isSAE(v0) && isZMM(v1) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x18)
            m.emit(0x42)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VGETEXPPD zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x42)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VGETEXPPD")
    }
    return p
}

// VGETEXPPS performs "Extract Exponents of Packed Single-Precision Floating-Point Values as Single-Precision Floating-Point Values".
//
// Mnemonic        : VGETEXPPS
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (7 forms)
//
//    * VGETEXPPS m128/m32bcst, xmm{k}{z}
//    * VGETEXPPS m256/m32bcst, ymm{k}{z}
//    * VGETEXPPS m512/m32bcst, zmm{k}{z}
//    * VGETEXPPS xmm, xmm{k}{z}
//    * VGETEXPPS ymm, ymm{k}{z}
//    * VGETEXPPS {sae}, zmm, zmm{k}{z}
//    * VGETEXPPS zmm, zmm{k}{z}
//
func (self *Program) VGETEXPPS(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VGETEXPPS takes 2 or 3 operands")
    }
    // VGETEXPPS m128/m32bcst, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x42)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VGETEXPPS m256/m32bcst, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x42)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VGETEXPPS m512/m32bcst, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x42)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VGETEXPPS xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x42)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VGETEXPPS ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x42)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VGETEXPPS {sae}, zmm, zmm{k}{z}
    if len(vv) == 1 && isSAE(v0) && isZMM(v1) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x18)
            m.emit(0x42)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VGETEXPPS zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x42)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VGETEXPPS")
    }
    return p
}

// VGETEXPSD performs "Extract Exponent of Scalar Double-Precision Floating-Point Value as Double-Precision Floating-Point Value".
//
// Mnemonic        : VGETEXPSD
// ISA extensions  : AVX512F
// Supported forms : (3 forms)
//
//    * VGETEXPSD m64, xmm, xmm{k}{z}
//    * VGETEXPSD {sae}, xmm, xmm, xmm{k}{z}
//    * VGETEXPSD xmm, xmm, xmm{k}{z}
//
func (self *Program) VGETEXPSD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VGETEXPSD takes 3 or 4 operands")
    }
    // VGETEXPSD m64, xmm, xmm{k}{z}
    if len(vv) == 0 && isM64(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x43)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    // VGETEXPSD {sae}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isSAE(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x43)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VGETEXPSD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x43)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VGETEXPSD")
    }
    return p
}

// VGETEXPSS performs "Extract Exponent of Scalar Single-Precision Floating-Point Value as Single-Precision Floating-Point Value".
//
// Mnemonic        : VGETEXPSS
// ISA extensions  : AVX512F
// Supported forms : (3 forms)
//
//    * VGETEXPSS m32, xmm, xmm{k}{z}
//    * VGETEXPSS {sae}, xmm, xmm, xmm{k}{z}
//    * VGETEXPSS xmm, xmm, xmm{k}{z}
//
func (self *Program) VGETEXPSS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VGETEXPSS takes 3 or 4 operands")
    }
    // VGETEXPSS m32, xmm, xmm{k}{z}
    if len(vv) == 0 && isM32(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x43)
            m.mrsd(lcode(v[2]), addr(v[0]), 4)
        })
    }
    // VGETEXPSS {sae}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isSAE(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x43)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VGETEXPSS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x43)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VGETEXPSS")
    }
    return p
}

// VGETMANTPD performs "Extract Normalized Mantissas from Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VGETMANTPD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (7 forms)
//
//    * VGETMANTPD imm8, m128/m64bcst, xmm{k}{z}
//    * VGETMANTPD imm8, m256/m64bcst, ymm{k}{z}
//    * VGETMANTPD imm8, m512/m64bcst, zmm{k}{z}
//    * VGETMANTPD imm8, xmm, xmm{k}{z}
//    * VGETMANTPD imm8, ymm, ymm{k}{z}
//    * VGETMANTPD imm8, {sae}, zmm, zmm{k}{z}
//    * VGETMANTPD imm8, zmm, zmm{k}{z}
//
func (self *Program) VGETMANTPD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VGETMANTPD takes 3 or 4 operands")
    }
    // VGETMANTPD imm8, m128/m64bcst, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isM128M64bcst(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b00, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x26)
            m.mrsd(lcode(v[2]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VGETMANTPD imm8, m256/m64bcst, ymm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isM256M64bcst(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b01, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x26)
            m.mrsd(lcode(v[2]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VGETMANTPD imm8, m512/m64bcst, zmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isM512M64bcst(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b10, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x26)
            m.mrsd(lcode(v[2]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VGETMANTPD imm8, xmm, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x08)
            m.emit(0x26)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VGETMANTPD imm8, ymm, ymm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x28)
            m.emit(0x26)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VGETMANTPD imm8, {sae}, zmm, zmm{k}{z}
    if len(vv) == 1 && isImm8(v0) && isSAE(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[3]) << 7) | kcode(v[3]) | 0x18)
            m.emit(0x26)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VGETMANTPD imm8, zmm, zmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x48)
            m.emit(0x26)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VGETMANTPD")
    }
    return p
}

// VGETMANTPS performs "Extract Normalized Mantissas from Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VGETMANTPS
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (7 forms)
//
//    * VGETMANTPS imm8, m128/m32bcst, xmm{k}{z}
//    * VGETMANTPS imm8, m256/m32bcst, ymm{k}{z}
//    * VGETMANTPS imm8, m512/m32bcst, zmm{k}{z}
//    * VGETMANTPS imm8, xmm, xmm{k}{z}
//    * VGETMANTPS imm8, ymm, ymm{k}{z}
//    * VGETMANTPS imm8, {sae}, zmm, zmm{k}{z}
//    * VGETMANTPS imm8, zmm, zmm{k}{z}
//
func (self *Program) VGETMANTPS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VGETMANTPS takes 3 or 4 operands")
    }
    // VGETMANTPS imm8, m128/m32bcst, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isM128M32bcst(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b00, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x26)
            m.mrsd(lcode(v[2]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VGETMANTPS imm8, m256/m32bcst, ymm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isM256M32bcst(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b01, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x26)
            m.mrsd(lcode(v[2]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VGETMANTPS imm8, m512/m32bcst, zmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isM512M32bcst(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b10, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x26)
            m.mrsd(lcode(v[2]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VGETMANTPS imm8, xmm, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x08)
            m.emit(0x26)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VGETMANTPS imm8, ymm, ymm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x28)
            m.emit(0x26)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VGETMANTPS imm8, {sae}, zmm, zmm{k}{z}
    if len(vv) == 1 && isImm8(v0) && isSAE(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[3]) << 7) | kcode(v[3]) | 0x18)
            m.emit(0x26)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VGETMANTPS imm8, zmm, zmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x48)
            m.emit(0x26)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VGETMANTPS")
    }
    return p
}

// VGETMANTSD performs "Extract Normalized Mantissa from Scalar Double-Precision Floating-Point Value".
//
// Mnemonic        : VGETMANTSD
// ISA extensions  : AVX512F
// Supported forms : (3 forms)
//
//    * VGETMANTSD imm8, m64, xmm, xmm{k}{z}
//    * VGETMANTSD imm8, {sae}, xmm, xmm, xmm{k}{z}
//    * VGETMANTSD imm8, xmm, xmm, xmm{k}{z}
//
func (self *Program) VGETMANTSD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(4, Operands{v0, v1, v2, v3})
        case 1  : p = self.alloc(5, Operands{v0, v1, v2, v3, vv[0]})
        default : panic("instruction VGETMANTSD takes 4 or 5 operands")
    }
    // VGETMANTSD imm8, m64, xmm, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isM64(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), 0)
            m.emit(0x27)
            m.mrsd(lcode(v[3]), addr(v[1]), 8)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VGETMANTSD imm8, {sae}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isImm8(v0) && isSAE(v1) && isEVEXXMM(v2) && isEVEXXMM(v3) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[4]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[4]) << 4)))
            m.emit(0xfd ^ (hlcode(v[3]) << 3))
            m.emit((zcode(v[4]) << 7) | (0x08 ^ (ecode(v[3]) << 3)) | kcode(v[4]) | 0x10)
            m.emit(0x27)
            m.emit(0xc0 | lcode(v[4]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VGETMANTSD imm8, xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x27)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VGETMANTSD")
    }
    return p
}

// VGETMANTSS performs "Extract Normalized Mantissa from Scalar Single-Precision Floating-Point Value".
//
// Mnemonic        : VGETMANTSS
// ISA extensions  : AVX512F
// Supported forms : (3 forms)
//
//    * VGETMANTSS imm8, m32, xmm, xmm{k}{z}
//    * VGETMANTSS imm8, {sae}, xmm, xmm, xmm{k}{z}
//    * VGETMANTSS imm8, xmm, xmm, xmm{k}{z}
//
func (self *Program) VGETMANTSS(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(4, Operands{v0, v1, v2, v3})
        case 1  : p = self.alloc(5, Operands{v0, v1, v2, v3, vv[0]})
        default : panic("instruction VGETMANTSS takes 4 or 5 operands")
    }
    // VGETMANTSS imm8, m32, xmm, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isM32(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), 0)
            m.emit(0x27)
            m.mrsd(lcode(v[3]), addr(v[1]), 4)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VGETMANTSS imm8, {sae}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isImm8(v0) && isSAE(v1) && isEVEXXMM(v2) && isEVEXXMM(v3) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[4]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[4]) << 4)))
            m.emit(0x7d ^ (hlcode(v[3]) << 3))
            m.emit((zcode(v[4]) << 7) | (0x08 ^ (ecode(v[3]) << 3)) | kcode(v[4]) | 0x10)
            m.emit(0x27)
            m.emit(0xc0 | lcode(v[4]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VGETMANTSS imm8, xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x27)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VGETMANTSS")
    }
    return p
}

// VHADDPD performs "Packed Double-FP Horizontal Add".
//
// Mnemonic        : VHADDPD
// ISA extensions  : AVX
// Supported forms : (4 forms)
//
//    * VHADDPD xmm, xmm, xmm
//    * VHADDPD m128, xmm, xmm
//    * VHADDPD ymm, ymm, ymm
//    * VHADDPD m256, ymm, ymm
//
func (self *Program) VHADDPD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VHADDPD xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x7c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VHADDPD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x7c)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VHADDPD ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x7c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VHADDPD m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x7c)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VHADDPD")
    }
    return p
}

// VHADDPS performs "Packed Single-FP Horizontal Add".
//
// Mnemonic        : VHADDPS
// ISA extensions  : AVX
// Supported forms : (4 forms)
//
//    * VHADDPS xmm, xmm, xmm
//    * VHADDPS m128, xmm, xmm
//    * VHADDPS ymm, ymm, ymm
//    * VHADDPS m256, ymm, ymm
//
func (self *Program) VHADDPS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VHADDPS xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x7c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VHADDPS m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x7c)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VHADDPS ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(7, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x7c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VHADDPS m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(7, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x7c)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VHADDPS")
    }
    return p
}

// VHSUBPD performs "Packed Double-FP Horizontal Subtract".
//
// Mnemonic        : VHSUBPD
// ISA extensions  : AVX
// Supported forms : (4 forms)
//
//    * VHSUBPD xmm, xmm, xmm
//    * VHSUBPD m128, xmm, xmm
//    * VHSUBPD ymm, ymm, ymm
//    * VHSUBPD m256, ymm, ymm
//
func (self *Program) VHSUBPD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VHSUBPD xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x7d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VHSUBPD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x7d)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VHSUBPD ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x7d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VHSUBPD m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x7d)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VHSUBPD")
    }
    return p
}

// VHSUBPS performs "Packed Single-FP Horizontal Subtract".
//
// Mnemonic        : VHSUBPS
// ISA extensions  : AVX
// Supported forms : (4 forms)
//
//    * VHSUBPS xmm, xmm, xmm
//    * VHSUBPS m128, xmm, xmm
//    * VHSUBPS ymm, ymm, ymm
//    * VHSUBPS m256, ymm, ymm
//
func (self *Program) VHSUBPS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VHSUBPS xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x7d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VHSUBPS m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x7d)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VHSUBPS ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(7, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x7d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VHSUBPS m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(7, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x7d)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VHSUBPS")
    }
    return p
}

// VINSERTF128 performs "Insert Packed Floating-Point Values".
//
// Mnemonic        : VINSERTF128
// ISA extensions  : AVX
// Supported forms : (2 forms)
//
//    * VINSERTF128 imm8, xmm, ymm, ymm
//    * VINSERTF128 imm8, m128, ymm, ymm
//
func (self *Program) VINSERTF128(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VINSERTF128 imm8, xmm, ymm, ymm
    if isImm8(v0) && isXMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit(0x18)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VINSERTF128 imm8, m128, ymm, ymm
    if isImm8(v0) && isM128(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x18)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VINSERTF128")
    }
    return p
}

// VINSERTF32X4 performs "Insert 128 Bits of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VINSERTF32X4
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (4 forms)
//
//    * VINSERTF32X4 imm8, xmm, ymm, ymm{k}{z}
//    * VINSERTF32X4 imm8, m128, ymm, ymm{k}{z}
//    * VINSERTF32X4 imm8, xmm, zmm, zmm{k}{z}
//    * VINSERTF32X4 imm8, m128, zmm, zmm{k}{z}
//
func (self *Program) VINSERTF32X4(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VINSERTF32X4 imm8, xmm, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x20)
            m.emit(0x18)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VINSERTF32X4 imm8, m128, ymm, ymm{k}{z}
    if isImm8(v0) && isM128(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b01, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), 0)
            m.emit(0x18)
            m.mrsd(lcode(v[3]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VINSERTF32X4 imm8, xmm, zmm, zmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x18)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VINSERTF32X4 imm8, m128, zmm, zmm{k}{z}
    if isImm8(v0) && isM128(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), 0)
            m.emit(0x18)
            m.mrsd(lcode(v[3]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VINSERTF32X4")
    }
    return p
}

// VINSERTF32X8 performs "Insert 256 Bits of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VINSERTF32X8
// ISA extensions  : AVX512DQ
// Supported forms : (2 forms)
//
//    * VINSERTF32X8 imm8, ymm, zmm, zmm{k}{z}
//    * VINSERTF32X8 imm8, m256, zmm, zmm{k}{z}
//
func (self *Program) VINSERTF32X8(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VINSERTF32X8 imm8, ymm, zmm, zmm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x1a)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VINSERTF32X8 imm8, m256, zmm, zmm{k}{z}
    if isImm8(v0) && isM256(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), 0)
            m.emit(0x1a)
            m.mrsd(lcode(v[3]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VINSERTF32X8")
    }
    return p
}

// VINSERTF64X2 performs "Insert 128 Bits of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VINSERTF64X2
// ISA extensions  : AVX512DQ, AVX512VL
// Supported forms : (4 forms)
//
//    * VINSERTF64X2 imm8, xmm, ymm, ymm{k}{z}
//    * VINSERTF64X2 imm8, m128, ymm, ymm{k}{z}
//    * VINSERTF64X2 imm8, xmm, zmm, zmm{k}{z}
//    * VINSERTF64X2 imm8, m128, zmm, zmm{k}{z}
//
func (self *Program) VINSERTF64X2(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VINSERTF64X2 imm8, xmm, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x20)
            m.emit(0x18)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VINSERTF64X2 imm8, m128, ymm, ymm{k}{z}
    if isImm8(v0) && isM128(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b01, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), 0)
            m.emit(0x18)
            m.mrsd(lcode(v[3]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VINSERTF64X2 imm8, xmm, zmm, zmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x18)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VINSERTF64X2 imm8, m128, zmm, zmm{k}{z}
    if isImm8(v0) && isM128(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), 0)
            m.emit(0x18)
            m.mrsd(lcode(v[3]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VINSERTF64X2")
    }
    return p
}

// VINSERTF64X4 performs "Insert 256 Bits of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VINSERTF64X4
// ISA extensions  : AVX512F
// Supported forms : (2 forms)
//
//    * VINSERTF64X4 imm8, ymm, zmm, zmm{k}{z}
//    * VINSERTF64X4 imm8, m256, zmm, zmm{k}{z}
//
func (self *Program) VINSERTF64X4(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VINSERTF64X4 imm8, ymm, zmm, zmm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x1a)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VINSERTF64X4 imm8, m256, zmm, zmm{k}{z}
    if isImm8(v0) && isM256(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), 0)
            m.emit(0x1a)
            m.mrsd(lcode(v[3]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VINSERTF64X4")
    }
    return p
}

// VINSERTI128 performs "Insert Packed Integer Values".
//
// Mnemonic        : VINSERTI128
// ISA extensions  : AVX2
// Supported forms : (2 forms)
//
//    * VINSERTI128 imm8, xmm, ymm, ymm
//    * VINSERTI128 imm8, m128, ymm, ymm
//
func (self *Program) VINSERTI128(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VINSERTI128 imm8, xmm, ymm, ymm
    if isImm8(v0) && isXMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit(0x38)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VINSERTI128 imm8, m128, ymm, ymm
    if isImm8(v0) && isM128(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x38)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VINSERTI128")
    }
    return p
}

// VINSERTI32X4 performs "Insert 128 Bits of Packed Doubleword Integer Values".
//
// Mnemonic        : VINSERTI32X4
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (4 forms)
//
//    * VINSERTI32X4 imm8, xmm, ymm, ymm{k}{z}
//    * VINSERTI32X4 imm8, m128, ymm, ymm{k}{z}
//    * VINSERTI32X4 imm8, xmm, zmm, zmm{k}{z}
//    * VINSERTI32X4 imm8, m128, zmm, zmm{k}{z}
//
func (self *Program) VINSERTI32X4(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VINSERTI32X4 imm8, xmm, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x20)
            m.emit(0x38)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VINSERTI32X4 imm8, m128, ymm, ymm{k}{z}
    if isImm8(v0) && isM128(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b01, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), 0)
            m.emit(0x38)
            m.mrsd(lcode(v[3]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VINSERTI32X4 imm8, xmm, zmm, zmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x38)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VINSERTI32X4 imm8, m128, zmm, zmm{k}{z}
    if isImm8(v0) && isM128(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), 0)
            m.emit(0x38)
            m.mrsd(lcode(v[3]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VINSERTI32X4")
    }
    return p
}

// VINSERTI32X8 performs "Insert 256 Bits of Packed Doubleword Integer Values".
//
// Mnemonic        : VINSERTI32X8
// ISA extensions  : AVX512DQ
// Supported forms : (2 forms)
//
//    * VINSERTI32X8 imm8, ymm, zmm, zmm{k}{z}
//    * VINSERTI32X8 imm8, m256, zmm, zmm{k}{z}
//
func (self *Program) VINSERTI32X8(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VINSERTI32X8 imm8, ymm, zmm, zmm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x3a)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VINSERTI32X8 imm8, m256, zmm, zmm{k}{z}
    if isImm8(v0) && isM256(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), 0)
            m.emit(0x3a)
            m.mrsd(lcode(v[3]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VINSERTI32X8")
    }
    return p
}

// VINSERTI64X2 performs "Insert 128 Bits of Packed Quadword Integer Values".
//
// Mnemonic        : VINSERTI64X2
// ISA extensions  : AVX512DQ, AVX512VL
// Supported forms : (4 forms)
//
//    * VINSERTI64X2 imm8, xmm, ymm, ymm{k}{z}
//    * VINSERTI64X2 imm8, m128, ymm, ymm{k}{z}
//    * VINSERTI64X2 imm8, xmm, zmm, zmm{k}{z}
//    * VINSERTI64X2 imm8, m128, zmm, zmm{k}{z}
//
func (self *Program) VINSERTI64X2(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VINSERTI64X2 imm8, xmm, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x20)
            m.emit(0x38)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VINSERTI64X2 imm8, m128, ymm, ymm{k}{z}
    if isImm8(v0) && isM128(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b01, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), 0)
            m.emit(0x38)
            m.mrsd(lcode(v[3]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VINSERTI64X2 imm8, xmm, zmm, zmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x38)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VINSERTI64X2 imm8, m128, zmm, zmm{k}{z}
    if isImm8(v0) && isM128(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), 0)
            m.emit(0x38)
            m.mrsd(lcode(v[3]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VINSERTI64X2")
    }
    return p
}

// VINSERTI64X4 performs "Insert 256 Bits of Packed Quadword Integer Values".
//
// Mnemonic        : VINSERTI64X4
// ISA extensions  : AVX512F
// Supported forms : (2 forms)
//
//    * VINSERTI64X4 imm8, ymm, zmm, zmm{k}{z}
//    * VINSERTI64X4 imm8, m256, zmm, zmm{k}{z}
//
func (self *Program) VINSERTI64X4(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VINSERTI64X4 imm8, ymm, zmm, zmm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x3a)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VINSERTI64X4 imm8, m256, zmm, zmm{k}{z}
    if isImm8(v0) && isM256(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), 0)
            m.emit(0x3a)
            m.mrsd(lcode(v[3]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VINSERTI64X4")
    }
    return p
}

// VINSERTPS performs "Insert Packed Single Precision Floating-Point Value".
//
// Mnemonic        : VINSERTPS
// ISA extensions  : AVX, AVX512F
// Supported forms : (4 forms)
//
//    * VINSERTPS imm8, xmm, xmm, xmm
//    * VINSERTPS imm8, xmm, xmm, xmm
//    * VINSERTPS imm8, m32, xmm, xmm
//    * VINSERTPS imm8, m32, xmm, xmm
//
func (self *Program) VINSERTPS(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VINSERTPS imm8, xmm, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x21)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VINSERTPS imm8, xmm, xmm, xmm
    if isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isEVEXXMM(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | 0x00)
            m.emit(0x21)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VINSERTPS imm8, m32, xmm, xmm
    if isImm8(v0) && isM32(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x21)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VINSERTPS imm8, m32, xmm, xmm
    if isImm8(v0) && isM32(v1) && isEVEXXMM(v2) && isEVEXXMM(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), 0, 0, 0)
            m.emit(0x21)
            m.mrsd(lcode(v[3]), addr(v[1]), 4)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VINSERTPS")
    }
    return p
}

// VLDDQU performs "Load Unaligned Integer 128 Bits".
//
// Mnemonic        : VLDDQU
// ISA extensions  : AVX
// Supported forms : (2 forms)
//
//    * VLDDQU m128, xmm
//    * VLDDQU m256, ymm
//
func (self *Program) VLDDQU(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VLDDQU m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[1]), addr(v[0]), 0)
            m.emit(0xf0)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VLDDQU m256, ymm
    if isM256(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(7, hcode(v[1]), addr(v[0]), 0)
            m.emit(0xf0)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VLDDQU")
    }
    return p
}

// VLDMXCSR performs "Load MXCSR Register".
//
// Mnemonic        : VLDMXCSR
// ISA extensions  : AVX
// Supported forms : (1 form)
//
//    * VLDMXCSR m32
//
func (self *Program) VLDMXCSR(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // VLDMXCSR m32
    if isM32(v0) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, 0, addr(v[0]), 0)
            m.emit(0xae)
            m.mrsd(2, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VLDMXCSR")
    }
    return p
}

// VMASKMOVDQU performs "Store Selected Bytes of Double Quadword".
//
// Mnemonic        : VMASKMOVDQU
// ISA extensions  : AVX
// Supported forms : (1 form)
//
//    * VMASKMOVDQU xmm, xmm
//
func (self *Program) VMASKMOVDQU(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VMASKMOVDQU xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[1]), v[0], 0)
            m.emit(0xf7)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMASKMOVDQU")
    }
    return p
}

// VMASKMOVPD performs "Conditional Move Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VMASKMOVPD
// ISA extensions  : AVX
// Supported forms : (4 forms)
//
//    * VMASKMOVPD m128, xmm, xmm
//    * VMASKMOVPD m256, ymm, ymm
//    * VMASKMOVPD xmm, xmm, m128
//    * VMASKMOVPD ymm, ymm, m256
//
func (self *Program) VMASKMOVPD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VMASKMOVPD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x2d)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VMASKMOVPD m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x2d)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VMASKMOVPD xmm, xmm, m128
    if isXMM(v0) && isXMM(v1) && isM128(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[0]), addr(v[2]), hlcode(v[1]))
            m.emit(0x2f)
            m.mrsd(lcode(v[0]), addr(v[2]), 1)
        })
    }
    // VMASKMOVPD ymm, ymm, m256
    if isYMM(v0) && isYMM(v1) && isM256(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[0]), addr(v[2]), hlcode(v[1]))
            m.emit(0x2f)
            m.mrsd(lcode(v[0]), addr(v[2]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMASKMOVPD")
    }
    return p
}

// VMASKMOVPS performs "Conditional Move Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VMASKMOVPS
// ISA extensions  : AVX
// Supported forms : (4 forms)
//
//    * VMASKMOVPS m128, xmm, xmm
//    * VMASKMOVPS m256, ymm, ymm
//    * VMASKMOVPS xmm, xmm, m128
//    * VMASKMOVPS ymm, ymm, m256
//
func (self *Program) VMASKMOVPS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VMASKMOVPS m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x2c)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VMASKMOVPS m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x2c)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VMASKMOVPS xmm, xmm, m128
    if isXMM(v0) && isXMM(v1) && isM128(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[0]), addr(v[2]), hlcode(v[1]))
            m.emit(0x2e)
            m.mrsd(lcode(v[0]), addr(v[2]), 1)
        })
    }
    // VMASKMOVPS ymm, ymm, m256
    if isYMM(v0) && isYMM(v1) && isM256(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[0]), addr(v[2]), hlcode(v[1]))
            m.emit(0x2e)
            m.mrsd(lcode(v[0]), addr(v[2]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMASKMOVPS")
    }
    return p
}

// VMAXPD performs "Return Maximum Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VMAXPD
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (11 forms)
//
//    * VMAXPD m128/m64bcst, xmm, xmm{k}{z}
//    * VMAXPD xmm, xmm, xmm{k}{z}
//    * VMAXPD m256/m64bcst, ymm, ymm{k}{z}
//    * VMAXPD ymm, ymm, ymm{k}{z}
//    * VMAXPD m512/m64bcst, zmm, zmm{k}{z}
//    * VMAXPD xmm, xmm, xmm
//    * VMAXPD m128, xmm, xmm
//    * VMAXPD ymm, ymm, ymm
//    * VMAXPD m256, ymm, ymm
//    * VMAXPD {sae}, zmm, zmm, zmm{k}{z}
//    * VMAXPD zmm, zmm, zmm{k}{z}
//
func (self *Program) VMAXPD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VMAXPD takes 3 or 4 operands")
    }
    // VMAXPD m128/m64bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x5f)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VMAXPD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x5f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VMAXPD m256/m64bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x5f)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VMAXPD ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x5f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VMAXPD m512/m64bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x5f)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VMAXPD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x5f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VMAXPD m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x5f)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VMAXPD ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x5f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VMAXPD m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x5f)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VMAXPD {sae}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isSAE(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x5f)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VMAXPD zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x5f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMAXPD")
    }
    return p
}

// VMAXPS performs "Return Maximum Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VMAXPS
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (11 forms)
//
//    * VMAXPS m128/m32bcst, xmm, xmm{k}{z}
//    * VMAXPS xmm, xmm, xmm{k}{z}
//    * VMAXPS m256/m32bcst, ymm, ymm{k}{z}
//    * VMAXPS ymm, ymm, ymm{k}{z}
//    * VMAXPS m512/m32bcst, zmm, zmm{k}{z}
//    * VMAXPS xmm, xmm, xmm
//    * VMAXPS m128, xmm, xmm
//    * VMAXPS ymm, ymm, ymm
//    * VMAXPS m256, ymm, ymm
//    * VMAXPS {sae}, zmm, zmm, zmm{k}{z}
//    * VMAXPS zmm, zmm, zmm{k}{z}
//
func (self *Program) VMAXPS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VMAXPS takes 3 or 4 operands")
    }
    // VMAXPS m128/m32bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x5f)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VMAXPS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x5f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VMAXPS m256/m32bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x5f)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VMAXPS ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x5f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VMAXPS m512/m32bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x5f)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VMAXPS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x5f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VMAXPS m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x5f)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VMAXPS ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x5f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VMAXPS m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x5f)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VMAXPS {sae}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isSAE(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7c ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x5f)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VMAXPS zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x5f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMAXPS")
    }
    return p
}

// VMAXSD performs "Return Maximum Scalar Double-Precision Floating-Point Value".
//
// Mnemonic        : VMAXSD
// ISA extensions  : AVX, AVX512F
// Supported forms : (5 forms)
//
//    * VMAXSD m64, xmm, xmm{k}{z}
//    * VMAXSD xmm, xmm, xmm
//    * VMAXSD m64, xmm, xmm
//    * VMAXSD {sae}, xmm, xmm, xmm{k}{z}
//    * VMAXSD xmm, xmm, xmm{k}{z}
//
func (self *Program) VMAXSD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VMAXSD takes 3 or 4 operands")
    }
    // VMAXSD m64, xmm, xmm{k}{z}
    if len(vv) == 0 && isM64(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x87, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x5f)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    // VMAXSD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x5f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VMAXSD m64, xmm, xmm
    if len(vv) == 0 && isM64(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x5f)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VMAXSD {sae}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isSAE(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xff ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x5f)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VMAXSD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xff ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x5f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMAXSD")
    }
    return p
}

// VMAXSS performs "Return Maximum Scalar Single-Precision Floating-Point Value".
//
// Mnemonic        : VMAXSS
// ISA extensions  : AVX, AVX512F
// Supported forms : (5 forms)
//
//    * VMAXSS m32, xmm, xmm{k}{z}
//    * VMAXSS xmm, xmm, xmm
//    * VMAXSS m32, xmm, xmm
//    * VMAXSS {sae}, xmm, xmm, xmm{k}{z}
//    * VMAXSS xmm, xmm, xmm{k}{z}
//
func (self *Program) VMAXSS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VMAXSS takes 3 or 4 operands")
    }
    // VMAXSS m32, xmm, xmm{k}{z}
    if len(vv) == 0 && isM32(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x5f)
            m.mrsd(lcode(v[2]), addr(v[0]), 4)
        })
    }
    // VMAXSS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x5f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VMAXSS m32, xmm, xmm
    if len(vv) == 0 && isM32(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x5f)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VMAXSS {sae}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isSAE(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7e ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x5f)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VMAXSS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7e ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x5f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMAXSS")
    }
    return p
}

// VMINPD performs "Return Minimum Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VMINPD
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (11 forms)
//
//    * VMINPD m128/m64bcst, xmm, xmm{k}{z}
//    * VMINPD xmm, xmm, xmm{k}{z}
//    * VMINPD m256/m64bcst, ymm, ymm{k}{z}
//    * VMINPD ymm, ymm, ymm{k}{z}
//    * VMINPD m512/m64bcst, zmm, zmm{k}{z}
//    * VMINPD xmm, xmm, xmm
//    * VMINPD m128, xmm, xmm
//    * VMINPD ymm, ymm, ymm
//    * VMINPD m256, ymm, ymm
//    * VMINPD {sae}, zmm, zmm, zmm{k}{z}
//    * VMINPD zmm, zmm, zmm{k}{z}
//
func (self *Program) VMINPD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VMINPD takes 3 or 4 operands")
    }
    // VMINPD m128/m64bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x5d)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VMINPD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x5d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VMINPD m256/m64bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x5d)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VMINPD ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x5d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VMINPD m512/m64bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x5d)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VMINPD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x5d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VMINPD m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x5d)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VMINPD ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x5d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VMINPD m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x5d)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VMINPD {sae}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isSAE(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x5d)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VMINPD zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x5d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMINPD")
    }
    return p
}

// VMINPS performs "Return Minimum Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VMINPS
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (11 forms)
//
//    * VMINPS m128/m32bcst, xmm, xmm{k}{z}
//    * VMINPS xmm, xmm, xmm{k}{z}
//    * VMINPS m256/m32bcst, ymm, ymm{k}{z}
//    * VMINPS ymm, ymm, ymm{k}{z}
//    * VMINPS m512/m32bcst, zmm, zmm{k}{z}
//    * VMINPS xmm, xmm, xmm
//    * VMINPS m128, xmm, xmm
//    * VMINPS ymm, ymm, ymm
//    * VMINPS m256, ymm, ymm
//    * VMINPS {sae}, zmm, zmm, zmm{k}{z}
//    * VMINPS zmm, zmm, zmm{k}{z}
//
func (self *Program) VMINPS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VMINPS takes 3 or 4 operands")
    }
    // VMINPS m128/m32bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x5d)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VMINPS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x5d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VMINPS m256/m32bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x5d)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VMINPS ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x5d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VMINPS m512/m32bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x5d)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VMINPS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x5d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VMINPS m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x5d)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VMINPS ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x5d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VMINPS m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x5d)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VMINPS {sae}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isSAE(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7c ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x5d)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VMINPS zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x5d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMINPS")
    }
    return p
}

// VMINSD performs "Return Minimum Scalar Double-Precision Floating-Point Value".
//
// Mnemonic        : VMINSD
// ISA extensions  : AVX, AVX512F
// Supported forms : (5 forms)
//
//    * VMINSD m64, xmm, xmm{k}{z}
//    * VMINSD xmm, xmm, xmm
//    * VMINSD m64, xmm, xmm
//    * VMINSD {sae}, xmm, xmm, xmm{k}{z}
//    * VMINSD xmm, xmm, xmm{k}{z}
//
func (self *Program) VMINSD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VMINSD takes 3 or 4 operands")
    }
    // VMINSD m64, xmm, xmm{k}{z}
    if len(vv) == 0 && isM64(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x87, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x5d)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    // VMINSD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x5d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VMINSD m64, xmm, xmm
    if len(vv) == 0 && isM64(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x5d)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VMINSD {sae}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isSAE(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xff ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x5d)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VMINSD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xff ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x5d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMINSD")
    }
    return p
}

// VMINSS performs "Return Minimum Scalar Single-Precision Floating-Point Value".
//
// Mnemonic        : VMINSS
// ISA extensions  : AVX, AVX512F
// Supported forms : (5 forms)
//
//    * VMINSS m32, xmm, xmm{k}{z}
//    * VMINSS xmm, xmm, xmm
//    * VMINSS m32, xmm, xmm
//    * VMINSS {sae}, xmm, xmm, xmm{k}{z}
//    * VMINSS xmm, xmm, xmm{k}{z}
//
func (self *Program) VMINSS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VMINSS takes 3 or 4 operands")
    }
    // VMINSS m32, xmm, xmm{k}{z}
    if len(vv) == 0 && isM32(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x5d)
            m.mrsd(lcode(v[2]), addr(v[0]), 4)
        })
    }
    // VMINSS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x5d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VMINSS m32, xmm, xmm
    if len(vv) == 0 && isM32(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x5d)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VMINSS {sae}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isSAE(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7e ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x5d)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VMINSS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7e ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x5d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMINSS")
    }
    return p
}

// VMOVAPD performs "Move Aligned Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VMOVAPD
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (15 forms)
//
//    * VMOVAPD xmm, m128{k}{z}
//    * VMOVAPD xmm, xmm{k}{z}
//    * VMOVAPD ymm, m256{k}{z}
//    * VMOVAPD ymm, ymm{k}{z}
//    * VMOVAPD zmm, m512{k}{z}
//    * VMOVAPD zmm, zmm{k}{z}
//    * VMOVAPD m128, xmm{k}{z}
//    * VMOVAPD m256, ymm{k}{z}
//    * VMOVAPD m512, zmm{k}{z}
//    * VMOVAPD xmm, xmm
//    * VMOVAPD m128, xmm
//    * VMOVAPD ymm, ymm
//    * VMOVAPD m256, ymm
//    * VMOVAPD xmm, m128
//    * VMOVAPD ymm, m256
//
func (self *Program) VMOVAPD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VMOVAPD xmm, m128{k}{z}
    if isEVEXXMM(v0) && isM128kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x29)
            m.mrsd(lcode(v[0]), addr(v[1]), 16)
        })
    }
    // VMOVAPD xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x28)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x29)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVAPD ymm, m256{k}{z}
    if isEVEXYMM(v0) && isM256kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x29)
            m.mrsd(lcode(v[0]), addr(v[1]), 32)
        })
    }
    // VMOVAPD ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x28)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x29)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVAPD zmm, m512{k}{z}
    if isZMM(v0) && isM512kz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x29)
            m.mrsd(lcode(v[0]), addr(v[1]), 64)
        })
    }
    // VMOVAPD zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x28)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x29)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVAPD m128, xmm{k}{z}
    if isM128(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x28)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VMOVAPD m256, ymm{k}{z}
    if isM256(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x28)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VMOVAPD m512, zmm{k}{z}
    if isM512(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x28)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VMOVAPD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[1]), v[0], 0)
            m.emit(0x28)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[0]), v[1], 0)
            m.emit(0x29)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVAPD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x28)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VMOVAPD ymm, ymm
    if isYMM(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[1]), v[0], 0)
            m.emit(0x28)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[0]), v[1], 0)
            m.emit(0x29)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVAPD m256, ymm
    if isM256(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x28)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VMOVAPD xmm, m128
    if isXMM(v0) && isM128(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[0]), addr(v[1]), 0)
            m.emit(0x29)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    // VMOVAPD ymm, m256
    if isYMM(v0) && isM256(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[0]), addr(v[1]), 0)
            m.emit(0x29)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMOVAPD")
    }
    return p
}

// VMOVAPS performs "Move Aligned Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VMOVAPS
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (15 forms)
//
//    * VMOVAPS xmm, m128{k}{z}
//    * VMOVAPS xmm, xmm{k}{z}
//    * VMOVAPS ymm, m256{k}{z}
//    * VMOVAPS ymm, ymm{k}{z}
//    * VMOVAPS zmm, m512{k}{z}
//    * VMOVAPS zmm, zmm{k}{z}
//    * VMOVAPS m128, xmm{k}{z}
//    * VMOVAPS m256, ymm{k}{z}
//    * VMOVAPS m512, zmm{k}{z}
//    * VMOVAPS xmm, xmm
//    * VMOVAPS m128, xmm
//    * VMOVAPS ymm, ymm
//    * VMOVAPS m256, ymm
//    * VMOVAPS xmm, m128
//    * VMOVAPS ymm, m256
//
func (self *Program) VMOVAPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VMOVAPS xmm, m128{k}{z}
    if isEVEXXMM(v0) && isM128kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x29)
            m.mrsd(lcode(v[0]), addr(v[1]), 16)
        })
    }
    // VMOVAPS xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7c)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x28)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7c)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x29)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVAPS ymm, m256{k}{z}
    if isEVEXYMM(v0) && isM256kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x29)
            m.mrsd(lcode(v[0]), addr(v[1]), 32)
        })
    }
    // VMOVAPS ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7c)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x28)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7c)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x29)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVAPS zmm, m512{k}{z}
    if isZMM(v0) && isM512kz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x29)
            m.mrsd(lcode(v[0]), addr(v[1]), 64)
        })
    }
    // VMOVAPS zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7c)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x28)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7c)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x29)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVAPS m128, xmm{k}{z}
    if isM128(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x28)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VMOVAPS m256, ymm{k}{z}
    if isM256(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x28)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VMOVAPS m512, zmm{k}{z}
    if isM512(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x28)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VMOVAPS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[1]), v[0], 0)
            m.emit(0x28)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[0]), v[1], 0)
            m.emit(0x29)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVAPS m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x28)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VMOVAPS ymm, ymm
    if isYMM(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[1]), v[0], 0)
            m.emit(0x28)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[0]), v[1], 0)
            m.emit(0x29)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVAPS m256, ymm
    if isM256(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x28)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VMOVAPS xmm, m128
    if isXMM(v0) && isM128(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[0]), addr(v[1]), 0)
            m.emit(0x29)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    // VMOVAPS ymm, m256
    if isYMM(v0) && isM256(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[0]), addr(v[1]), 0)
            m.emit(0x29)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMOVAPS")
    }
    return p
}

// VMOVD performs "Move Doubleword".
//
// Mnemonic        : VMOVD
// ISA extensions  : AVX, AVX512F
// Supported forms : (8 forms)
//
//    * VMOVD xmm, r32
//    * VMOVD xmm, r32
//    * VMOVD r32, xmm
//    * VMOVD r32, xmm
//    * VMOVD m32, xmm
//    * VMOVD m32, xmm
//    * VMOVD xmm, m32
//    * VMOVD xmm, m32
//
func (self *Program) VMOVD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VMOVD xmm, r32
    if isXMM(v0) && isReg32(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[0]), v[1], 0)
            m.emit(0x7e)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVD xmm, r32
    if isEVEXXMM(v0) && isReg32(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7d)
            m.emit(0x08)
            m.emit(0x7e)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVD r32, xmm
    if isReg32(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[1]), v[0], 0)
            m.emit(0x6e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VMOVD r32, xmm
    if isReg32(v0) && isEVEXXMM(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit(0x08)
            m.emit(0x6e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VMOVD m32, xmm
    if isM32(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x6e)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VMOVD m32, xmm
    if isM32(v0) && isEVEXXMM(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, 0, 0, 0)
            m.emit(0x6e)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VMOVD xmm, m32
    if isXMM(v0) && isM32(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[0]), addr(v[1]), 0)
            m.emit(0x7e)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    // VMOVD xmm, m32
    if isEVEXXMM(v0) && isM32(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[0]), addr(v[1]), 0, 0, 0, 0)
            m.emit(0x7e)
            m.mrsd(lcode(v[0]), addr(v[1]), 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMOVD")
    }
    return p
}

// VMOVDDUP performs "Move One Double-FP and Duplicate".
//
// Mnemonic        : VMOVDDUP
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VMOVDDUP xmm, xmm{k}{z}
//    * VMOVDDUP ymm, ymm{k}{z}
//    * VMOVDDUP zmm, zmm{k}{z}
//    * VMOVDDUP m64, xmm{k}{z}
//    * VMOVDDUP m256, ymm{k}{z}
//    * VMOVDDUP m512, zmm{k}{z}
//    * VMOVDDUP xmm, xmm
//    * VMOVDDUP m64, xmm
//    * VMOVDDUP ymm, ymm
//    * VMOVDDUP m256, ymm
//
func (self *Program) VMOVDDUP(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VMOVDDUP xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xff)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x12)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VMOVDDUP ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xff)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x12)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VMOVDDUP zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xff)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x12)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VMOVDDUP m64, xmm{k}{z}
    if isM64(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x87, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x12)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VMOVDDUP m256, ymm{k}{z}
    if isM256(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x87, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x12)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VMOVDDUP m512, zmm{k}{z}
    if isM512(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x87, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x12)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VMOVDDUP xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[1]), v[0], 0)
            m.emit(0x12)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VMOVDDUP m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x12)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VMOVDDUP ymm, ymm
    if isYMM(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(7, hcode(v[1]), v[0], 0)
            m.emit(0x12)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VMOVDDUP m256, ymm
    if isM256(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(7, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x12)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMOVDDUP")
    }
    return p
}

// VMOVDQA performs "Move Aligned Double Quadword".
//
// Mnemonic        : VMOVDQA
// ISA extensions  : AVX
// Supported forms : (6 forms)
//
//    * VMOVDQA xmm, xmm
//    * VMOVDQA m128, xmm
//    * VMOVDQA ymm, ymm
//    * VMOVDQA m256, ymm
//    * VMOVDQA xmm, m128
//    * VMOVDQA ymm, m256
//
func (self *Program) VMOVDQA(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VMOVDQA xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[1]), v[0], 0)
            m.emit(0x6f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[0]), v[1], 0)
            m.emit(0x7f)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVDQA m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x6f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VMOVDQA ymm, ymm
    if isYMM(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[1]), v[0], 0)
            m.emit(0x6f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[0]), v[1], 0)
            m.emit(0x7f)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVDQA m256, ymm
    if isM256(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x6f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VMOVDQA xmm, m128
    if isXMM(v0) && isM128(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[0]), addr(v[1]), 0)
            m.emit(0x7f)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    // VMOVDQA ymm, m256
    if isYMM(v0) && isM256(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[0]), addr(v[1]), 0)
            m.emit(0x7f)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMOVDQA")
    }
    return p
}

// VMOVDQA32 performs "Move Aligned Doubleword Values".
//
// Mnemonic        : VMOVDQA32
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (9 forms)
//
//    * VMOVDQA32 xmm, m128{k}{z}
//    * VMOVDQA32 xmm, xmm{k}{z}
//    * VMOVDQA32 ymm, m256{k}{z}
//    * VMOVDQA32 ymm, ymm{k}{z}
//    * VMOVDQA32 zmm, m512{k}{z}
//    * VMOVDQA32 zmm, zmm{k}{z}
//    * VMOVDQA32 m128, xmm{k}{z}
//    * VMOVDQA32 m256, ymm{k}{z}
//    * VMOVDQA32 m512, zmm{k}{z}
//
func (self *Program) VMOVDQA32(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VMOVDQA32 xmm, m128{k}{z}
    if isEVEXXMM(v0) && isM128kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x7f)
            m.mrsd(lcode(v[0]), addr(v[1]), 16)
        })
    }
    // VMOVDQA32 xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x6f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x7f)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVDQA32 ymm, m256{k}{z}
    if isEVEXYMM(v0) && isM256kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x7f)
            m.mrsd(lcode(v[0]), addr(v[1]), 32)
        })
    }
    // VMOVDQA32 ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x6f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x7f)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVDQA32 zmm, m512{k}{z}
    if isZMM(v0) && isM512kz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x7f)
            m.mrsd(lcode(v[0]), addr(v[1]), 64)
        })
    }
    // VMOVDQA32 zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x6f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x7f)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVDQA32 m128, xmm{k}{z}
    if isM128(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x6f)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VMOVDQA32 m256, ymm{k}{z}
    if isM256(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x6f)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VMOVDQA32 m512, zmm{k}{z}
    if isM512(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x6f)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMOVDQA32")
    }
    return p
}

// VMOVDQA64 performs "Move Aligned Quadword Values".
//
// Mnemonic        : VMOVDQA64
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (9 forms)
//
//    * VMOVDQA64 xmm, m128{k}{z}
//    * VMOVDQA64 xmm, xmm{k}{z}
//    * VMOVDQA64 ymm, m256{k}{z}
//    * VMOVDQA64 ymm, ymm{k}{z}
//    * VMOVDQA64 zmm, m512{k}{z}
//    * VMOVDQA64 zmm, zmm{k}{z}
//    * VMOVDQA64 m128, xmm{k}{z}
//    * VMOVDQA64 m256, ymm{k}{z}
//    * VMOVDQA64 m512, zmm{k}{z}
//
func (self *Program) VMOVDQA64(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VMOVDQA64 xmm, m128{k}{z}
    if isEVEXXMM(v0) && isM128kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x7f)
            m.mrsd(lcode(v[0]), addr(v[1]), 16)
        })
    }
    // VMOVDQA64 xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x6f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x7f)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVDQA64 ymm, m256{k}{z}
    if isEVEXYMM(v0) && isM256kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x7f)
            m.mrsd(lcode(v[0]), addr(v[1]), 32)
        })
    }
    // VMOVDQA64 ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x6f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x7f)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVDQA64 zmm, m512{k}{z}
    if isZMM(v0) && isM512kz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x7f)
            m.mrsd(lcode(v[0]), addr(v[1]), 64)
        })
    }
    // VMOVDQA64 zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x6f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x7f)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVDQA64 m128, xmm{k}{z}
    if isM128(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x6f)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VMOVDQA64 m256, ymm{k}{z}
    if isM256(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x6f)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VMOVDQA64 m512, zmm{k}{z}
    if isM512(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x6f)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMOVDQA64")
    }
    return p
}

// VMOVDQU performs "Move Unaligned Double Quadword".
//
// Mnemonic        : VMOVDQU
// ISA extensions  : AVX
// Supported forms : (6 forms)
//
//    * VMOVDQU xmm, xmm
//    * VMOVDQU m128, xmm
//    * VMOVDQU ymm, ymm
//    * VMOVDQU m256, ymm
//    * VMOVDQU xmm, m128
//    * VMOVDQU ymm, m256
//
func (self *Program) VMOVDQU(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VMOVDQU xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[1]), v[0], 0)
            m.emit(0x6f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[0]), v[1], 0)
            m.emit(0x7f)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVDQU m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x6f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VMOVDQU ymm, ymm
    if isYMM(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(6, hcode(v[1]), v[0], 0)
            m.emit(0x6f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(6, hcode(v[0]), v[1], 0)
            m.emit(0x7f)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVDQU m256, ymm
    if isM256(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(6, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x6f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VMOVDQU xmm, m128
    if isXMM(v0) && isM128(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[0]), addr(v[1]), 0)
            m.emit(0x7f)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    // VMOVDQU ymm, m256
    if isYMM(v0) && isM256(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(6, hcode(v[0]), addr(v[1]), 0)
            m.emit(0x7f)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMOVDQU")
    }
    return p
}

// VMOVDQU16 performs "Move Unaligned Word Values".
//
// Mnemonic        : VMOVDQU16
// ISA extensions  : AVX512BW, AVX512VL
// Supported forms : (9 forms)
//
//    * VMOVDQU16 xmm, m128{k}{z}
//    * VMOVDQU16 xmm, xmm{k}{z}
//    * VMOVDQU16 ymm, m256{k}{z}
//    * VMOVDQU16 ymm, ymm{k}{z}
//    * VMOVDQU16 zmm, m512{k}{z}
//    * VMOVDQU16 zmm, zmm{k}{z}
//    * VMOVDQU16 m128, xmm{k}{z}
//    * VMOVDQU16 m256, ymm{k}{z}
//    * VMOVDQU16 m512, zmm{k}{z}
//
func (self *Program) VMOVDQU16(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VMOVDQU16 xmm, m128{k}{z}
    if isEVEXXMM(v0) && isM128kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x87, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x7f)
            m.mrsd(lcode(v[0]), addr(v[1]), 16)
        })
    }
    // VMOVDQU16 xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xff)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x6f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0xff)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x7f)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVDQU16 ymm, m256{k}{z}
    if isEVEXYMM(v0) && isM256kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x87, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x7f)
            m.mrsd(lcode(v[0]), addr(v[1]), 32)
        })
    }
    // VMOVDQU16 ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xff)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x6f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0xff)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x7f)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVDQU16 zmm, m512{k}{z}
    if isZMM(v0) && isM512kz(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x87, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x7f)
            m.mrsd(lcode(v[0]), addr(v[1]), 64)
        })
    }
    // VMOVDQU16 zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xff)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x6f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0xff)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x7f)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVDQU16 m128, xmm{k}{z}
    if isM128(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x87, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x6f)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VMOVDQU16 m256, ymm{k}{z}
    if isM256(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x87, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x6f)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VMOVDQU16 m512, zmm{k}{z}
    if isM512(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x87, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x6f)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMOVDQU16")
    }
    return p
}

// VMOVDQU32 performs "Move Unaligned Doubleword Values".
//
// Mnemonic        : VMOVDQU32
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (9 forms)
//
//    * VMOVDQU32 xmm, m128{k}{z}
//    * VMOVDQU32 xmm, xmm{k}{z}
//    * VMOVDQU32 ymm, m256{k}{z}
//    * VMOVDQU32 ymm, ymm{k}{z}
//    * VMOVDQU32 zmm, m512{k}{z}
//    * VMOVDQU32 zmm, zmm{k}{z}
//    * VMOVDQU32 m128, xmm{k}{z}
//    * VMOVDQU32 m256, ymm{k}{z}
//    * VMOVDQU32 m512, zmm{k}{z}
//
func (self *Program) VMOVDQU32(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VMOVDQU32 xmm, m128{k}{z}
    if isEVEXXMM(v0) && isM128kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x7f)
            m.mrsd(lcode(v[0]), addr(v[1]), 16)
        })
    }
    // VMOVDQU32 xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x6f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x7f)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVDQU32 ymm, m256{k}{z}
    if isEVEXYMM(v0) && isM256kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x7f)
            m.mrsd(lcode(v[0]), addr(v[1]), 32)
        })
    }
    // VMOVDQU32 ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x6f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x7f)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVDQU32 zmm, m512{k}{z}
    if isZMM(v0) && isM512kz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x7f)
            m.mrsd(lcode(v[0]), addr(v[1]), 64)
        })
    }
    // VMOVDQU32 zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x6f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x7f)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVDQU32 m128, xmm{k}{z}
    if isM128(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x6f)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VMOVDQU32 m256, ymm{k}{z}
    if isM256(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x6f)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VMOVDQU32 m512, zmm{k}{z}
    if isM512(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x6f)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMOVDQU32")
    }
    return p
}

// VMOVDQU64 performs "Move Unaligned Quadword Values".
//
// Mnemonic        : VMOVDQU64
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (9 forms)
//
//    * VMOVDQU64 xmm, m128{k}{z}
//    * VMOVDQU64 xmm, xmm{k}{z}
//    * VMOVDQU64 ymm, m256{k}{z}
//    * VMOVDQU64 ymm, ymm{k}{z}
//    * VMOVDQU64 zmm, m512{k}{z}
//    * VMOVDQU64 zmm, zmm{k}{z}
//    * VMOVDQU64 m128, xmm{k}{z}
//    * VMOVDQU64 m256, ymm{k}{z}
//    * VMOVDQU64 m512, zmm{k}{z}
//
func (self *Program) VMOVDQU64(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VMOVDQU64 xmm, m128{k}{z}
    if isEVEXXMM(v0) && isM128kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x86, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x7f)
            m.mrsd(lcode(v[0]), addr(v[1]), 16)
        })
    }
    // VMOVDQU64 xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfe)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x6f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0xfe)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x7f)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVDQU64 ymm, m256{k}{z}
    if isEVEXYMM(v0) && isM256kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x86, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x7f)
            m.mrsd(lcode(v[0]), addr(v[1]), 32)
        })
    }
    // VMOVDQU64 ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfe)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x6f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0xfe)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x7f)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVDQU64 zmm, m512{k}{z}
    if isZMM(v0) && isM512kz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x86, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x7f)
            m.mrsd(lcode(v[0]), addr(v[1]), 64)
        })
    }
    // VMOVDQU64 zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfe)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x6f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0xfe)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x7f)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVDQU64 m128, xmm{k}{z}
    if isM128(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x86, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x6f)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VMOVDQU64 m256, ymm{k}{z}
    if isM256(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x86, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x6f)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VMOVDQU64 m512, zmm{k}{z}
    if isM512(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x86, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x6f)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMOVDQU64")
    }
    return p
}

// VMOVDQU8 performs "Move Unaligned Byte Values".
//
// Mnemonic        : VMOVDQU8
// ISA extensions  : AVX512BW, AVX512VL
// Supported forms : (9 forms)
//
//    * VMOVDQU8 xmm, m128{k}{z}
//    * VMOVDQU8 xmm, xmm{k}{z}
//    * VMOVDQU8 ymm, m256{k}{z}
//    * VMOVDQU8 ymm, ymm{k}{z}
//    * VMOVDQU8 zmm, m512{k}{z}
//    * VMOVDQU8 zmm, zmm{k}{z}
//    * VMOVDQU8 m128, xmm{k}{z}
//    * VMOVDQU8 m256, ymm{k}{z}
//    * VMOVDQU8 m512, zmm{k}{z}
//
func (self *Program) VMOVDQU8(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VMOVDQU8 xmm, m128{k}{z}
    if isEVEXXMM(v0) && isM128kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x07, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x7f)
            m.mrsd(lcode(v[0]), addr(v[1]), 16)
        })
    }
    // VMOVDQU8 xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7f)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x6f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7f)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x7f)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVDQU8 ymm, m256{k}{z}
    if isEVEXYMM(v0) && isM256kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x07, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x7f)
            m.mrsd(lcode(v[0]), addr(v[1]), 32)
        })
    }
    // VMOVDQU8 ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7f)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x6f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7f)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x7f)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVDQU8 zmm, m512{k}{z}
    if isZMM(v0) && isM512kz(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x07, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x7f)
            m.mrsd(lcode(v[0]), addr(v[1]), 64)
        })
    }
    // VMOVDQU8 zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7f)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x6f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7f)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x7f)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVDQU8 m128, xmm{k}{z}
    if isM128(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x07, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x6f)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VMOVDQU8 m256, ymm{k}{z}
    if isM256(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x07, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x6f)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VMOVDQU8 m512, zmm{k}{z}
    if isM512(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x07, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x6f)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMOVDQU8")
    }
    return p
}

// VMOVHLPS performs "Move Packed Single-Precision Floating-Point Values High to Low".
//
// Mnemonic        : VMOVHLPS
// ISA extensions  : AVX, AVX512F
// Supported forms : (2 forms)
//
//    * VMOVHLPS xmm, xmm, xmm
//    * VMOVHLPS xmm, xmm, xmm
//
func (self *Program) VMOVHLPS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VMOVHLPS xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x12)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VMOVHLPS xmm, xmm, xmm
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | 0x00)
            m.emit(0x12)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMOVHLPS")
    }
    return p
}

// VMOVHPD performs "Move High Packed Double-Precision Floating-Point Value".
//
// Mnemonic        : VMOVHPD
// ISA extensions  : AVX, AVX512F
// Supported forms : (4 forms)
//
//    * VMOVHPD xmm, m64
//    * VMOVHPD xmm, m64
//    * VMOVHPD m64, xmm, xmm
//    * VMOVHPD m64, xmm, xmm
//
func (self *Program) VMOVHPD(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VMOVHPD takes 2 or 3 operands")
    }
    // VMOVHPD xmm, m64
    if len(vv) == 0 && isXMM(v0) && isM64(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[0]), addr(v[1]), 0)
            m.emit(0x17)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    // VMOVHPD xmm, m64
    if len(vv) == 0 && isEVEXXMM(v0) && isM64(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[0]), addr(v[1]), 0, 0, 0, 0)
            m.emit(0x17)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    // VMOVHPD m64, xmm, xmm
    if len(vv) == 1 && isM64(v0) && isXMM(v1) && isXMM(vv[0]) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x16)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VMOVHPD m64, xmm, xmm
    if len(vv) == 1 && isM64(v0) && isEVEXXMM(v1) && isEVEXXMM(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), 0, 0, 0)
            m.emit(0x16)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMOVHPD")
    }
    return p
}

// VMOVHPS performs "Move High Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VMOVHPS
// ISA extensions  : AVX, AVX512F
// Supported forms : (4 forms)
//
//    * VMOVHPS xmm, m64
//    * VMOVHPS xmm, m64
//    * VMOVHPS m64, xmm, xmm
//    * VMOVHPS m64, xmm, xmm
//
func (self *Program) VMOVHPS(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VMOVHPS takes 2 or 3 operands")
    }
    // VMOVHPS xmm, m64
    if len(vv) == 0 && isXMM(v0) && isM64(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[0]), addr(v[1]), 0)
            m.emit(0x17)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    // VMOVHPS xmm, m64
    if len(vv) == 0 && isEVEXXMM(v0) && isM64(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b00, ehcode(v[0]), addr(v[1]), 0, 0, 0, 0)
            m.emit(0x17)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    // VMOVHPS m64, xmm, xmm
    if len(vv) == 1 && isM64(v0) && isXMM(v1) && isXMM(vv[0]) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x16)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VMOVHPS m64, xmm, xmm
    if len(vv) == 1 && isM64(v0) && isEVEXXMM(v1) && isEVEXXMM(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), 0, 0, 0)
            m.emit(0x16)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMOVHPS")
    }
    return p
}

// VMOVLHPS performs "Move Packed Single-Precision Floating-Point Values Low to High".
//
// Mnemonic        : VMOVLHPS
// ISA extensions  : AVX, AVX512F
// Supported forms : (2 forms)
//
//    * VMOVLHPS xmm, xmm, xmm
//    * VMOVLHPS xmm, xmm, xmm
//
func (self *Program) VMOVLHPS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VMOVLHPS xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x16)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VMOVLHPS xmm, xmm, xmm
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | 0x00)
            m.emit(0x16)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMOVLHPS")
    }
    return p
}

// VMOVLPD performs "Move Low Packed Double-Precision Floating-Point Value".
//
// Mnemonic        : VMOVLPD
// ISA extensions  : AVX, AVX512F
// Supported forms : (4 forms)
//
//    * VMOVLPD xmm, m64
//    * VMOVLPD xmm, m64
//    * VMOVLPD m64, xmm, xmm
//    * VMOVLPD m64, xmm, xmm
//
func (self *Program) VMOVLPD(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VMOVLPD takes 2 or 3 operands")
    }
    // VMOVLPD xmm, m64
    if len(vv) == 0 && isXMM(v0) && isM64(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[0]), addr(v[1]), 0)
            m.emit(0x13)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    // VMOVLPD xmm, m64
    if len(vv) == 0 && isEVEXXMM(v0) && isM64(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[0]), addr(v[1]), 0, 0, 0, 0)
            m.emit(0x13)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    // VMOVLPD m64, xmm, xmm
    if len(vv) == 1 && isM64(v0) && isXMM(v1) && isXMM(vv[0]) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x12)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VMOVLPD m64, xmm, xmm
    if len(vv) == 1 && isM64(v0) && isEVEXXMM(v1) && isEVEXXMM(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), 0, 0, 0)
            m.emit(0x12)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMOVLPD")
    }
    return p
}

// VMOVLPS performs "Move Low Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VMOVLPS
// ISA extensions  : AVX, AVX512F
// Supported forms : (4 forms)
//
//    * VMOVLPS xmm, m64
//    * VMOVLPS xmm, m64
//    * VMOVLPS m64, xmm, xmm
//    * VMOVLPS m64, xmm, xmm
//
func (self *Program) VMOVLPS(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VMOVLPS takes 2 or 3 operands")
    }
    // VMOVLPS xmm, m64
    if len(vv) == 0 && isXMM(v0) && isM64(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[0]), addr(v[1]), 0)
            m.emit(0x13)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    // VMOVLPS xmm, m64
    if len(vv) == 0 && isEVEXXMM(v0) && isM64(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b00, ehcode(v[0]), addr(v[1]), 0, 0, 0, 0)
            m.emit(0x13)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    // VMOVLPS m64, xmm, xmm
    if len(vv) == 1 && isM64(v0) && isXMM(v1) && isXMM(vv[0]) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x12)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VMOVLPS m64, xmm, xmm
    if len(vv) == 1 && isM64(v0) && isEVEXXMM(v1) && isEVEXXMM(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), 0, 0, 0)
            m.emit(0x12)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMOVLPS")
    }
    return p
}

// VMOVMSKPD performs "Extract Packed Double-Precision Floating-Point Sign Mask".
//
// Mnemonic        : VMOVMSKPD
// ISA extensions  : AVX
// Supported forms : (2 forms)
//
//    * VMOVMSKPD xmm, r32
//    * VMOVMSKPD ymm, r32
//
func (self *Program) VMOVMSKPD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VMOVMSKPD xmm, r32
    if isXMM(v0) && isReg32(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[1]), v[0], 0)
            m.emit(0x50)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VMOVMSKPD ymm, r32
    if isYMM(v0) && isReg32(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[1]), v[0], 0)
            m.emit(0x50)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMOVMSKPD")
    }
    return p
}

// VMOVMSKPS performs "Extract Packed Single-Precision Floating-Point Sign Mask".
//
// Mnemonic        : VMOVMSKPS
// ISA extensions  : AVX
// Supported forms : (2 forms)
//
//    * VMOVMSKPS xmm, r32
//    * VMOVMSKPS ymm, r32
//
func (self *Program) VMOVMSKPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VMOVMSKPS xmm, r32
    if isXMM(v0) && isReg32(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[1]), v[0], 0)
            m.emit(0x50)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VMOVMSKPS ymm, r32
    if isYMM(v0) && isReg32(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[1]), v[0], 0)
            m.emit(0x50)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMOVMSKPS")
    }
    return p
}

// VMOVNTDQ performs "Store Double Quadword Using Non-Temporal Hint".
//
// Mnemonic        : VMOVNTDQ
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (5 forms)
//
//    * VMOVNTDQ xmm, m128
//    * VMOVNTDQ xmm, m128
//    * VMOVNTDQ ymm, m256
//    * VMOVNTDQ ymm, m256
//    * VMOVNTDQ zmm, m512
//
func (self *Program) VMOVNTDQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VMOVNTDQ xmm, m128
    if isXMM(v0) && isM128(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[0]), addr(v[1]), 0)
            m.emit(0xe7)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    // VMOVNTDQ xmm, m128
    if isEVEXXMM(v0) && isM128(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[0]), addr(v[1]), 0, 0, 0, 0)
            m.emit(0xe7)
            m.mrsd(lcode(v[0]), addr(v[1]), 16)
        })
    }
    // VMOVNTDQ ymm, m256
    if isYMM(v0) && isM256(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[0]), addr(v[1]), 0)
            m.emit(0xe7)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    // VMOVNTDQ ymm, m256
    if isEVEXYMM(v0) && isM256(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[0]), addr(v[1]), 0, 0, 0, 0)
            m.emit(0xe7)
            m.mrsd(lcode(v[0]), addr(v[1]), 32)
        })
    }
    // VMOVNTDQ zmm, m512
    if isZMM(v0) && isM512(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[0]), addr(v[1]), 0, 0, 0, 0)
            m.emit(0xe7)
            m.mrsd(lcode(v[0]), addr(v[1]), 64)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMOVNTDQ")
    }
    return p
}

// VMOVNTDQA performs "Load Double Quadword Non-Temporal Aligned Hint".
//
// Mnemonic        : VMOVNTDQA
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (5 forms)
//
//    * VMOVNTDQA m128, xmm
//    * VMOVNTDQA m128, xmm
//    * VMOVNTDQA m256, ymm
//    * VMOVNTDQA m256, ymm
//    * VMOVNTDQA m512, zmm
//
func (self *Program) VMOVNTDQA(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VMOVNTDQA m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x2a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VMOVNTDQA m128, xmm
    if isM128(v0) && isEVEXXMM(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, 0, 0, 0)
            m.emit(0x2a)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VMOVNTDQA m256, ymm
    if isM256(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x2a)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VMOVNTDQA m256, ymm
    if isM256(v0) && isEVEXYMM(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, 0, 0, 0)
            m.emit(0x2a)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VMOVNTDQA m512, zmm
    if isM512(v0) && isZMM(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, 0, 0, 0)
            m.emit(0x2a)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMOVNTDQA")
    }
    return p
}

// VMOVNTPD performs "Store Packed Double-Precision Floating-Point Values Using Non-Temporal Hint".
//
// Mnemonic        : VMOVNTPD
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (5 forms)
//
//    * VMOVNTPD xmm, m128
//    * VMOVNTPD xmm, m128
//    * VMOVNTPD ymm, m256
//    * VMOVNTPD ymm, m256
//    * VMOVNTPD zmm, m512
//
func (self *Program) VMOVNTPD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VMOVNTPD xmm, m128
    if isXMM(v0) && isM128(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[0]), addr(v[1]), 0)
            m.emit(0x2b)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    // VMOVNTPD xmm, m128
    if isEVEXXMM(v0) && isM128(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[0]), addr(v[1]), 0, 0, 0, 0)
            m.emit(0x2b)
            m.mrsd(lcode(v[0]), addr(v[1]), 16)
        })
    }
    // VMOVNTPD ymm, m256
    if isYMM(v0) && isM256(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[0]), addr(v[1]), 0)
            m.emit(0x2b)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    // VMOVNTPD ymm, m256
    if isEVEXYMM(v0) && isM256(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[0]), addr(v[1]), 0, 0, 0, 0)
            m.emit(0x2b)
            m.mrsd(lcode(v[0]), addr(v[1]), 32)
        })
    }
    // VMOVNTPD zmm, m512
    if isZMM(v0) && isM512(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[0]), addr(v[1]), 0, 0, 0, 0)
            m.emit(0x2b)
            m.mrsd(lcode(v[0]), addr(v[1]), 64)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMOVNTPD")
    }
    return p
}

// VMOVNTPS performs "Store Packed Single-Precision Floating-Point Values Using Non-Temporal Hint".
//
// Mnemonic        : VMOVNTPS
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (5 forms)
//
//    * VMOVNTPS xmm, m128
//    * VMOVNTPS xmm, m128
//    * VMOVNTPS ymm, m256
//    * VMOVNTPS ymm, m256
//    * VMOVNTPS zmm, m512
//
func (self *Program) VMOVNTPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VMOVNTPS xmm, m128
    if isXMM(v0) && isM128(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[0]), addr(v[1]), 0)
            m.emit(0x2b)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    // VMOVNTPS xmm, m128
    if isEVEXXMM(v0) && isM128(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[0]), addr(v[1]), 0, 0, 0, 0)
            m.emit(0x2b)
            m.mrsd(lcode(v[0]), addr(v[1]), 16)
        })
    }
    // VMOVNTPS ymm, m256
    if isYMM(v0) && isM256(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[0]), addr(v[1]), 0)
            m.emit(0x2b)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    // VMOVNTPS ymm, m256
    if isEVEXYMM(v0) && isM256(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[0]), addr(v[1]), 0, 0, 0, 0)
            m.emit(0x2b)
            m.mrsd(lcode(v[0]), addr(v[1]), 32)
        })
    }
    // VMOVNTPS zmm, m512
    if isZMM(v0) && isM512(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b10, ehcode(v[0]), addr(v[1]), 0, 0, 0, 0)
            m.emit(0x2b)
            m.mrsd(lcode(v[0]), addr(v[1]), 64)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMOVNTPS")
    }
    return p
}

// VMOVQ performs "Move Quadword".
//
// Mnemonic        : VMOVQ
// ISA extensions  : AVX, AVX512F
// Supported forms : (10 forms)
//
//    * VMOVQ xmm, r64
//    * VMOVQ xmm, r64
//    * VMOVQ r64, xmm
//    * VMOVQ r64, xmm
//    * VMOVQ xmm, xmm
//    * VMOVQ xmm, xmm
//    * VMOVQ m64, xmm
//    * VMOVQ m64, xmm
//    * VMOVQ xmm, m64
//    * VMOVQ xmm, m64
//
func (self *Program) VMOVQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VMOVQ xmm, r64
    if isXMM(v0) && isReg64(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe1 ^ (hcode(v[0]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0xf9)
            m.emit(0x7e)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVQ xmm, r64
    if isEVEXXMM(v0) && isReg64(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0xfd)
            m.emit(0x08)
            m.emit(0x7e)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVQ r64, xmm
    if isReg64(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe1 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9)
            m.emit(0x6e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VMOVQ r64, xmm
    if isReg64(v0) && isEVEXXMM(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit(0x08)
            m.emit(0x6e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VMOVQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[1]), v[0], 0)
            m.emit(0x7e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[0]), v[1], 0)
            m.emit(0xd6)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVQ xmm, xmm
    if isEVEXXMM(v0) && isEVEXXMM(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfe)
            m.emit(0x08)
            m.emit(0x7e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0xfd)
            m.emit(0x08)
            m.emit(0xd6)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVQ m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x7e)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b1, 0x81, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x6e)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VMOVQ m64, xmm
    if isM64(v0) && isEVEXXMM(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[1]), addr(v[0]), 0, 0, 0, 0)
            m.emit(0x6e)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x86, 0b00, ehcode(v[1]), addr(v[0]), 0, 0, 0, 0)
            m.emit(0x7e)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VMOVQ xmm, m64
    if isXMM(v0) && isM64(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[0]), addr(v[1]), 0)
            m.emit(0xd6)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b1, 0x81, hcode(v[0]), addr(v[1]), 0)
            m.emit(0x7e)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    // VMOVQ xmm, m64
    if isEVEXXMM(v0) && isM64(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[0]), addr(v[1]), 0, 0, 0, 0)
            m.emit(0x7e)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[0]), addr(v[1]), 0, 0, 0, 0)
            m.emit(0xd6)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMOVQ")
    }
    return p
}

// VMOVSD performs "Move Scalar Double-Precision Floating-Point Value".
//
// Mnemonic        : VMOVSD
// ISA extensions  : AVX, AVX512F
// Supported forms : (6 forms)
//
//    * VMOVSD xmm, m64{k}
//    * VMOVSD m64, xmm{k}{z}
//    * VMOVSD m64, xmm
//    * VMOVSD xmm, m64
//    * VMOVSD xmm, xmm, xmm{k}{z}
//    * VMOVSD xmm, xmm, xmm
//
func (self *Program) VMOVSD(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VMOVSD takes 2 or 3 operands")
    }
    // VMOVSD xmm, m64{k}
    if len(vv) == 0 && isEVEXXMM(v0) && isM64k(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x87, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), 0, 0)
            m.emit(0x11)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    // VMOVSD m64, xmm{k}{z}
    if len(vv) == 0 && isM64(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x87, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x10)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VMOVSD m64, xmm
    if len(vv) == 0 && isM64(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x10)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VMOVSD xmm, m64
    if len(vv) == 0 && isXMM(v0) && isM64(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[0]), addr(v[1]), 0)
            m.emit(0x11)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    // VMOVSD xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xff ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x10)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0xff ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x11)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[2]))
        })
    }
    // VMOVSD xmm, xmm, xmm
    if len(vv) == 1 && isXMM(v0) && isXMM(v1) && isXMM(vv[0]) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x10)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[0]), v[2], hlcode(v[1]))
            m.emit(0x11)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[2]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMOVSD")
    }
    return p
}

// VMOVSHDUP performs "Move Packed Single-FP High and Duplicate".
//
// Mnemonic        : VMOVSHDUP
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VMOVSHDUP xmm, xmm{k}{z}
//    * VMOVSHDUP ymm, ymm{k}{z}
//    * VMOVSHDUP zmm, zmm{k}{z}
//    * VMOVSHDUP m128, xmm{k}{z}
//    * VMOVSHDUP m256, ymm{k}{z}
//    * VMOVSHDUP m512, zmm{k}{z}
//    * VMOVSHDUP xmm, xmm
//    * VMOVSHDUP m128, xmm
//    * VMOVSHDUP ymm, ymm
//    * VMOVSHDUP m256, ymm
//
func (self *Program) VMOVSHDUP(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VMOVSHDUP xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x16)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VMOVSHDUP ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x16)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VMOVSHDUP zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x16)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VMOVSHDUP m128, xmm{k}{z}
    if isM128(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x16)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VMOVSHDUP m256, ymm{k}{z}
    if isM256(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x16)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VMOVSHDUP m512, zmm{k}{z}
    if isM512(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x16)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VMOVSHDUP xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[1]), v[0], 0)
            m.emit(0x16)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VMOVSHDUP m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x16)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VMOVSHDUP ymm, ymm
    if isYMM(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(6, hcode(v[1]), v[0], 0)
            m.emit(0x16)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VMOVSHDUP m256, ymm
    if isM256(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(6, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x16)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMOVSHDUP")
    }
    return p
}

// VMOVSLDUP performs "Move Packed Single-FP Low and Duplicate".
//
// Mnemonic        : VMOVSLDUP
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VMOVSLDUP xmm, xmm{k}{z}
//    * VMOVSLDUP ymm, ymm{k}{z}
//    * VMOVSLDUP zmm, zmm{k}{z}
//    * VMOVSLDUP m128, xmm{k}{z}
//    * VMOVSLDUP m256, ymm{k}{z}
//    * VMOVSLDUP m512, zmm{k}{z}
//    * VMOVSLDUP xmm, xmm
//    * VMOVSLDUP m128, xmm
//    * VMOVSLDUP ymm, ymm
//    * VMOVSLDUP m256, ymm
//
func (self *Program) VMOVSLDUP(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VMOVSLDUP xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x12)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VMOVSLDUP ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x12)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VMOVSLDUP zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x12)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VMOVSLDUP m128, xmm{k}{z}
    if isM128(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x12)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VMOVSLDUP m256, ymm{k}{z}
    if isM256(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x12)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VMOVSLDUP m512, zmm{k}{z}
    if isM512(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x12)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VMOVSLDUP xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[1]), v[0], 0)
            m.emit(0x12)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VMOVSLDUP m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x12)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VMOVSLDUP ymm, ymm
    if isYMM(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(6, hcode(v[1]), v[0], 0)
            m.emit(0x12)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VMOVSLDUP m256, ymm
    if isM256(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(6, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x12)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMOVSLDUP")
    }
    return p
}

// VMOVSS performs "Move Scalar Single-Precision Floating-Point Values".
//
// Mnemonic        : VMOVSS
// ISA extensions  : AVX, AVX512F
// Supported forms : (6 forms)
//
//    * VMOVSS xmm, m32{k}
//    * VMOVSS m32, xmm{k}{z}
//    * VMOVSS m32, xmm
//    * VMOVSS xmm, m32
//    * VMOVSS xmm, xmm, xmm{k}{z}
//    * VMOVSS xmm, xmm, xmm
//
func (self *Program) VMOVSS(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VMOVSS takes 2 or 3 operands")
    }
    // VMOVSS xmm, m32{k}
    if len(vv) == 0 && isEVEXXMM(v0) && isM32k(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), 0, 0)
            m.emit(0x11)
            m.mrsd(lcode(v[0]), addr(v[1]), 4)
        })
    }
    // VMOVSS m32, xmm{k}{z}
    if len(vv) == 0 && isM32(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x10)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VMOVSS m32, xmm
    if len(vv) == 0 && isM32(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x10)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VMOVSS xmm, m32
    if len(vv) == 0 && isXMM(v0) && isM32(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[0]), addr(v[1]), 0)
            m.emit(0x11)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    // VMOVSS xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7e ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x10)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x11)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[2]))
        })
    }
    // VMOVSS xmm, xmm, xmm
    if len(vv) == 1 && isXMM(v0) && isXMM(v1) && isXMM(vv[0]) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x10)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[0]), v[2], hlcode(v[1]))
            m.emit(0x11)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[2]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMOVSS")
    }
    return p
}

// VMOVUPD performs "Move Unaligned Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VMOVUPD
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (15 forms)
//
//    * VMOVUPD xmm, m128{k}{z}
//    * VMOVUPD xmm, xmm{k}{z}
//    * VMOVUPD ymm, m256{k}{z}
//    * VMOVUPD ymm, ymm{k}{z}
//    * VMOVUPD zmm, m512{k}{z}
//    * VMOVUPD zmm, zmm{k}{z}
//    * VMOVUPD m128, xmm{k}{z}
//    * VMOVUPD m256, ymm{k}{z}
//    * VMOVUPD m512, zmm{k}{z}
//    * VMOVUPD xmm, xmm
//    * VMOVUPD m128, xmm
//    * VMOVUPD ymm, ymm
//    * VMOVUPD m256, ymm
//    * VMOVUPD xmm, m128
//    * VMOVUPD ymm, m256
//
func (self *Program) VMOVUPD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VMOVUPD xmm, m128{k}{z}
    if isEVEXXMM(v0) && isM128kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x11)
            m.mrsd(lcode(v[0]), addr(v[1]), 16)
        })
    }
    // VMOVUPD xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x10)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x11)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVUPD ymm, m256{k}{z}
    if isEVEXYMM(v0) && isM256kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x11)
            m.mrsd(lcode(v[0]), addr(v[1]), 32)
        })
    }
    // VMOVUPD ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x10)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x11)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVUPD zmm, m512{k}{z}
    if isZMM(v0) && isM512kz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x11)
            m.mrsd(lcode(v[0]), addr(v[1]), 64)
        })
    }
    // VMOVUPD zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x10)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x11)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVUPD m128, xmm{k}{z}
    if isM128(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x10)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VMOVUPD m256, ymm{k}{z}
    if isM256(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x10)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VMOVUPD m512, zmm{k}{z}
    if isM512(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x10)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VMOVUPD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[1]), v[0], 0)
            m.emit(0x10)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[0]), v[1], 0)
            m.emit(0x11)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVUPD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x10)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VMOVUPD ymm, ymm
    if isYMM(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[1]), v[0], 0)
            m.emit(0x10)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[0]), v[1], 0)
            m.emit(0x11)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVUPD m256, ymm
    if isM256(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x10)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VMOVUPD xmm, m128
    if isXMM(v0) && isM128(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[0]), addr(v[1]), 0)
            m.emit(0x11)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    // VMOVUPD ymm, m256
    if isYMM(v0) && isM256(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[0]), addr(v[1]), 0)
            m.emit(0x11)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMOVUPD")
    }
    return p
}

// VMOVUPS performs "Move Unaligned Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VMOVUPS
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (15 forms)
//
//    * VMOVUPS xmm, m128{k}{z}
//    * VMOVUPS xmm, xmm{k}{z}
//    * VMOVUPS ymm, m256{k}{z}
//    * VMOVUPS ymm, ymm{k}{z}
//    * VMOVUPS zmm, m512{k}{z}
//    * VMOVUPS zmm, zmm{k}{z}
//    * VMOVUPS m128, xmm{k}{z}
//    * VMOVUPS m256, ymm{k}{z}
//    * VMOVUPS m512, zmm{k}{z}
//    * VMOVUPS xmm, xmm
//    * VMOVUPS m128, xmm
//    * VMOVUPS ymm, ymm
//    * VMOVUPS m256, ymm
//    * VMOVUPS xmm, m128
//    * VMOVUPS ymm, m256
//
func (self *Program) VMOVUPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VMOVUPS xmm, m128{k}{z}
    if isEVEXXMM(v0) && isM128kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x11)
            m.mrsd(lcode(v[0]), addr(v[1]), 16)
        })
    }
    // VMOVUPS xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7c)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x10)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7c)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x11)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVUPS ymm, m256{k}{z}
    if isEVEXYMM(v0) && isM256kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x11)
            m.mrsd(lcode(v[0]), addr(v[1]), 32)
        })
    }
    // VMOVUPS ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7c)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x10)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7c)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x11)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVUPS zmm, m512{k}{z}
    if isZMM(v0) && isM512kz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x11)
            m.mrsd(lcode(v[0]), addr(v[1]), 64)
        })
    }
    // VMOVUPS zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7c)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x10)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7c)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x11)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVUPS m128, xmm{k}{z}
    if isM128(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x10)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VMOVUPS m256, ymm{k}{z}
    if isM256(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x10)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VMOVUPS m512, zmm{k}{z}
    if isM512(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x10)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VMOVUPS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[1]), v[0], 0)
            m.emit(0x10)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[0]), v[1], 0)
            m.emit(0x11)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVUPS m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x10)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VMOVUPS ymm, ymm
    if isYMM(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[1]), v[0], 0)
            m.emit(0x10)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[0]), v[1], 0)
            m.emit(0x11)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VMOVUPS m256, ymm
    if isM256(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x10)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VMOVUPS xmm, m128
    if isXMM(v0) && isM128(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[0]), addr(v[1]), 0)
            m.emit(0x11)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    // VMOVUPS ymm, m256
    if isYMM(v0) && isM256(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[0]), addr(v[1]), 0)
            m.emit(0x11)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMOVUPS")
    }
    return p
}

// VMPSADBW performs "Compute Multiple Packed Sums of Absolute Difference".
//
// Mnemonic        : VMPSADBW
// ISA extensions  : AVX, AVX2
// Supported forms : (4 forms)
//
//    * VMPSADBW imm8, xmm, xmm, xmm
//    * VMPSADBW imm8, m128, xmm, xmm
//    * VMPSADBW imm8, ymm, ymm, ymm
//    * VMPSADBW imm8, m256, ymm, ymm
//
func (self *Program) VMPSADBW(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VMPSADBW imm8, xmm, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x42)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VMPSADBW imm8, m128, xmm, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x42)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VMPSADBW imm8, ymm, ymm, ymm
    if isImm8(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit(0x42)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VMPSADBW imm8, m256, ymm, ymm
    if isImm8(v0) && isM256(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x42)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMPSADBW")
    }
    return p
}

// VMULPD performs "Multiply Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VMULPD
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (11 forms)
//
//    * VMULPD m128/m64bcst, xmm, xmm{k}{z}
//    * VMULPD xmm, xmm, xmm{k}{z}
//    * VMULPD m256/m64bcst, ymm, ymm{k}{z}
//    * VMULPD ymm, ymm, ymm{k}{z}
//    * VMULPD m512/m64bcst, zmm, zmm{k}{z}
//    * VMULPD xmm, xmm, xmm
//    * VMULPD m128, xmm, xmm
//    * VMULPD ymm, ymm, ymm
//    * VMULPD m256, ymm, ymm
//    * VMULPD {er}, zmm, zmm, zmm{k}{z}
//    * VMULPD zmm, zmm, zmm{k}{z}
//
func (self *Program) VMULPD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VMULPD takes 3 or 4 operands")
    }
    // VMULPD m128/m64bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x59)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VMULPD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x59)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VMULPD m256/m64bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x59)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VMULPD ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x59)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VMULPD m512/m64bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x59)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VMULPD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x59)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VMULPD m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x59)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VMULPD ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x59)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VMULPD m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x59)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VMULPD {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x59)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VMULPD zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x59)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMULPD")
    }
    return p
}

// VMULPS performs "Multiply Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VMULPS
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (11 forms)
//
//    * VMULPS m128/m32bcst, xmm, xmm{k}{z}
//    * VMULPS xmm, xmm, xmm{k}{z}
//    * VMULPS m256/m32bcst, ymm, ymm{k}{z}
//    * VMULPS ymm, ymm, ymm{k}{z}
//    * VMULPS m512/m32bcst, zmm, zmm{k}{z}
//    * VMULPS xmm, xmm, xmm
//    * VMULPS m128, xmm, xmm
//    * VMULPS ymm, ymm, ymm
//    * VMULPS m256, ymm, ymm
//    * VMULPS {er}, zmm, zmm, zmm{k}{z}
//    * VMULPS zmm, zmm, zmm{k}{z}
//
func (self *Program) VMULPS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VMULPS takes 3 or 4 operands")
    }
    // VMULPS m128/m32bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x59)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VMULPS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x59)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VMULPS m256/m32bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x59)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VMULPS ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x59)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VMULPS m512/m32bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x59)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VMULPS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x59)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VMULPS m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x59)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VMULPS ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x59)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VMULPS m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x59)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VMULPS {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7c ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x59)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VMULPS zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x59)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMULPS")
    }
    return p
}

// VMULSD performs "Multiply Scalar Double-Precision Floating-Point Values".
//
// Mnemonic        : VMULSD
// ISA extensions  : AVX, AVX512F
// Supported forms : (5 forms)
//
//    * VMULSD m64, xmm, xmm{k}{z}
//    * VMULSD xmm, xmm, xmm
//    * VMULSD m64, xmm, xmm
//    * VMULSD {er}, xmm, xmm, xmm{k}{z}
//    * VMULSD xmm, xmm, xmm{k}{z}
//
func (self *Program) VMULSD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VMULSD takes 3 or 4 operands")
    }
    // VMULSD m64, xmm, xmm{k}{z}
    if len(vv) == 0 && isM64(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x87, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x59)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    // VMULSD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x59)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VMULSD m64, xmm, xmm
    if len(vv) == 0 && isM64(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x59)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VMULSD {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xff ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x59)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VMULSD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xff ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x59)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMULSD")
    }
    return p
}

// VMULSS performs "Multiply Scalar Single-Precision Floating-Point Values".
//
// Mnemonic        : VMULSS
// ISA extensions  : AVX, AVX512F
// Supported forms : (5 forms)
//
//    * VMULSS m32, xmm, xmm{k}{z}
//    * VMULSS xmm, xmm, xmm
//    * VMULSS m32, xmm, xmm
//    * VMULSS {er}, xmm, xmm, xmm{k}{z}
//    * VMULSS xmm, xmm, xmm{k}{z}
//
func (self *Program) VMULSS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VMULSS takes 3 or 4 operands")
    }
    // VMULSS m32, xmm, xmm{k}{z}
    if len(vv) == 0 && isM32(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x59)
            m.mrsd(lcode(v[2]), addr(v[0]), 4)
        })
    }
    // VMULSS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x59)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VMULSS m32, xmm, xmm
    if len(vv) == 0 && isM32(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x59)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VMULSS {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7e ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x59)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VMULSS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7e ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x59)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VMULSS")
    }
    return p
}

// VORPD performs "Bitwise Logical OR of Double-Precision Floating-Point Values".
//
// Mnemonic        : VORPD
// ISA extensions  : AVX, AVX512DQ, AVX512VL
// Supported forms : (10 forms)
//
//    * VORPD m128/m64bcst, xmm, xmm{k}{z}
//    * VORPD xmm, xmm, xmm{k}{z}
//    * VORPD m256/m64bcst, ymm, ymm{k}{z}
//    * VORPD ymm, ymm, ymm{k}{z}
//    * VORPD m512/m64bcst, zmm, zmm{k}{z}
//    * VORPD zmm, zmm, zmm{k}{z}
//    * VORPD xmm, xmm, xmm
//    * VORPD m128, xmm, xmm
//    * VORPD ymm, ymm, ymm
//    * VORPD m256, ymm, ymm
//
func (self *Program) VORPD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VORPD m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x56)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VORPD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x56)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VORPD m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x56)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VORPD ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x56)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VORPD m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x56)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VORPD zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x56)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VORPD xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x56)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VORPD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x56)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VORPD ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x56)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VORPD m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x56)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VORPD")
    }
    return p
}

// VORPS performs "Bitwise Logical OR of Single-Precision Floating-Point Values".
//
// Mnemonic        : VORPS
// ISA extensions  : AVX, AVX512DQ, AVX512VL
// Supported forms : (10 forms)
//
//    * VORPS m128/m32bcst, xmm, xmm{k}{z}
//    * VORPS xmm, xmm, xmm{k}{z}
//    * VORPS m256/m32bcst, ymm, ymm{k}{z}
//    * VORPS ymm, ymm, ymm{k}{z}
//    * VORPS m512/m32bcst, zmm, zmm{k}{z}
//    * VORPS zmm, zmm, zmm{k}{z}
//    * VORPS xmm, xmm, xmm
//    * VORPS m128, xmm, xmm
//    * VORPS ymm, ymm, ymm
//    * VORPS m256, ymm, ymm
//
func (self *Program) VORPS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VORPS m128/m32bcst, xmm, xmm{k}{z}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x56)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VORPS xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x56)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VORPS m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x56)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VORPS ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x56)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VORPS m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x56)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VORPS zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x56)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VORPS xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x56)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VORPS m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x56)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VORPS ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x56)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VORPS m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x56)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VORPS")
    }
    return p
}

// VPABSB performs "Packed Absolute Value of Byte Integers".
//
// Mnemonic        : VPABSB
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPABSB xmm, xmm{k}{z}
//    * VPABSB ymm, ymm{k}{z}
//    * VPABSB zmm, zmm{k}{z}
//    * VPABSB m128, xmm{k}{z}
//    * VPABSB m256, ymm{k}{z}
//    * VPABSB m512, zmm{k}{z}
//    * VPABSB xmm, xmm
//    * VPABSB m128, xmm
//    * VPABSB ymm, ymm
//    * VPABSB m256, ymm
//
func (self *Program) VPABSB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPABSB xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x1c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPABSB ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x1c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPABSB zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x1c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPABSB m128, xmm{k}{z}
    if isM128(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x1c)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VPABSB m256, ymm{k}{z}
    if isM256(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x1c)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VPABSB m512, zmm{k}{z}
    if isM512(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x1c)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VPABSB xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79)
            m.emit(0x1c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPABSB m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x1c)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VPABSB ymm, ymm
    if isYMM(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d)
            m.emit(0x1c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPABSB m256, ymm
    if isM256(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x1c)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPABSB")
    }
    return p
}

// VPABSD performs "Packed Absolute Value of Doubleword Integers".
//
// Mnemonic        : VPABSD
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPABSD m128/m32bcst, xmm{k}{z}
//    * VPABSD m256/m32bcst, ymm{k}{z}
//    * VPABSD m512/m32bcst, zmm{k}{z}
//    * VPABSD xmm, xmm{k}{z}
//    * VPABSD ymm, ymm{k}{z}
//    * VPABSD zmm, zmm{k}{z}
//    * VPABSD xmm, xmm
//    * VPABSD m128, xmm
//    * VPABSD ymm, ymm
//    * VPABSD m256, ymm
//
func (self *Program) VPABSD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPABSD m128/m32bcst, xmm{k}{z}
    if isM128M32bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x1e)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VPABSD m256/m32bcst, ymm{k}{z}
    if isM256M32bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x1e)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VPABSD m512/m32bcst, zmm{k}{z}
    if isM512M32bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x1e)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VPABSD xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x1e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPABSD ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x1e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPABSD zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x1e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPABSD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79)
            m.emit(0x1e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPABSD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x1e)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VPABSD ymm, ymm
    if isYMM(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d)
            m.emit(0x1e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPABSD m256, ymm
    if isM256(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x1e)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPABSD")
    }
    return p
}

// VPABSQ performs "Packed Absolute Value of Quadword Integers".
//
// Mnemonic        : VPABSQ
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPABSQ m128/m64bcst, xmm{k}{z}
//    * VPABSQ m256/m64bcst, ymm{k}{z}
//    * VPABSQ m512/m64bcst, zmm{k}{z}
//    * VPABSQ xmm, xmm{k}{z}
//    * VPABSQ ymm, ymm{k}{z}
//    * VPABSQ zmm, zmm{k}{z}
//
func (self *Program) VPABSQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPABSQ m128/m64bcst, xmm{k}{z}
    if isM128M64bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x1f)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VPABSQ m256/m64bcst, ymm{k}{z}
    if isM256M64bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x1f)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VPABSQ m512/m64bcst, zmm{k}{z}
    if isM512M64bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x1f)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VPABSQ xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x1f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPABSQ ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x1f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPABSQ zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x1f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPABSQ")
    }
    return p
}

// VPABSW performs "Packed Absolute Value of Word Integers".
//
// Mnemonic        : VPABSW
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPABSW xmm, xmm{k}{z}
//    * VPABSW ymm, ymm{k}{z}
//    * VPABSW zmm, zmm{k}{z}
//    * VPABSW m128, xmm{k}{z}
//    * VPABSW m256, ymm{k}{z}
//    * VPABSW m512, zmm{k}{z}
//    * VPABSW xmm, xmm
//    * VPABSW m128, xmm
//    * VPABSW ymm, ymm
//    * VPABSW m256, ymm
//
func (self *Program) VPABSW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPABSW xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x1d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPABSW ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x1d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPABSW zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x1d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPABSW m128, xmm{k}{z}
    if isM128(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x1d)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VPABSW m256, ymm{k}{z}
    if isM256(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x1d)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VPABSW m512, zmm{k}{z}
    if isM512(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x1d)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VPABSW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79)
            m.emit(0x1d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPABSW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x1d)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VPABSW ymm, ymm
    if isYMM(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d)
            m.emit(0x1d)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPABSW m256, ymm
    if isM256(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x1d)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPABSW")
    }
    return p
}

// VPACKSSDW performs "Pack Doublewords into Words with Signed Saturation".
//
// Mnemonic        : VPACKSSDW
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPACKSSDW m128/m32bcst, xmm, xmm{k}{z}
//    * VPACKSSDW xmm, xmm, xmm{k}{z}
//    * VPACKSSDW m256/m32bcst, ymm, ymm{k}{z}
//    * VPACKSSDW ymm, ymm, ymm{k}{z}
//    * VPACKSSDW m512/m32bcst, zmm, zmm{k}{z}
//    * VPACKSSDW zmm, zmm, zmm{k}{z}
//    * VPACKSSDW xmm, xmm, xmm
//    * VPACKSSDW m128, xmm, xmm
//    * VPACKSSDW ymm, ymm, ymm
//    * VPACKSSDW m256, ymm, ymm
//
func (self *Program) VPACKSSDW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPACKSSDW m128/m32bcst, xmm, xmm{k}{z}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x6b)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPACKSSDW xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x6b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPACKSSDW m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x6b)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPACKSSDW ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x6b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPACKSSDW m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x6b)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPACKSSDW zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x6b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPACKSSDW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x6b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPACKSSDW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x6b)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPACKSSDW ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x6b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPACKSSDW m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x6b)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPACKSSDW")
    }
    return p
}

// VPACKSSWB performs "Pack Words into Bytes with Signed Saturation".
//
// Mnemonic        : VPACKSSWB
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPACKSSWB xmm, xmm, xmm{k}{z}
//    * VPACKSSWB m128, xmm, xmm{k}{z}
//    * VPACKSSWB ymm, ymm, ymm{k}{z}
//    * VPACKSSWB m256, ymm, ymm{k}{z}
//    * VPACKSSWB zmm, zmm, zmm{k}{z}
//    * VPACKSSWB m512, zmm, zmm{k}{z}
//    * VPACKSSWB xmm, xmm, xmm
//    * VPACKSSWB m128, xmm, xmm
//    * VPACKSSWB ymm, ymm, ymm
//    * VPACKSSWB m256, ymm, ymm
//
func (self *Program) VPACKSSWB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPACKSSWB xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x63)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPACKSSWB m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x63)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPACKSSWB ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x63)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPACKSSWB m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x63)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPACKSSWB zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x63)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPACKSSWB m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x63)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPACKSSWB xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x63)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPACKSSWB m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x63)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPACKSSWB ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x63)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPACKSSWB m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x63)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPACKSSWB")
    }
    return p
}

// VPACKUSDW performs "Pack Doublewords into Words with Unsigned Saturation".
//
// Mnemonic        : VPACKUSDW
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPACKUSDW m128/m32bcst, xmm, xmm{k}{z}
//    * VPACKUSDW xmm, xmm, xmm{k}{z}
//    * VPACKUSDW m256/m32bcst, ymm, ymm{k}{z}
//    * VPACKUSDW ymm, ymm, ymm{k}{z}
//    * VPACKUSDW m512/m32bcst, zmm, zmm{k}{z}
//    * VPACKUSDW zmm, zmm, zmm{k}{z}
//    * VPACKUSDW xmm, xmm, xmm
//    * VPACKUSDW m128, xmm, xmm
//    * VPACKUSDW ymm, ymm, ymm
//    * VPACKUSDW m256, ymm, ymm
//
func (self *Program) VPACKUSDW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPACKUSDW m128/m32bcst, xmm, xmm{k}{z}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x2b)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPACKUSDW xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x2b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPACKUSDW m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x2b)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPACKUSDW ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x2b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPACKUSDW m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x2b)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPACKUSDW zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x2b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPACKUSDW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x2b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPACKUSDW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x2b)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPACKUSDW ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x2b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPACKUSDW m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x2b)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPACKUSDW")
    }
    return p
}

// VPACKUSWB performs "Pack Words into Bytes with Unsigned Saturation".
//
// Mnemonic        : VPACKUSWB
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPACKUSWB xmm, xmm, xmm{k}{z}
//    * VPACKUSWB m128, xmm, xmm{k}{z}
//    * VPACKUSWB ymm, ymm, ymm{k}{z}
//    * VPACKUSWB m256, ymm, ymm{k}{z}
//    * VPACKUSWB zmm, zmm, zmm{k}{z}
//    * VPACKUSWB m512, zmm, zmm{k}{z}
//    * VPACKUSWB xmm, xmm, xmm
//    * VPACKUSWB m128, xmm, xmm
//    * VPACKUSWB ymm, ymm, ymm
//    * VPACKUSWB m256, ymm, ymm
//
func (self *Program) VPACKUSWB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPACKUSWB xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x67)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPACKUSWB m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x67)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPACKUSWB ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x67)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPACKUSWB m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x67)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPACKUSWB zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x67)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPACKUSWB m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x67)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPACKUSWB xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x67)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPACKUSWB m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x67)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPACKUSWB ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x67)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPACKUSWB m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x67)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPACKUSWB")
    }
    return p
}

// VPADDB performs "Add Packed Byte Integers".
//
// Mnemonic        : VPADDB
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPADDB xmm, xmm, xmm{k}{z}
//    * VPADDB m128, xmm, xmm{k}{z}
//    * VPADDB ymm, ymm, ymm{k}{z}
//    * VPADDB m256, ymm, ymm{k}{z}
//    * VPADDB zmm, zmm, zmm{k}{z}
//    * VPADDB m512, zmm, zmm{k}{z}
//    * VPADDB xmm, xmm, xmm
//    * VPADDB m128, xmm, xmm
//    * VPADDB ymm, ymm, ymm
//    * VPADDB m256, ymm, ymm
//
func (self *Program) VPADDB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPADDB xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xfc)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDB m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xfc)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPADDB ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xfc)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDB m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xfc)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPADDB zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xfc)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDB m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xfc)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPADDB xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xfc)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDB m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xfc)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPADDB ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xfc)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDB m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xfc)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPADDB")
    }
    return p
}

// VPADDD performs "Add Packed Doubleword Integers".
//
// Mnemonic        : VPADDD
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPADDD m128/m32bcst, xmm, xmm{k}{z}
//    * VPADDD xmm, xmm, xmm{k}{z}
//    * VPADDD m256/m32bcst, ymm, ymm{k}{z}
//    * VPADDD ymm, ymm, ymm{k}{z}
//    * VPADDD m512/m32bcst, zmm, zmm{k}{z}
//    * VPADDD zmm, zmm, zmm{k}{z}
//    * VPADDD xmm, xmm, xmm
//    * VPADDD m128, xmm, xmm
//    * VPADDD ymm, ymm, ymm
//    * VPADDD m256, ymm, ymm
//
func (self *Program) VPADDD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPADDD m128/m32bcst, xmm, xmm{k}{z}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xfe)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPADDD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xfe)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDD m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xfe)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPADDD ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xfe)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDD m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xfe)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPADDD zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xfe)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDD xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xfe)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xfe)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPADDD ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xfe)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDD m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xfe)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPADDD")
    }
    return p
}

// VPADDQ performs "Add Packed Quadword Integers".
//
// Mnemonic        : VPADDQ
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPADDQ m128/m64bcst, xmm, xmm{k}{z}
//    * VPADDQ xmm, xmm, xmm{k}{z}
//    * VPADDQ m256/m64bcst, ymm, ymm{k}{z}
//    * VPADDQ ymm, ymm, ymm{k}{z}
//    * VPADDQ m512/m64bcst, zmm, zmm{k}{z}
//    * VPADDQ zmm, zmm, zmm{k}{z}
//    * VPADDQ xmm, xmm, xmm
//    * VPADDQ m128, xmm, xmm
//    * VPADDQ ymm, ymm, ymm
//    * VPADDQ m256, ymm, ymm
//
func (self *Program) VPADDQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPADDQ m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xd4)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPADDQ xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xd4)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDQ m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xd4)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPADDQ ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xd4)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDQ m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xd4)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPADDQ zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xd4)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDQ xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xd4)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDQ m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xd4)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPADDQ ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xd4)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDQ m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xd4)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPADDQ")
    }
    return p
}

// VPADDSB performs "Add Packed Signed Byte Integers with Signed Saturation".
//
// Mnemonic        : VPADDSB
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPADDSB xmm, xmm, xmm{k}{z}
//    * VPADDSB m128, xmm, xmm{k}{z}
//    * VPADDSB ymm, ymm, ymm{k}{z}
//    * VPADDSB m256, ymm, ymm{k}{z}
//    * VPADDSB zmm, zmm, zmm{k}{z}
//    * VPADDSB m512, zmm, zmm{k}{z}
//    * VPADDSB xmm, xmm, xmm
//    * VPADDSB m128, xmm, xmm
//    * VPADDSB ymm, ymm, ymm
//    * VPADDSB m256, ymm, ymm
//
func (self *Program) VPADDSB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPADDSB xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xec)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDSB m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xec)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPADDSB ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xec)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDSB m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xec)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPADDSB zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xec)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDSB m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xec)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPADDSB xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xec)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDSB m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xec)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPADDSB ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xec)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDSB m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xec)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPADDSB")
    }
    return p
}

// VPADDSW performs "Add Packed Signed Word Integers with Signed Saturation".
//
// Mnemonic        : VPADDSW
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPADDSW xmm, xmm, xmm{k}{z}
//    * VPADDSW m128, xmm, xmm{k}{z}
//    * VPADDSW ymm, ymm, ymm{k}{z}
//    * VPADDSW m256, ymm, ymm{k}{z}
//    * VPADDSW zmm, zmm, zmm{k}{z}
//    * VPADDSW m512, zmm, zmm{k}{z}
//    * VPADDSW xmm, xmm, xmm
//    * VPADDSW m128, xmm, xmm
//    * VPADDSW ymm, ymm, ymm
//    * VPADDSW m256, ymm, ymm
//
func (self *Program) VPADDSW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPADDSW xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xed)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDSW m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xed)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPADDSW ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xed)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDSW m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xed)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPADDSW zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xed)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDSW m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xed)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPADDSW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xed)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDSW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xed)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPADDSW ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xed)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDSW m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xed)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPADDSW")
    }
    return p
}

// VPADDUSB performs "Add Packed Unsigned Byte Integers with Unsigned Saturation".
//
// Mnemonic        : VPADDUSB
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPADDUSB xmm, xmm, xmm{k}{z}
//    * VPADDUSB m128, xmm, xmm{k}{z}
//    * VPADDUSB ymm, ymm, ymm{k}{z}
//    * VPADDUSB m256, ymm, ymm{k}{z}
//    * VPADDUSB zmm, zmm, zmm{k}{z}
//    * VPADDUSB m512, zmm, zmm{k}{z}
//    * VPADDUSB xmm, xmm, xmm
//    * VPADDUSB m128, xmm, xmm
//    * VPADDUSB ymm, ymm, ymm
//    * VPADDUSB m256, ymm, ymm
//
func (self *Program) VPADDUSB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPADDUSB xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xdc)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDUSB m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xdc)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPADDUSB ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xdc)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDUSB m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xdc)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPADDUSB zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xdc)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDUSB m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xdc)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPADDUSB xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xdc)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDUSB m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xdc)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPADDUSB ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xdc)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDUSB m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xdc)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPADDUSB")
    }
    return p
}

// VPADDUSW performs "Add Packed Unsigned Word Integers with Unsigned Saturation".
//
// Mnemonic        : VPADDUSW
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPADDUSW xmm, xmm, xmm{k}{z}
//    * VPADDUSW m128, xmm, xmm{k}{z}
//    * VPADDUSW ymm, ymm, ymm{k}{z}
//    * VPADDUSW m256, ymm, ymm{k}{z}
//    * VPADDUSW zmm, zmm, zmm{k}{z}
//    * VPADDUSW m512, zmm, zmm{k}{z}
//    * VPADDUSW xmm, xmm, xmm
//    * VPADDUSW m128, xmm, xmm
//    * VPADDUSW ymm, ymm, ymm
//    * VPADDUSW m256, ymm, ymm
//
func (self *Program) VPADDUSW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPADDUSW xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xdd)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDUSW m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xdd)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPADDUSW ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xdd)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDUSW m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xdd)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPADDUSW zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xdd)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDUSW m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xdd)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPADDUSW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xdd)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDUSW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xdd)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPADDUSW ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xdd)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDUSW m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xdd)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPADDUSW")
    }
    return p
}

// VPADDW performs "Add Packed Word Integers".
//
// Mnemonic        : VPADDW
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPADDW xmm, xmm, xmm{k}{z}
//    * VPADDW m128, xmm, xmm{k}{z}
//    * VPADDW ymm, ymm, ymm{k}{z}
//    * VPADDW m256, ymm, ymm{k}{z}
//    * VPADDW zmm, zmm, zmm{k}{z}
//    * VPADDW m512, zmm, zmm{k}{z}
//    * VPADDW xmm, xmm, xmm
//    * VPADDW m128, xmm, xmm
//    * VPADDW ymm, ymm, ymm
//    * VPADDW m256, ymm, ymm
//
func (self *Program) VPADDW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPADDW xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xfd)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDW m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xfd)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPADDW ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xfd)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDW m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xfd)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPADDW zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xfd)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDW m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xfd)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPADDW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xfd)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xfd)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPADDW ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xfd)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPADDW m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xfd)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPADDW")
    }
    return p
}

// VPALIGNR performs "Packed Align Right".
//
// Mnemonic        : VPALIGNR
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPALIGNR imm8, xmm, xmm, xmm{k}{z}
//    * VPALIGNR imm8, m128, xmm, xmm{k}{z}
//    * VPALIGNR imm8, ymm, ymm, ymm{k}{z}
//    * VPALIGNR imm8, m256, ymm, ymm{k}{z}
//    * VPALIGNR imm8, zmm, zmm, zmm{k}{z}
//    * VPALIGNR imm8, m512, zmm, zmm{k}{z}
//    * VPALIGNR imm8, xmm, xmm, xmm
//    * VPALIGNR imm8, m128, xmm, xmm
//    * VPALIGNR imm8, ymm, ymm, ymm
//    * VPALIGNR imm8, m256, ymm, ymm
//
func (self *Program) VPALIGNR(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPALIGNR imm8, xmm, xmm, xmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x00)
            m.emit(0x0f)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPALIGNR imm8, m128, xmm, xmm{k}{z}
    if isImm8(v0) && isM128(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), 0)
            m.emit(0x0f)
            m.mrsd(lcode(v[3]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPALIGNR imm8, ymm, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x20)
            m.emit(0x0f)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPALIGNR imm8, m256, ymm, ymm{k}{z}
    if isImm8(v0) && isM256(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b01, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), 0)
            m.emit(0x0f)
            m.mrsd(lcode(v[3]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPALIGNR imm8, zmm, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x0f)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPALIGNR imm8, m512, zmm, zmm{k}{z}
    if isImm8(v0) && isM512(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), 0)
            m.emit(0x0f)
            m.mrsd(lcode(v[3]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPALIGNR imm8, xmm, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x0f)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPALIGNR imm8, m128, xmm, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x0f)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPALIGNR imm8, ymm, ymm, ymm
    if isImm8(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit(0x0f)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPALIGNR imm8, m256, ymm, ymm
    if isImm8(v0) && isM256(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x0f)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPALIGNR")
    }
    return p
}

// VPAND performs "Packed Bitwise Logical AND".
//
// Mnemonic        : VPAND
// ISA extensions  : AVX, AVX2
// Supported forms : (4 forms)
//
//    * VPAND xmm, xmm, xmm
//    * VPAND m128, xmm, xmm
//    * VPAND ymm, ymm, ymm
//    * VPAND m256, ymm, ymm
//
func (self *Program) VPAND(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPAND xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xdb)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPAND m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xdb)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPAND ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xdb)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPAND m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xdb)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPAND")
    }
    return p
}

// VPANDD performs "Bitwise Logical AND of Packed Doubleword Integers".
//
// Mnemonic        : VPANDD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPANDD m128/m32bcst, xmm, xmm{k}{z}
//    * VPANDD xmm, xmm, xmm{k}{z}
//    * VPANDD m256/m32bcst, ymm, ymm{k}{z}
//    * VPANDD ymm, ymm, ymm{k}{z}
//    * VPANDD m512/m32bcst, zmm, zmm{k}{z}
//    * VPANDD zmm, zmm, zmm{k}{z}
//
func (self *Program) VPANDD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPANDD m128/m32bcst, xmm, xmm{k}{z}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xdb)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPANDD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xdb)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPANDD m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xdb)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPANDD ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xdb)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPANDD m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xdb)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPANDD zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xdb)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPANDD")
    }
    return p
}

// VPANDN performs "Packed Bitwise Logical AND NOT".
//
// Mnemonic        : VPANDN
// ISA extensions  : AVX, AVX2
// Supported forms : (4 forms)
//
//    * VPANDN xmm, xmm, xmm
//    * VPANDN m128, xmm, xmm
//    * VPANDN ymm, ymm, ymm
//    * VPANDN m256, ymm, ymm
//
func (self *Program) VPANDN(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPANDN xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xdf)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPANDN m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xdf)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPANDN ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xdf)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPANDN m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xdf)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPANDN")
    }
    return p
}

// VPANDND performs "Bitwise Logical AND NOT of Packed Doubleword Integers".
//
// Mnemonic        : VPANDND
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPANDND m128/m32bcst, xmm, xmm{k}{z}
//    * VPANDND xmm, xmm, xmm{k}{z}
//    * VPANDND m256/m32bcst, ymm, ymm{k}{z}
//    * VPANDND ymm, ymm, ymm{k}{z}
//    * VPANDND m512/m32bcst, zmm, zmm{k}{z}
//    * VPANDND zmm, zmm, zmm{k}{z}
//
func (self *Program) VPANDND(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPANDND m128/m32bcst, xmm, xmm{k}{z}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xdf)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPANDND xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xdf)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPANDND m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xdf)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPANDND ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xdf)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPANDND m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xdf)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPANDND zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xdf)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPANDND")
    }
    return p
}

// VPANDNQ performs "Bitwise Logical AND NOT of Packed Quadword Integers".
//
// Mnemonic        : VPANDNQ
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPANDNQ m128/m64bcst, xmm, xmm{k}{z}
//    * VPANDNQ xmm, xmm, xmm{k}{z}
//    * VPANDNQ m256/m64bcst, ymm, ymm{k}{z}
//    * VPANDNQ ymm, ymm, ymm{k}{z}
//    * VPANDNQ m512/m64bcst, zmm, zmm{k}{z}
//    * VPANDNQ zmm, zmm, zmm{k}{z}
//
func (self *Program) VPANDNQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPANDNQ m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xdf)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPANDNQ xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xdf)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPANDNQ m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xdf)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPANDNQ ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xdf)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPANDNQ m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xdf)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPANDNQ zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xdf)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPANDNQ")
    }
    return p
}

// VPANDQ performs "Bitwise Logical AND of Packed Quadword Integers".
//
// Mnemonic        : VPANDQ
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPANDQ m128/m64bcst, xmm, xmm{k}{z}
//    * VPANDQ xmm, xmm, xmm{k}{z}
//    * VPANDQ m256/m64bcst, ymm, ymm{k}{z}
//    * VPANDQ ymm, ymm, ymm{k}{z}
//    * VPANDQ m512/m64bcst, zmm, zmm{k}{z}
//    * VPANDQ zmm, zmm, zmm{k}{z}
//
func (self *Program) VPANDQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPANDQ m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xdb)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPANDQ xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xdb)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPANDQ m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xdb)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPANDQ ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xdb)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPANDQ m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xdb)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPANDQ zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xdb)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPANDQ")
    }
    return p
}

// VPAVGB performs "Average Packed Byte Integers".
//
// Mnemonic        : VPAVGB
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPAVGB xmm, xmm, xmm{k}{z}
//    * VPAVGB m128, xmm, xmm{k}{z}
//    * VPAVGB ymm, ymm, ymm{k}{z}
//    * VPAVGB m256, ymm, ymm{k}{z}
//    * VPAVGB zmm, zmm, zmm{k}{z}
//    * VPAVGB m512, zmm, zmm{k}{z}
//    * VPAVGB xmm, xmm, xmm
//    * VPAVGB m128, xmm, xmm
//    * VPAVGB ymm, ymm, ymm
//    * VPAVGB m256, ymm, ymm
//
func (self *Program) VPAVGB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPAVGB xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xe0)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPAVGB m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xe0)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPAVGB ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xe0)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPAVGB m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xe0)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPAVGB zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xe0)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPAVGB m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xe0)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPAVGB xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xe0)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPAVGB m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xe0)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPAVGB ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xe0)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPAVGB m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xe0)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPAVGB")
    }
    return p
}

// VPAVGW performs "Average Packed Word Integers".
//
// Mnemonic        : VPAVGW
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPAVGW xmm, xmm, xmm{k}{z}
//    * VPAVGW m128, xmm, xmm{k}{z}
//    * VPAVGW ymm, ymm, ymm{k}{z}
//    * VPAVGW m256, ymm, ymm{k}{z}
//    * VPAVGW zmm, zmm, zmm{k}{z}
//    * VPAVGW m512, zmm, zmm{k}{z}
//    * VPAVGW xmm, xmm, xmm
//    * VPAVGW m128, xmm, xmm
//    * VPAVGW ymm, ymm, ymm
//    * VPAVGW m256, ymm, ymm
//
func (self *Program) VPAVGW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPAVGW xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xe3)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPAVGW m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xe3)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPAVGW ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xe3)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPAVGW m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xe3)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPAVGW zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xe3)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPAVGW m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xe3)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPAVGW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xe3)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPAVGW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xe3)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPAVGW ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xe3)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPAVGW m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xe3)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPAVGW")
    }
    return p
}

// VPBLENDD performs "Blend Packed Doublewords".
//
// Mnemonic        : VPBLENDD
// ISA extensions  : AVX2
// Supported forms : (4 forms)
//
//    * VPBLENDD imm8, xmm, xmm, xmm
//    * VPBLENDD imm8, m128, xmm, xmm
//    * VPBLENDD imm8, ymm, ymm, ymm
//    * VPBLENDD imm8, m256, ymm, ymm
//
func (self *Program) VPBLENDD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPBLENDD imm8, xmm, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x02)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPBLENDD imm8, m128, xmm, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x02)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPBLENDD imm8, ymm, ymm, ymm
    if isImm8(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit(0x02)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPBLENDD imm8, m256, ymm, ymm
    if isImm8(v0) && isM256(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x02)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPBLENDD")
    }
    return p
}

// VPBLENDMB performs "Blend Byte Vectors Using an OpMask Control".
//
// Mnemonic        : VPBLENDMB
// ISA extensions  : AVX512BW, AVX512VL
// Supported forms : (6 forms)
//
//    * VPBLENDMB xmm, xmm, xmm{k}{z}
//    * VPBLENDMB m128, xmm, xmm{k}{z}
//    * VPBLENDMB ymm, ymm, ymm{k}{z}
//    * VPBLENDMB m256, ymm, ymm{k}{z}
//    * VPBLENDMB zmm, zmm, zmm{k}{z}
//    * VPBLENDMB m512, zmm, zmm{k}{z}
//
func (self *Program) VPBLENDMB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPBLENDMB xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x66)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPBLENDMB m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x66)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPBLENDMB ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x66)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPBLENDMB m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x66)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPBLENDMB zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x66)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPBLENDMB m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x66)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPBLENDMB")
    }
    return p
}

// VPBLENDMD performs "Blend Doubleword Vectors Using an OpMask Control".
//
// Mnemonic        : VPBLENDMD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPBLENDMD m128/m32bcst, xmm, xmm{k}{z}
//    * VPBLENDMD xmm, xmm, xmm{k}{z}
//    * VPBLENDMD m256/m32bcst, ymm, ymm{k}{z}
//    * VPBLENDMD ymm, ymm, ymm{k}{z}
//    * VPBLENDMD m512/m32bcst, zmm, zmm{k}{z}
//    * VPBLENDMD zmm, zmm, zmm{k}{z}
//
func (self *Program) VPBLENDMD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPBLENDMD m128/m32bcst, xmm, xmm{k}{z}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x64)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPBLENDMD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x64)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPBLENDMD m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x64)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPBLENDMD ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x64)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPBLENDMD m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x64)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPBLENDMD zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x64)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPBLENDMD")
    }
    return p
}

// VPBLENDMQ performs "Blend Quadword Vectors Using an OpMask Control".
//
// Mnemonic        : VPBLENDMQ
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPBLENDMQ m128/m64bcst, xmm, xmm{k}{z}
//    * VPBLENDMQ xmm, xmm, xmm{k}{z}
//    * VPBLENDMQ m256/m64bcst, ymm, ymm{k}{z}
//    * VPBLENDMQ ymm, ymm, ymm{k}{z}
//    * VPBLENDMQ m512/m64bcst, zmm, zmm{k}{z}
//    * VPBLENDMQ zmm, zmm, zmm{k}{z}
//
func (self *Program) VPBLENDMQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPBLENDMQ m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x64)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPBLENDMQ xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x64)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPBLENDMQ m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x64)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPBLENDMQ ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x64)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPBLENDMQ m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x64)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPBLENDMQ zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x64)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPBLENDMQ")
    }
    return p
}

// VPBLENDMW performs "Blend Word Vectors Using an OpMask Control".
//
// Mnemonic        : VPBLENDMW
// ISA extensions  : AVX512BW, AVX512VL
// Supported forms : (6 forms)
//
//    * VPBLENDMW xmm, xmm, xmm{k}{z}
//    * VPBLENDMW m128, xmm, xmm{k}{z}
//    * VPBLENDMW ymm, ymm, ymm{k}{z}
//    * VPBLENDMW m256, ymm, ymm{k}{z}
//    * VPBLENDMW zmm, zmm, zmm{k}{z}
//    * VPBLENDMW m512, zmm, zmm{k}{z}
//
func (self *Program) VPBLENDMW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPBLENDMW xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x66)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPBLENDMW m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x66)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPBLENDMW ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x66)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPBLENDMW m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x66)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPBLENDMW zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x66)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPBLENDMW m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x66)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPBLENDMW")
    }
    return p
}

// VPBLENDVB performs "Variable Blend Packed Bytes".
//
// Mnemonic        : VPBLENDVB
// ISA extensions  : AVX, AVX2
// Supported forms : (4 forms)
//
//    * VPBLENDVB xmm, xmm, xmm, xmm
//    * VPBLENDVB xmm, m128, xmm, xmm
//    * VPBLENDVB ymm, ymm, ymm, ymm
//    * VPBLENDVB ymm, m256, ymm, ymm
//
func (self *Program) VPBLENDVB(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPBLENDVB xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x4c)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VPBLENDVB xmm, m128, xmm, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x4c)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VPBLENDVB ymm, ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit(0x4c)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VPBLENDVB ymm, m256, ymm, ymm
    if isYMM(v0) && isM256(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x4c)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPBLENDVB")
    }
    return p
}

// VPBLENDW performs "Blend Packed Words".
//
// Mnemonic        : VPBLENDW
// ISA extensions  : AVX, AVX2
// Supported forms : (4 forms)
//
//    * VPBLENDW imm8, xmm, xmm, xmm
//    * VPBLENDW imm8, m128, xmm, xmm
//    * VPBLENDW imm8, ymm, ymm, ymm
//    * VPBLENDW imm8, m256, ymm, ymm
//
func (self *Program) VPBLENDW(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPBLENDW imm8, xmm, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x0e)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPBLENDW imm8, m128, xmm, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x0e)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPBLENDW imm8, ymm, ymm, ymm
    if isImm8(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit(0x0e)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPBLENDW imm8, m256, ymm, ymm
    if isImm8(v0) && isM256(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x0e)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPBLENDW")
    }
    return p
}

// VPBROADCASTB performs "Broadcast Byte Integer".
//
// Mnemonic        : VPBROADCASTB
// ISA extensions  : AVX2, AVX512BW, AVX512VL
// Supported forms : (13 forms)
//
//    * VPBROADCASTB r32, xmm{k}{z}
//    * VPBROADCASTB r32, ymm{k}{z}
//    * VPBROADCASTB r32, zmm{k}{z}
//    * VPBROADCASTB xmm, xmm{k}{z}
//    * VPBROADCASTB xmm, ymm{k}{z}
//    * VPBROADCASTB xmm, zmm{k}{z}
//    * VPBROADCASTB m8, xmm{k}{z}
//    * VPBROADCASTB m8, ymm{k}{z}
//    * VPBROADCASTB m8, zmm{k}{z}
//    * VPBROADCASTB xmm, xmm
//    * VPBROADCASTB m8, xmm
//    * VPBROADCASTB xmm, ymm
//    * VPBROADCASTB m8, ymm
//
func (self *Program) VPBROADCASTB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPBROADCASTB r32, xmm{k}{z}
    if isReg32(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x7a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTB r32, ymm{k}{z}
    if isReg32(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x7a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTB r32, zmm{k}{z}
    if isReg32(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x7a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTB xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTB xmm, ymm{k}{z}
    if isEVEXXMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTB xmm, zmm{k}{z}
    if isEVEXXMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTB m8, xmm{k}{z}
    if isM8(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x78)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VPBROADCASTB m8, ymm{k}{z}
    if isM8(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x78)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VPBROADCASTB m8, zmm{k}{z}
    if isM8(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x78)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VPBROADCASTB xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79)
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTB m8, xmm
    if isM8(v0) && isXMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x78)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VPBROADCASTB xmm, ymm
    if isXMM(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d)
            m.emit(0x78)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTB m8, ymm
    if isM8(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x78)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPBROADCASTB")
    }
    return p
}

// VPBROADCASTD performs "Broadcast Doubleword Integer".
//
// Mnemonic        : VPBROADCASTD
// ISA extensions  : AVX2, AVX512F, AVX512VL
// Supported forms : (13 forms)
//
//    * VPBROADCASTD r32, xmm{k}{z}
//    * VPBROADCASTD r32, ymm{k}{z}
//    * VPBROADCASTD r32, zmm{k}{z}
//    * VPBROADCASTD xmm, xmm{k}{z}
//    * VPBROADCASTD xmm, ymm{k}{z}
//    * VPBROADCASTD xmm, zmm{k}{z}
//    * VPBROADCASTD m32, xmm{k}{z}
//    * VPBROADCASTD m32, ymm{k}{z}
//    * VPBROADCASTD m32, zmm{k}{z}
//    * VPBROADCASTD xmm, xmm
//    * VPBROADCASTD m32, xmm
//    * VPBROADCASTD xmm, ymm
//    * VPBROADCASTD m32, ymm
//
func (self *Program) VPBROADCASTD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPBROADCASTD r32, xmm{k}{z}
    if isReg32(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x7c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTD r32, ymm{k}{z}
    if isReg32(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x7c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTD r32, zmm{k}{z}
    if isReg32(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x7c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTD xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x58)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTD xmm, ymm{k}{z}
    if isEVEXXMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x58)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTD xmm, zmm{k}{z}
    if isEVEXXMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x58)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTD m32, xmm{k}{z}
    if isM32(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x58)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VPBROADCASTD m32, ymm{k}{z}
    if isM32(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x58)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VPBROADCASTD m32, zmm{k}{z}
    if isM32(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x58)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VPBROADCASTD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79)
            m.emit(0x58)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTD m32, xmm
    if isM32(v0) && isXMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x58)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VPBROADCASTD xmm, ymm
    if isXMM(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d)
            m.emit(0x58)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTD m32, ymm
    if isM32(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x58)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPBROADCASTD")
    }
    return p
}

// VPBROADCASTMB2Q performs "Broadcast Low Byte of Mask Register to Packed Quadword Values".
//
// Mnemonic        : VPBROADCASTMB2Q
// ISA extensions  : AVX512CD, AVX512VL
// Supported forms : (3 forms)
//
//    * VPBROADCASTMB2Q k, xmm
//    * VPBROADCASTMB2Q k, ymm
//    * VPBROADCASTMB2Q k, zmm
//
func (self *Program) VPBROADCASTMB2Q(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPBROADCASTMB2Q k, xmm
    if isK(v0) && isEVEXXMM(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512CD)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfe)
            m.emit(0x08)
            m.emit(0x2a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTMB2Q k, ymm
    if isK(v0) && isEVEXYMM(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512CD)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfe)
            m.emit(0x28)
            m.emit(0x2a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTMB2Q k, zmm
    if isK(v0) && isZMM(v1) {
        self.require(ISA_AVX512CD)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfe)
            m.emit(0x48)
            m.emit(0x2a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPBROADCASTMB2Q")
    }
    return p
}

// VPBROADCASTMW2D performs "Broadcast Low Word of Mask Register to Packed Doubleword Values".
//
// Mnemonic        : VPBROADCASTMW2D
// ISA extensions  : AVX512CD, AVX512VL
// Supported forms : (3 forms)
//
//    * VPBROADCASTMW2D k, xmm
//    * VPBROADCASTMW2D k, ymm
//    * VPBROADCASTMW2D k, zmm
//
func (self *Program) VPBROADCASTMW2D(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPBROADCASTMW2D k, xmm
    if isK(v0) && isEVEXXMM(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512CD)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit(0x08)
            m.emit(0x3a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTMW2D k, ymm
    if isK(v0) && isEVEXYMM(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512CD)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit(0x28)
            m.emit(0x3a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTMW2D k, zmm
    if isK(v0) && isZMM(v1) {
        self.require(ISA_AVX512CD)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit(0x48)
            m.emit(0x3a)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPBROADCASTMW2D")
    }
    return p
}

// VPBROADCASTQ performs "Broadcast Quadword Integer".
//
// Mnemonic        : VPBROADCASTQ
// ISA extensions  : AVX2, AVX512F, AVX512VL
// Supported forms : (13 forms)
//
//    * VPBROADCASTQ r64, xmm{k}{z}
//    * VPBROADCASTQ r64, ymm{k}{z}
//    * VPBROADCASTQ r64, zmm{k}{z}
//    * VPBROADCASTQ xmm, xmm{k}{z}
//    * VPBROADCASTQ xmm, ymm{k}{z}
//    * VPBROADCASTQ xmm, zmm{k}{z}
//    * VPBROADCASTQ m64, xmm{k}{z}
//    * VPBROADCASTQ m64, ymm{k}{z}
//    * VPBROADCASTQ m64, zmm{k}{z}
//    * VPBROADCASTQ xmm, xmm
//    * VPBROADCASTQ m64, xmm
//    * VPBROADCASTQ xmm, ymm
//    * VPBROADCASTQ m64, ymm
//
func (self *Program) VPBROADCASTQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPBROADCASTQ r64, xmm{k}{z}
    if isReg64(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x7c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTQ r64, ymm{k}{z}
    if isReg64(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x7c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTQ r64, zmm{k}{z}
    if isReg64(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x7c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTQ xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x59)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTQ xmm, ymm{k}{z}
    if isEVEXXMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x59)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTQ xmm, zmm{k}{z}
    if isEVEXXMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x59)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTQ m64, xmm{k}{z}
    if isM64(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x59)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VPBROADCASTQ m64, ymm{k}{z}
    if isM64(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x59)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VPBROADCASTQ m64, zmm{k}{z}
    if isM64(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x59)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VPBROADCASTQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79)
            m.emit(0x59)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTQ m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x59)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VPBROADCASTQ xmm, ymm
    if isXMM(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d)
            m.emit(0x59)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTQ m64, ymm
    if isM64(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x59)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPBROADCASTQ")
    }
    return p
}

// VPBROADCASTW performs "Broadcast Word Integer".
//
// Mnemonic        : VPBROADCASTW
// ISA extensions  : AVX2, AVX512BW, AVX512VL
// Supported forms : (13 forms)
//
//    * VPBROADCASTW r32, xmm{k}{z}
//    * VPBROADCASTW r32, ymm{k}{z}
//    * VPBROADCASTW r32, zmm{k}{z}
//    * VPBROADCASTW xmm, xmm{k}{z}
//    * VPBROADCASTW xmm, ymm{k}{z}
//    * VPBROADCASTW xmm, zmm{k}{z}
//    * VPBROADCASTW m16, xmm{k}{z}
//    * VPBROADCASTW m16, ymm{k}{z}
//    * VPBROADCASTW m16, zmm{k}{z}
//    * VPBROADCASTW xmm, xmm
//    * VPBROADCASTW m16, xmm
//    * VPBROADCASTW xmm, ymm
//    * VPBROADCASTW m16, ymm
//
func (self *Program) VPBROADCASTW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPBROADCASTW r32, xmm{k}{z}
    if isReg32(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x7b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTW r32, ymm{k}{z}
    if isReg32(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x7b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTW r32, zmm{k}{z}
    if isReg32(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x7b)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTW xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTW xmm, ymm{k}{z}
    if isEVEXXMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTW xmm, zmm{k}{z}
    if isEVEXXMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTW m16, xmm{k}{z}
    if isM16(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x79)
            m.mrsd(lcode(v[1]), addr(v[0]), 2)
        })
    }
    // VPBROADCASTW m16, ymm{k}{z}
    if isM16(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x79)
            m.mrsd(lcode(v[1]), addr(v[0]), 2)
        })
    }
    // VPBROADCASTW m16, zmm{k}{z}
    if isM16(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x79)
            m.mrsd(lcode(v[1]), addr(v[0]), 2)
        })
    }
    // VPBROADCASTW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79)
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTW m16, xmm
    if isM16(v0) && isXMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x79)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VPBROADCASTW xmm, ymm
    if isXMM(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d)
            m.emit(0x79)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPBROADCASTW m16, ymm
    if isM16(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x79)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPBROADCASTW")
    }
    return p
}

// VPCLMULQDQ performs "Carry-Less Quadword Multiplication".
//
// Mnemonic        : VPCLMULQDQ
// ISA extensions  : AVX, PCLMULQDQ
// Supported forms : (2 forms)
//
//    * VPCLMULQDQ imm8, xmm, xmm, xmm
//    * VPCLMULQDQ imm8, m128, xmm, xmm
//
func (self *Program) VPCLMULQDQ(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPCLMULQDQ imm8, xmm, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX | ISA_PCLMULQDQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x44)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCLMULQDQ imm8, m128, xmm, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX | ISA_PCLMULQDQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x44)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCLMULQDQ")
    }
    return p
}

// VPCMOV performs "Packed Conditional Move".
//
// Mnemonic        : VPCMOV
// ISA extensions  : XOP
// Supported forms : (6 forms)
//
//    * VPCMOV xmm, xmm, xmm, xmm
//    * VPCMOV m128, xmm, xmm, xmm
//    * VPCMOV xmm, m128, xmm, xmm
//    * VPCMOV ymm, ymm, ymm, ymm
//    * VPCMOV m256, ymm, ymm, ymm
//    * VPCMOV ymm, m256, ymm, ymm
//
func (self *Program) VPCMOV(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPCMOV xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe8 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[2]) << 3))
            m.emit(0xa2)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe8 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf8 ^ (hlcode(v[2]) << 3))
            m.emit(0xa2)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VPCMOV m128, xmm, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1000, 0x80, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0xa2)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VPCMOV xmm, m128, xmm, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1000, 0x00, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0xa2)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VPCMOV ymm, ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe8 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7c ^ (hlcode(v[2]) << 3))
            m.emit(0xa2)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe8 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfc ^ (hlcode(v[2]) << 3))
            m.emit(0xa2)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VPCMOV m256, ymm, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1000, 0x84, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0xa2)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VPCMOV ymm, m256, ymm, ymm
    if isYMM(v0) && isM256(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1000, 0x04, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0xa2)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCMOV")
    }
    return p
}

// VPCMPB performs "Compare Packed Signed Byte Values".
//
// Mnemonic        : VPCMPB
// ISA extensions  : AVX512BW, AVX512VL
// Supported forms : (6 forms)
//
//    * VPCMPB imm8, xmm, xmm, k{k}
//    * VPCMPB imm8, m128, xmm, k{k}
//    * VPCMPB imm8, ymm, ymm, k{k}
//    * VPCMPB imm8, m256, ymm, k{k}
//    * VPCMPB imm8, zmm, zmm, k{k}
//    * VPCMPB imm8, m512, zmm, k{k}
//
func (self *Program) VPCMPB(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPCMPB imm8, xmm, xmm, k{k}
    if isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x00)
            m.emit(0x3f)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPB imm8, m128, xmm, k{k}
    if isImm8(v0) && isM128(v1) && isEVEXXMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), 0, 0)
            m.emit(0x3f)
            m.mrsd(lcode(v[3]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPB imm8, ymm, ymm, k{k}
    if isImm8(v0) && isEVEXYMM(v1) && isEVEXYMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x20)
            m.emit(0x3f)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPB imm8, m256, ymm, k{k}
    if isImm8(v0) && isM256(v1) && isEVEXYMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b01, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), 0, 0)
            m.emit(0x3f)
            m.mrsd(lcode(v[3]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPB imm8, zmm, zmm, k{k}
    if isImm8(v0) && isZMM(v1) && isZMM(v2) && isKk(v3) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x3f)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPB imm8, m512, zmm, k{k}
    if isImm8(v0) && isM512(v1) && isZMM(v2) && isKk(v3) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), 0, 0)
            m.emit(0x3f)
            m.mrsd(lcode(v[3]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCMPB")
    }
    return p
}

// VPCMPD performs "Compare Packed Signed Doubleword Values".
//
// Mnemonic        : VPCMPD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPCMPD imm8, m128/m32bcst, xmm, k{k}
//    * VPCMPD imm8, xmm, xmm, k{k}
//    * VPCMPD imm8, m256/m32bcst, ymm, k{k}
//    * VPCMPD imm8, ymm, ymm, k{k}
//    * VPCMPD imm8, m512/m32bcst, zmm, k{k}
//    * VPCMPD imm8, zmm, zmm, k{k}
//
func (self *Program) VPCMPD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPCMPD imm8, m128/m32bcst, xmm, k{k}
    if isImm8(v0) && isM128M32bcst(v1) && isEVEXXMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), 0, bcode(v[1]))
            m.emit(0x1f)
            m.mrsd(lcode(v[3]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPD imm8, xmm, xmm, k{k}
    if isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x00)
            m.emit(0x1f)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPD imm8, m256/m32bcst, ymm, k{k}
    if isImm8(v0) && isM256M32bcst(v1) && isEVEXYMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b01, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), 0, bcode(v[1]))
            m.emit(0x1f)
            m.mrsd(lcode(v[3]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPD imm8, ymm, ymm, k{k}
    if isImm8(v0) && isEVEXYMM(v1) && isEVEXYMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x20)
            m.emit(0x1f)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPD imm8, m512/m32bcst, zmm, k{k}
    if isImm8(v0) && isM512M32bcst(v1) && isZMM(v2) && isKk(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), 0, bcode(v[1]))
            m.emit(0x1f)
            m.mrsd(lcode(v[3]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPD imm8, zmm, zmm, k{k}
    if isImm8(v0) && isZMM(v1) && isZMM(v2) && isKk(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x1f)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCMPD")
    }
    return p
}

// VPCMPEQB performs "Compare Packed Byte Data for Equality".
//
// Mnemonic        : VPCMPEQB
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPCMPEQB xmm, xmm, k{k}
//    * VPCMPEQB m128, xmm, k{k}
//    * VPCMPEQB ymm, ymm, k{k}
//    * VPCMPEQB m256, ymm, k{k}
//    * VPCMPEQB zmm, zmm, k{k}
//    * VPCMPEQB m512, zmm, k{k}
//    * VPCMPEQB xmm, xmm, xmm
//    * VPCMPEQB m128, xmm, xmm
//    * VPCMPEQB ymm, ymm, ymm
//    * VPCMPEQB m256, ymm, ymm
//
func (self *Program) VPCMPEQB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPCMPEQB xmm, xmm, k{k}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x74)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPEQB m128, xmm, k{k}
    if isM128(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, 0)
            m.emit(0x74)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPCMPEQB ymm, ymm, k{k}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x74)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPEQB m256, ymm, k{k}
    if isM256(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, 0)
            m.emit(0x74)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPCMPEQB zmm, zmm, k{k}
    if isZMM(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x74)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPEQB m512, zmm, k{k}
    if isM512(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, 0)
            m.emit(0x74)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPCMPEQB xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x74)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPEQB m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x74)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPCMPEQB ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x74)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPEQB m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x74)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCMPEQB")
    }
    return p
}

// VPCMPEQD performs "Compare Packed Doubleword Data for Equality".
//
// Mnemonic        : VPCMPEQD
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPCMPEQD m128/m32bcst, xmm, k{k}
//    * VPCMPEQD xmm, xmm, k{k}
//    * VPCMPEQD m256/m32bcst, ymm, k{k}
//    * VPCMPEQD ymm, ymm, k{k}
//    * VPCMPEQD m512/m32bcst, zmm, k{k}
//    * VPCMPEQD zmm, zmm, k{k}
//    * VPCMPEQD xmm, xmm, xmm
//    * VPCMPEQD m128, xmm, xmm
//    * VPCMPEQD ymm, ymm, ymm
//    * VPCMPEQD m256, ymm, ymm
//
func (self *Program) VPCMPEQD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPCMPEQD m128/m32bcst, xmm, k{k}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, bcode(v[0]))
            m.emit(0x76)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPCMPEQD xmm, xmm, k{k}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x76)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPEQD m256/m32bcst, ymm, k{k}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, bcode(v[0]))
            m.emit(0x76)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPCMPEQD ymm, ymm, k{k}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x76)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPEQD m512/m32bcst, zmm, k{k}
    if isM512M32bcst(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, bcode(v[0]))
            m.emit(0x76)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPCMPEQD zmm, zmm, k{k}
    if isZMM(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x76)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPEQD xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x76)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPEQD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x76)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPCMPEQD ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x76)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPEQD m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x76)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCMPEQD")
    }
    return p
}

// VPCMPEQQ performs "Compare Packed Quadword Data for Equality".
//
// Mnemonic        : VPCMPEQQ
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPCMPEQQ m128/m64bcst, xmm, k{k}
//    * VPCMPEQQ xmm, xmm, k{k}
//    * VPCMPEQQ m256/m64bcst, ymm, k{k}
//    * VPCMPEQQ ymm, ymm, k{k}
//    * VPCMPEQQ m512/m64bcst, zmm, k{k}
//    * VPCMPEQQ zmm, zmm, k{k}
//    * VPCMPEQQ xmm, xmm, xmm
//    * VPCMPEQQ m128, xmm, xmm
//    * VPCMPEQQ ymm, ymm, ymm
//    * VPCMPEQQ m256, ymm, ymm
//
func (self *Program) VPCMPEQQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPCMPEQQ m128/m64bcst, xmm, k{k}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, bcode(v[0]))
            m.emit(0x29)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPCMPEQQ xmm, xmm, k{k}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x29)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPEQQ m256/m64bcst, ymm, k{k}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, bcode(v[0]))
            m.emit(0x29)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPCMPEQQ ymm, ymm, k{k}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x29)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPEQQ m512/m64bcst, zmm, k{k}
    if isM512M64bcst(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, bcode(v[0]))
            m.emit(0x29)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPCMPEQQ zmm, zmm, k{k}
    if isZMM(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x29)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPEQQ xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x29)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPEQQ m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x29)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPCMPEQQ ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x29)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPEQQ m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x29)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCMPEQQ")
    }
    return p
}

// VPCMPEQW performs "Compare Packed Word Data for Equality".
//
// Mnemonic        : VPCMPEQW
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPCMPEQW xmm, xmm, k{k}
//    * VPCMPEQW m128, xmm, k{k}
//    * VPCMPEQW ymm, ymm, k{k}
//    * VPCMPEQW m256, ymm, k{k}
//    * VPCMPEQW zmm, zmm, k{k}
//    * VPCMPEQW m512, zmm, k{k}
//    * VPCMPEQW xmm, xmm, xmm
//    * VPCMPEQW m128, xmm, xmm
//    * VPCMPEQW ymm, ymm, ymm
//    * VPCMPEQW m256, ymm, ymm
//
func (self *Program) VPCMPEQW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPCMPEQW xmm, xmm, k{k}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x75)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPEQW m128, xmm, k{k}
    if isM128(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, 0)
            m.emit(0x75)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPCMPEQW ymm, ymm, k{k}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x75)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPEQW m256, ymm, k{k}
    if isM256(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, 0)
            m.emit(0x75)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPCMPEQW zmm, zmm, k{k}
    if isZMM(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x75)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPEQW m512, zmm, k{k}
    if isM512(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, 0)
            m.emit(0x75)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPCMPEQW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x75)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPEQW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x75)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPCMPEQW ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x75)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPEQW m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x75)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCMPEQW")
    }
    return p
}

// VPCMPESTRI performs "Packed Compare Explicit Length Strings, Return Index".
//
// Mnemonic        : VPCMPESTRI
// ISA extensions  : AVX
// Supported forms : (2 forms)
//
//    * VPCMPESTRI imm8, xmm, xmm
//    * VPCMPESTRI imm8, m128, xmm
//
func (self *Program) VPCMPESTRI(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPCMPESTRI imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79)
            m.emit(0x61)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPESTRI imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[2]), addr(v[1]), 0)
            m.emit(0x61)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCMPESTRI")
    }
    return p
}

// VPCMPESTRM performs "Packed Compare Explicit Length Strings, Return Mask".
//
// Mnemonic        : VPCMPESTRM
// ISA extensions  : AVX
// Supported forms : (2 forms)
//
//    * VPCMPESTRM imm8, xmm, xmm
//    * VPCMPESTRM imm8, m128, xmm
//
func (self *Program) VPCMPESTRM(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPCMPESTRM imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79)
            m.emit(0x60)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPESTRM imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[2]), addr(v[1]), 0)
            m.emit(0x60)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCMPESTRM")
    }
    return p
}

// VPCMPGTB performs "Compare Packed Signed Byte Integers for Greater Than".
//
// Mnemonic        : VPCMPGTB
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPCMPGTB xmm, xmm, k{k}
//    * VPCMPGTB m128, xmm, k{k}
//    * VPCMPGTB ymm, ymm, k{k}
//    * VPCMPGTB m256, ymm, k{k}
//    * VPCMPGTB zmm, zmm, k{k}
//    * VPCMPGTB m512, zmm, k{k}
//    * VPCMPGTB xmm, xmm, xmm
//    * VPCMPGTB m128, xmm, xmm
//    * VPCMPGTB ymm, ymm, ymm
//    * VPCMPGTB m256, ymm, ymm
//
func (self *Program) VPCMPGTB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPCMPGTB xmm, xmm, k{k}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x64)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPGTB m128, xmm, k{k}
    if isM128(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, 0)
            m.emit(0x64)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPCMPGTB ymm, ymm, k{k}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x64)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPGTB m256, ymm, k{k}
    if isM256(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, 0)
            m.emit(0x64)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPCMPGTB zmm, zmm, k{k}
    if isZMM(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x64)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPGTB m512, zmm, k{k}
    if isM512(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, 0)
            m.emit(0x64)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPCMPGTB xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x64)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPGTB m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x64)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPCMPGTB ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x64)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPGTB m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x64)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCMPGTB")
    }
    return p
}

// VPCMPGTD performs "Compare Packed Signed Doubleword Integers for Greater Than".
//
// Mnemonic        : VPCMPGTD
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPCMPGTD m128/m32bcst, xmm, k{k}
//    * VPCMPGTD xmm, xmm, k{k}
//    * VPCMPGTD m256/m32bcst, ymm, k{k}
//    * VPCMPGTD ymm, ymm, k{k}
//    * VPCMPGTD m512/m32bcst, zmm, k{k}
//    * VPCMPGTD zmm, zmm, k{k}
//    * VPCMPGTD xmm, xmm, xmm
//    * VPCMPGTD m128, xmm, xmm
//    * VPCMPGTD ymm, ymm, ymm
//    * VPCMPGTD m256, ymm, ymm
//
func (self *Program) VPCMPGTD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPCMPGTD m128/m32bcst, xmm, k{k}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, bcode(v[0]))
            m.emit(0x66)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPCMPGTD xmm, xmm, k{k}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x66)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPGTD m256/m32bcst, ymm, k{k}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, bcode(v[0]))
            m.emit(0x66)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPCMPGTD ymm, ymm, k{k}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x66)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPGTD m512/m32bcst, zmm, k{k}
    if isM512M32bcst(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, bcode(v[0]))
            m.emit(0x66)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPCMPGTD zmm, zmm, k{k}
    if isZMM(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x66)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPGTD xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x66)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPGTD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x66)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPCMPGTD ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x66)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPGTD m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x66)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCMPGTD")
    }
    return p
}

// VPCMPGTQ performs "Compare Packed Data for Greater Than".
//
// Mnemonic        : VPCMPGTQ
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPCMPGTQ m128/m64bcst, xmm, k{k}
//    * VPCMPGTQ xmm, xmm, k{k}
//    * VPCMPGTQ m256/m64bcst, ymm, k{k}
//    * VPCMPGTQ ymm, ymm, k{k}
//    * VPCMPGTQ m512/m64bcst, zmm, k{k}
//    * VPCMPGTQ zmm, zmm, k{k}
//    * VPCMPGTQ xmm, xmm, xmm
//    * VPCMPGTQ m128, xmm, xmm
//    * VPCMPGTQ ymm, ymm, ymm
//    * VPCMPGTQ m256, ymm, ymm
//
func (self *Program) VPCMPGTQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPCMPGTQ m128/m64bcst, xmm, k{k}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, bcode(v[0]))
            m.emit(0x37)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPCMPGTQ xmm, xmm, k{k}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x37)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPGTQ m256/m64bcst, ymm, k{k}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, bcode(v[0]))
            m.emit(0x37)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPCMPGTQ ymm, ymm, k{k}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x37)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPGTQ m512/m64bcst, zmm, k{k}
    if isM512M64bcst(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, bcode(v[0]))
            m.emit(0x37)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPCMPGTQ zmm, zmm, k{k}
    if isZMM(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x37)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPGTQ xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x37)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPGTQ m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x37)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPCMPGTQ ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x37)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPGTQ m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x37)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCMPGTQ")
    }
    return p
}

// VPCMPGTW performs "Compare Packed Signed Word Integers for Greater Than".
//
// Mnemonic        : VPCMPGTW
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPCMPGTW xmm, xmm, k{k}
//    * VPCMPGTW m128, xmm, k{k}
//    * VPCMPGTW ymm, ymm, k{k}
//    * VPCMPGTW m256, ymm, k{k}
//    * VPCMPGTW zmm, zmm, k{k}
//    * VPCMPGTW m512, zmm, k{k}
//    * VPCMPGTW xmm, xmm, xmm
//    * VPCMPGTW m128, xmm, xmm
//    * VPCMPGTW ymm, ymm, ymm
//    * VPCMPGTW m256, ymm, ymm
//
func (self *Program) VPCMPGTW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPCMPGTW xmm, xmm, k{k}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x65)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPGTW m128, xmm, k{k}
    if isM128(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, 0)
            m.emit(0x65)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPCMPGTW ymm, ymm, k{k}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x65)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPGTW m256, ymm, k{k}
    if isM256(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, 0)
            m.emit(0x65)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPCMPGTW zmm, zmm, k{k}
    if isZMM(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x65)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPGTW m512, zmm, k{k}
    if isM512(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, 0)
            m.emit(0x65)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPCMPGTW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x65)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPGTW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x65)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPCMPGTW ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x65)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPCMPGTW m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x65)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCMPGTW")
    }
    return p
}

// VPCMPISTRI performs "Packed Compare Implicit Length Strings, Return Index".
//
// Mnemonic        : VPCMPISTRI
// ISA extensions  : AVX
// Supported forms : (2 forms)
//
//    * VPCMPISTRI imm8, xmm, xmm
//    * VPCMPISTRI imm8, m128, xmm
//
func (self *Program) VPCMPISTRI(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPCMPISTRI imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79)
            m.emit(0x63)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPISTRI imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[2]), addr(v[1]), 0)
            m.emit(0x63)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCMPISTRI")
    }
    return p
}

// VPCMPISTRM performs "Packed Compare Implicit Length Strings, Return Mask".
//
// Mnemonic        : VPCMPISTRM
// ISA extensions  : AVX
// Supported forms : (2 forms)
//
//    * VPCMPISTRM imm8, xmm, xmm
//    * VPCMPISTRM imm8, m128, xmm
//
func (self *Program) VPCMPISTRM(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPCMPISTRM imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79)
            m.emit(0x62)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPISTRM imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[2]), addr(v[1]), 0)
            m.emit(0x62)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCMPISTRM")
    }
    return p
}

// VPCMPQ performs "Compare Packed Signed Quadword Values".
//
// Mnemonic        : VPCMPQ
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPCMPQ imm8, m128/m64bcst, xmm, k{k}
//    * VPCMPQ imm8, xmm, xmm, k{k}
//    * VPCMPQ imm8, m256/m64bcst, ymm, k{k}
//    * VPCMPQ imm8, ymm, ymm, k{k}
//    * VPCMPQ imm8, m512/m64bcst, zmm, k{k}
//    * VPCMPQ imm8, zmm, zmm, k{k}
//
func (self *Program) VPCMPQ(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPCMPQ imm8, m128/m64bcst, xmm, k{k}
    if isImm8(v0) && isM128M64bcst(v1) && isEVEXXMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), 0, bcode(v[1]))
            m.emit(0x1f)
            m.mrsd(lcode(v[3]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPQ imm8, xmm, xmm, k{k}
    if isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x00)
            m.emit(0x1f)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPQ imm8, m256/m64bcst, ymm, k{k}
    if isImm8(v0) && isM256M64bcst(v1) && isEVEXYMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b01, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), 0, bcode(v[1]))
            m.emit(0x1f)
            m.mrsd(lcode(v[3]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPQ imm8, ymm, ymm, k{k}
    if isImm8(v0) && isEVEXYMM(v1) && isEVEXYMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x20)
            m.emit(0x1f)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPQ imm8, m512/m64bcst, zmm, k{k}
    if isImm8(v0) && isM512M64bcst(v1) && isZMM(v2) && isKk(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), 0, bcode(v[1]))
            m.emit(0x1f)
            m.mrsd(lcode(v[3]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPQ imm8, zmm, zmm, k{k}
    if isImm8(v0) && isZMM(v1) && isZMM(v2) && isKk(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x1f)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCMPQ")
    }
    return p
}

// VPCMPUB performs "Compare Packed Unsigned Byte Values".
//
// Mnemonic        : VPCMPUB
// ISA extensions  : AVX512BW, AVX512VL
// Supported forms : (6 forms)
//
//    * VPCMPUB imm8, xmm, xmm, k{k}
//    * VPCMPUB imm8, m128, xmm, k{k}
//    * VPCMPUB imm8, ymm, ymm, k{k}
//    * VPCMPUB imm8, m256, ymm, k{k}
//    * VPCMPUB imm8, zmm, zmm, k{k}
//    * VPCMPUB imm8, m512, zmm, k{k}
//
func (self *Program) VPCMPUB(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPCMPUB imm8, xmm, xmm, k{k}
    if isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x00)
            m.emit(0x3e)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPUB imm8, m128, xmm, k{k}
    if isImm8(v0) && isM128(v1) && isEVEXXMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), 0, 0)
            m.emit(0x3e)
            m.mrsd(lcode(v[3]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPUB imm8, ymm, ymm, k{k}
    if isImm8(v0) && isEVEXYMM(v1) && isEVEXYMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x20)
            m.emit(0x3e)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPUB imm8, m256, ymm, k{k}
    if isImm8(v0) && isM256(v1) && isEVEXYMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b01, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), 0, 0)
            m.emit(0x3e)
            m.mrsd(lcode(v[3]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPUB imm8, zmm, zmm, k{k}
    if isImm8(v0) && isZMM(v1) && isZMM(v2) && isKk(v3) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x3e)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPUB imm8, m512, zmm, k{k}
    if isImm8(v0) && isM512(v1) && isZMM(v2) && isKk(v3) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), 0, 0)
            m.emit(0x3e)
            m.mrsd(lcode(v[3]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCMPUB")
    }
    return p
}

// VPCMPUD performs "Compare Packed Unsigned Doubleword Values".
//
// Mnemonic        : VPCMPUD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPCMPUD imm8, m128/m32bcst, xmm, k{k}
//    * VPCMPUD imm8, xmm, xmm, k{k}
//    * VPCMPUD imm8, m256/m32bcst, ymm, k{k}
//    * VPCMPUD imm8, ymm, ymm, k{k}
//    * VPCMPUD imm8, m512/m32bcst, zmm, k{k}
//    * VPCMPUD imm8, zmm, zmm, k{k}
//
func (self *Program) VPCMPUD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPCMPUD imm8, m128/m32bcst, xmm, k{k}
    if isImm8(v0) && isM128M32bcst(v1) && isEVEXXMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), 0, bcode(v[1]))
            m.emit(0x1e)
            m.mrsd(lcode(v[3]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPUD imm8, xmm, xmm, k{k}
    if isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x00)
            m.emit(0x1e)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPUD imm8, m256/m32bcst, ymm, k{k}
    if isImm8(v0) && isM256M32bcst(v1) && isEVEXYMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b01, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), 0, bcode(v[1]))
            m.emit(0x1e)
            m.mrsd(lcode(v[3]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPUD imm8, ymm, ymm, k{k}
    if isImm8(v0) && isEVEXYMM(v1) && isEVEXYMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x20)
            m.emit(0x1e)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPUD imm8, m512/m32bcst, zmm, k{k}
    if isImm8(v0) && isM512M32bcst(v1) && isZMM(v2) && isKk(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), 0, bcode(v[1]))
            m.emit(0x1e)
            m.mrsd(lcode(v[3]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPUD imm8, zmm, zmm, k{k}
    if isImm8(v0) && isZMM(v1) && isZMM(v2) && isKk(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x1e)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCMPUD")
    }
    return p
}

// VPCMPUQ performs "Compare Packed Unsigned Quadword Values".
//
// Mnemonic        : VPCMPUQ
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPCMPUQ imm8, m128/m64bcst, xmm, k{k}
//    * VPCMPUQ imm8, xmm, xmm, k{k}
//    * VPCMPUQ imm8, m256/m64bcst, ymm, k{k}
//    * VPCMPUQ imm8, ymm, ymm, k{k}
//    * VPCMPUQ imm8, m512/m64bcst, zmm, k{k}
//    * VPCMPUQ imm8, zmm, zmm, k{k}
//
func (self *Program) VPCMPUQ(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPCMPUQ imm8, m128/m64bcst, xmm, k{k}
    if isImm8(v0) && isM128M64bcst(v1) && isEVEXXMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), 0, bcode(v[1]))
            m.emit(0x1e)
            m.mrsd(lcode(v[3]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPUQ imm8, xmm, xmm, k{k}
    if isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x00)
            m.emit(0x1e)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPUQ imm8, m256/m64bcst, ymm, k{k}
    if isImm8(v0) && isM256M64bcst(v1) && isEVEXYMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b01, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), 0, bcode(v[1]))
            m.emit(0x1e)
            m.mrsd(lcode(v[3]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPUQ imm8, ymm, ymm, k{k}
    if isImm8(v0) && isEVEXYMM(v1) && isEVEXYMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x20)
            m.emit(0x1e)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPUQ imm8, m512/m64bcst, zmm, k{k}
    if isImm8(v0) && isM512M64bcst(v1) && isZMM(v2) && isKk(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), 0, bcode(v[1]))
            m.emit(0x1e)
            m.mrsd(lcode(v[3]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPUQ imm8, zmm, zmm, k{k}
    if isImm8(v0) && isZMM(v1) && isZMM(v2) && isKk(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x1e)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCMPUQ")
    }
    return p
}

// VPCMPUW performs "Compare Packed Unsigned Word Values".
//
// Mnemonic        : VPCMPUW
// ISA extensions  : AVX512BW, AVX512VL
// Supported forms : (6 forms)
//
//    * VPCMPUW imm8, xmm, xmm, k{k}
//    * VPCMPUW imm8, m128, xmm, k{k}
//    * VPCMPUW imm8, ymm, ymm, k{k}
//    * VPCMPUW imm8, m256, ymm, k{k}
//    * VPCMPUW imm8, zmm, zmm, k{k}
//    * VPCMPUW imm8, m512, zmm, k{k}
//
func (self *Program) VPCMPUW(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPCMPUW imm8, xmm, xmm, k{k}
    if isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x00)
            m.emit(0x3e)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPUW imm8, m128, xmm, k{k}
    if isImm8(v0) && isM128(v1) && isEVEXXMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), 0, 0)
            m.emit(0x3e)
            m.mrsd(lcode(v[3]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPUW imm8, ymm, ymm, k{k}
    if isImm8(v0) && isEVEXYMM(v1) && isEVEXYMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x20)
            m.emit(0x3e)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPUW imm8, m256, ymm, k{k}
    if isImm8(v0) && isM256(v1) && isEVEXYMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b01, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), 0, 0)
            m.emit(0x3e)
            m.mrsd(lcode(v[3]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPUW imm8, zmm, zmm, k{k}
    if isImm8(v0) && isZMM(v1) && isZMM(v2) && isKk(v3) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x3e)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPUW imm8, m512, zmm, k{k}
    if isImm8(v0) && isM512(v1) && isZMM(v2) && isKk(v3) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), 0, 0)
            m.emit(0x3e)
            m.mrsd(lcode(v[3]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCMPUW")
    }
    return p
}

// VPCMPW performs "Compare Packed Signed Word Values".
//
// Mnemonic        : VPCMPW
// ISA extensions  : AVX512BW, AVX512VL
// Supported forms : (6 forms)
//
//    * VPCMPW imm8, xmm, xmm, k{k}
//    * VPCMPW imm8, m128, xmm, k{k}
//    * VPCMPW imm8, ymm, ymm, k{k}
//    * VPCMPW imm8, m256, ymm, k{k}
//    * VPCMPW imm8, zmm, zmm, k{k}
//    * VPCMPW imm8, m512, zmm, k{k}
//
func (self *Program) VPCMPW(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPCMPW imm8, xmm, xmm, k{k}
    if isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x00)
            m.emit(0x3f)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPW imm8, m128, xmm, k{k}
    if isImm8(v0) && isM128(v1) && isEVEXXMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), 0, 0)
            m.emit(0x3f)
            m.mrsd(lcode(v[3]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPW imm8, ymm, ymm, k{k}
    if isImm8(v0) && isEVEXYMM(v1) && isEVEXYMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x20)
            m.emit(0x3f)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPW imm8, m256, ymm, k{k}
    if isImm8(v0) && isM256(v1) && isEVEXYMM(v2) && isKk(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b01, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), 0, 0)
            m.emit(0x3f)
            m.mrsd(lcode(v[3]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPW imm8, zmm, zmm, k{k}
    if isImm8(v0) && isZMM(v1) && isZMM(v2) && isKk(v3) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x3f)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCMPW imm8, m512, zmm, k{k}
    if isImm8(v0) && isM512(v1) && isZMM(v2) && isKk(v3) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), 0, 0)
            m.emit(0x3f)
            m.mrsd(lcode(v[3]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCMPW")
    }
    return p
}

// VPCOMB performs "Compare Packed Signed Byte Integers".
//
// Mnemonic        : VPCOMB
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPCOMB imm8, xmm, xmm, xmm
//    * VPCOMB imm8, m128, xmm, xmm
//
func (self *Program) VPCOMB(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPCOMB imm8, xmm, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe8 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[2]) << 3))
            m.emit(0xcc)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCOMB imm8, m128, xmm, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1000, 0x00, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0xcc)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCOMB")
    }
    return p
}

// VPCOMD performs "Compare Packed Signed Doubleword Integers".
//
// Mnemonic        : VPCOMD
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPCOMD imm8, xmm, xmm, xmm
//    * VPCOMD imm8, m128, xmm, xmm
//
func (self *Program) VPCOMD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPCOMD imm8, xmm, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe8 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[2]) << 3))
            m.emit(0xce)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCOMD imm8, m128, xmm, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1000, 0x00, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0xce)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCOMD")
    }
    return p
}

// VPCOMPRESSD performs "Store Sparse Packed Doubleword Integer Values into Dense Memory/Register".
//
// Mnemonic        : VPCOMPRESSD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPCOMPRESSD xmm, xmm{k}{z}
//    * VPCOMPRESSD xmm, m128{k}{z}
//    * VPCOMPRESSD ymm, ymm{k}{z}
//    * VPCOMPRESSD ymm, m256{k}{z}
//    * VPCOMPRESSD zmm, zmm{k}{z}
//    * VPCOMPRESSD zmm, m512{k}{z}
//
func (self *Program) VPCOMPRESSD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPCOMPRESSD xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x8b)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPCOMPRESSD xmm, m128{k}{z}
    if isEVEXXMM(v0) && isM128kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x8b)
            m.mrsd(lcode(v[0]), addr(v[1]), 4)
        })
    }
    // VPCOMPRESSD ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x8b)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPCOMPRESSD ymm, m256{k}{z}
    if isEVEXYMM(v0) && isM256kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x8b)
            m.mrsd(lcode(v[0]), addr(v[1]), 4)
        })
    }
    // VPCOMPRESSD zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x8b)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPCOMPRESSD zmm, m512{k}{z}
    if isZMM(v0) && isM512kz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x8b)
            m.mrsd(lcode(v[0]), addr(v[1]), 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCOMPRESSD")
    }
    return p
}

// VPCOMPRESSQ performs "Store Sparse Packed Quadword Integer Values into Dense Memory/Register".
//
// Mnemonic        : VPCOMPRESSQ
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPCOMPRESSQ xmm, xmm{k}{z}
//    * VPCOMPRESSQ xmm, m128{k}{z}
//    * VPCOMPRESSQ ymm, ymm{k}{z}
//    * VPCOMPRESSQ ymm, m256{k}{z}
//    * VPCOMPRESSQ zmm, zmm{k}{z}
//    * VPCOMPRESSQ zmm, m512{k}{z}
//
func (self *Program) VPCOMPRESSQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPCOMPRESSQ xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x8b)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPCOMPRESSQ xmm, m128{k}{z}
    if isEVEXXMM(v0) && isM128kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x8b)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    // VPCOMPRESSQ ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x8b)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPCOMPRESSQ ymm, m256{k}{z}
    if isEVEXYMM(v0) && isM256kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x8b)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    // VPCOMPRESSQ zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x8b)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPCOMPRESSQ zmm, m512{k}{z}
    if isZMM(v0) && isM512kz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x8b)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCOMPRESSQ")
    }
    return p
}

// VPCOMQ performs "Compare Packed Signed Quadword Integers".
//
// Mnemonic        : VPCOMQ
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPCOMQ imm8, xmm, xmm, xmm
//    * VPCOMQ imm8, m128, xmm, xmm
//
func (self *Program) VPCOMQ(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPCOMQ imm8, xmm, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe8 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[2]) << 3))
            m.emit(0xcf)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCOMQ imm8, m128, xmm, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1000, 0x00, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0xcf)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCOMQ")
    }
    return p
}

// VPCOMUB performs "Compare Packed Unsigned Byte Integers".
//
// Mnemonic        : VPCOMUB
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPCOMUB imm8, xmm, xmm, xmm
//    * VPCOMUB imm8, m128, xmm, xmm
//
func (self *Program) VPCOMUB(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPCOMUB imm8, xmm, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe8 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[2]) << 3))
            m.emit(0xec)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCOMUB imm8, m128, xmm, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1000, 0x00, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0xec)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCOMUB")
    }
    return p
}

// VPCOMUD performs "Compare Packed Unsigned Doubleword Integers".
//
// Mnemonic        : VPCOMUD
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPCOMUD imm8, xmm, xmm, xmm
//    * VPCOMUD imm8, m128, xmm, xmm
//
func (self *Program) VPCOMUD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPCOMUD imm8, xmm, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe8 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[2]) << 3))
            m.emit(0xee)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCOMUD imm8, m128, xmm, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1000, 0x00, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0xee)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCOMUD")
    }
    return p
}

// VPCOMUQ performs "Compare Packed Unsigned Quadword Integers".
//
// Mnemonic        : VPCOMUQ
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPCOMUQ imm8, xmm, xmm, xmm
//    * VPCOMUQ imm8, m128, xmm, xmm
//
func (self *Program) VPCOMUQ(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPCOMUQ imm8, xmm, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe8 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[2]) << 3))
            m.emit(0xef)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCOMUQ imm8, m128, xmm, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1000, 0x00, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0xef)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCOMUQ")
    }
    return p
}

// VPCOMUW performs "Compare Packed Unsigned Word Integers".
//
// Mnemonic        : VPCOMUW
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPCOMUW imm8, xmm, xmm, xmm
//    * VPCOMUW imm8, m128, xmm, xmm
//
func (self *Program) VPCOMUW(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPCOMUW imm8, xmm, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe8 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[2]) << 3))
            m.emit(0xed)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCOMUW imm8, m128, xmm, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1000, 0x00, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0xed)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCOMUW")
    }
    return p
}

// VPCOMW performs "Compare Packed Signed Word Integers".
//
// Mnemonic        : VPCOMW
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPCOMW imm8, xmm, xmm, xmm
//    * VPCOMW imm8, m128, xmm, xmm
//
func (self *Program) VPCOMW(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPCOMW imm8, xmm, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe8 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[2]) << 3))
            m.emit(0xcd)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPCOMW imm8, m128, xmm, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1000, 0x00, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0xcd)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCOMW")
    }
    return p
}

// VPCONFLICTD performs "Detect Conflicts Within a Vector of Packed Doubleword Values into Dense Memory/Register".
//
// Mnemonic        : VPCONFLICTD
// ISA extensions  : AVX512CD, AVX512VL
// Supported forms : (6 forms)
//
//    * VPCONFLICTD m128/m32bcst, xmm{k}{z}
//    * VPCONFLICTD m256/m32bcst, ymm{k}{z}
//    * VPCONFLICTD m512/m32bcst, zmm{k}{z}
//    * VPCONFLICTD xmm, xmm{k}{z}
//    * VPCONFLICTD ymm, ymm{k}{z}
//    * VPCONFLICTD zmm, zmm{k}{z}
//
func (self *Program) VPCONFLICTD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPCONFLICTD m128/m32bcst, xmm{k}{z}
    if isM128M32bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512CD)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0xc4)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VPCONFLICTD m256/m32bcst, ymm{k}{z}
    if isM256M32bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512CD)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0xc4)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VPCONFLICTD m512/m32bcst, zmm{k}{z}
    if isM512M32bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512CD)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0xc4)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VPCONFLICTD xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512CD)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0xc4)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPCONFLICTD ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512CD)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0xc4)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPCONFLICTD zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512CD)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0xc4)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCONFLICTD")
    }
    return p
}

// VPCONFLICTQ performs "Detect Conflicts Within a Vector of Packed Quadword Values into Dense Memory/Register".
//
// Mnemonic        : VPCONFLICTQ
// ISA extensions  : AVX512CD, AVX512VL
// Supported forms : (6 forms)
//
//    * VPCONFLICTQ m128/m64bcst, xmm{k}{z}
//    * VPCONFLICTQ m256/m64bcst, ymm{k}{z}
//    * VPCONFLICTQ m512/m64bcst, zmm{k}{z}
//    * VPCONFLICTQ xmm, xmm{k}{z}
//    * VPCONFLICTQ ymm, ymm{k}{z}
//    * VPCONFLICTQ zmm, zmm{k}{z}
//
func (self *Program) VPCONFLICTQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPCONFLICTQ m128/m64bcst, xmm{k}{z}
    if isM128M64bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512CD)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0xc4)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VPCONFLICTQ m256/m64bcst, ymm{k}{z}
    if isM256M64bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512CD)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0xc4)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VPCONFLICTQ m512/m64bcst, zmm{k}{z}
    if isM512M64bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512CD)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0xc4)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VPCONFLICTQ xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512CD)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0xc4)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPCONFLICTQ ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512CD)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0xc4)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPCONFLICTQ zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512CD)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0xc4)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPCONFLICTQ")
    }
    return p
}

// VPERM2F128 performs "Permute Floating-Point Values".
//
// Mnemonic        : VPERM2F128
// ISA extensions  : AVX
// Supported forms : (2 forms)
//
//    * VPERM2F128 imm8, ymm, ymm, ymm
//    * VPERM2F128 imm8, m256, ymm, ymm
//
func (self *Program) VPERM2F128(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPERM2F128 imm8, ymm, ymm, ymm
    if isImm8(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit(0x06)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPERM2F128 imm8, m256, ymm, ymm
    if isImm8(v0) && isM256(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x06)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPERM2F128")
    }
    return p
}

// VPERM2I128 performs "Permute 128-Bit Integer Values".
//
// Mnemonic        : VPERM2I128
// ISA extensions  : AVX2
// Supported forms : (2 forms)
//
//    * VPERM2I128 imm8, ymm, ymm, ymm
//    * VPERM2I128 imm8, m256, ymm, ymm
//
func (self *Program) VPERM2I128(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPERM2I128 imm8, ymm, ymm, ymm
    if isImm8(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit(0x46)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPERM2I128 imm8, m256, ymm, ymm
    if isImm8(v0) && isM256(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x46)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPERM2I128")
    }
    return p
}

// VPERMB performs "Permute Byte Integers".
//
// Mnemonic        : VPERMB
// ISA extensions  : AVX512VBMI, AVX512VL
// Supported forms : (6 forms)
//
//    * VPERMB xmm, xmm, xmm{k}{z}
//    * VPERMB m128, xmm, xmm{k}{z}
//    * VPERMB ymm, ymm, ymm{k}{z}
//    * VPERMB m256, ymm, ymm{k}{z}
//    * VPERMB zmm, zmm, zmm{k}{z}
//    * VPERMB m512, zmm, zmm{k}{z}
//
func (self *Program) VPERMB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPERMB xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512VBMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x8d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMB m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512VBMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x8d)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPERMB ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512VBMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x8d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMB m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512VBMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x8d)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPERMB zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512VBMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x8d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMB m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512VBMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x8d)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPERMB")
    }
    return p
}

// VPERMD performs "Permute Doubleword Integers".
//
// Mnemonic        : VPERMD
// ISA extensions  : AVX2, AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPERMD m256/m32bcst, ymm, ymm{k}{z}
//    * VPERMD ymm, ymm, ymm{k}{z}
//    * VPERMD m512/m32bcst, zmm, zmm{k}{z}
//    * VPERMD zmm, zmm, zmm{k}{z}
//    * VPERMD ymm, ymm, ymm
//    * VPERMD m256, ymm, ymm
//
func (self *Program) VPERMD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPERMD m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x36)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPERMD ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x36)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMD m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x36)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPERMD zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x36)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMD ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x36)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMD m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x36)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPERMD")
    }
    return p
}

// VPERMI2B performs "Full Permute of Bytes From Two Tables Overwriting the Index".
//
// Mnemonic        : VPERMI2B
// ISA extensions  : AVX512VBMI, AVX512VL
// Supported forms : (6 forms)
//
//    * VPERMI2B xmm, xmm, xmm{k}{z}
//    * VPERMI2B m128, xmm, xmm{k}{z}
//    * VPERMI2B ymm, ymm, ymm{k}{z}
//    * VPERMI2B m256, ymm, ymm{k}{z}
//    * VPERMI2B zmm, zmm, zmm{k}{z}
//    * VPERMI2B m512, zmm, zmm{k}{z}
//
func (self *Program) VPERMI2B(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPERMI2B xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512VBMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x75)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMI2B m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512VBMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x75)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPERMI2B ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512VBMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x75)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMI2B m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512VBMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x75)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPERMI2B zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512VBMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x75)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMI2B m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512VBMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x75)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPERMI2B")
    }
    return p
}

// VPERMI2D performs "Full Permute of Doublewords From Two Tables Overwriting the Index".
//
// Mnemonic        : VPERMI2D
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPERMI2D m128/m32bcst, xmm, xmm{k}{z}
//    * VPERMI2D xmm, xmm, xmm{k}{z}
//    * VPERMI2D m256/m32bcst, ymm, ymm{k}{z}
//    * VPERMI2D ymm, ymm, ymm{k}{z}
//    * VPERMI2D m512/m32bcst, zmm, zmm{k}{z}
//    * VPERMI2D zmm, zmm, zmm{k}{z}
//
func (self *Program) VPERMI2D(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPERMI2D m128/m32bcst, xmm, xmm{k}{z}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x76)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPERMI2D xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x76)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMI2D m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x76)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPERMI2D ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x76)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMI2D m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x76)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPERMI2D zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x76)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPERMI2D")
    }
    return p
}

// VPERMI2PD performs "Full Permute of Double-Precision Floating-Point Values From Two Tables Overwriting the Index".
//
// Mnemonic        : VPERMI2PD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPERMI2PD m128/m64bcst, xmm, xmm{k}{z}
//    * VPERMI2PD xmm, xmm, xmm{k}{z}
//    * VPERMI2PD m256/m64bcst, ymm, ymm{k}{z}
//    * VPERMI2PD ymm, ymm, ymm{k}{z}
//    * VPERMI2PD m512/m64bcst, zmm, zmm{k}{z}
//    * VPERMI2PD zmm, zmm, zmm{k}{z}
//
func (self *Program) VPERMI2PD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPERMI2PD m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x77)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPERMI2PD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x77)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMI2PD m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x77)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPERMI2PD ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x77)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMI2PD m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x77)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPERMI2PD zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x77)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPERMI2PD")
    }
    return p
}

// VPERMI2PS performs "Full Permute of Single-Precision Floating-Point Values From Two Tables Overwriting the Index".
//
// Mnemonic        : VPERMI2PS
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPERMI2PS m128/m32bcst, xmm, xmm{k}{z}
//    * VPERMI2PS xmm, xmm, xmm{k}{z}
//    * VPERMI2PS m256/m32bcst, ymm, ymm{k}{z}
//    * VPERMI2PS ymm, ymm, ymm{k}{z}
//    * VPERMI2PS m512/m32bcst, zmm, zmm{k}{z}
//    * VPERMI2PS zmm, zmm, zmm{k}{z}
//
func (self *Program) VPERMI2PS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPERMI2PS m128/m32bcst, xmm, xmm{k}{z}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x77)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPERMI2PS xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x77)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMI2PS m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x77)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPERMI2PS ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x77)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMI2PS m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x77)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPERMI2PS zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x77)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPERMI2PS")
    }
    return p
}

// VPERMI2Q performs "Full Permute of Quadwords From Two Tables Overwriting the Index".
//
// Mnemonic        : VPERMI2Q
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPERMI2Q m128/m64bcst, xmm, xmm{k}{z}
//    * VPERMI2Q xmm, xmm, xmm{k}{z}
//    * VPERMI2Q m256/m64bcst, ymm, ymm{k}{z}
//    * VPERMI2Q ymm, ymm, ymm{k}{z}
//    * VPERMI2Q m512/m64bcst, zmm, zmm{k}{z}
//    * VPERMI2Q zmm, zmm, zmm{k}{z}
//
func (self *Program) VPERMI2Q(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPERMI2Q m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x76)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPERMI2Q xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x76)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMI2Q m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x76)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPERMI2Q ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x76)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMI2Q m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x76)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPERMI2Q zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x76)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPERMI2Q")
    }
    return p
}

// VPERMI2W performs "Full Permute of Words From Two Tables Overwriting the Index".
//
// Mnemonic        : VPERMI2W
// ISA extensions  : AVX512BW, AVX512VL
// Supported forms : (6 forms)
//
//    * VPERMI2W xmm, xmm, xmm{k}{z}
//    * VPERMI2W m128, xmm, xmm{k}{z}
//    * VPERMI2W ymm, ymm, ymm{k}{z}
//    * VPERMI2W m256, ymm, ymm{k}{z}
//    * VPERMI2W zmm, zmm, zmm{k}{z}
//    * VPERMI2W m512, zmm, zmm{k}{z}
//
func (self *Program) VPERMI2W(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPERMI2W xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x75)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMI2W m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x75)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPERMI2W ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x75)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMI2W m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x75)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPERMI2W zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x75)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMI2W m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x75)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPERMI2W")
    }
    return p
}

// VPERMIL2PD performs "Permute Two-Source Double-Precision Floating-Point Vectors".
//
// Mnemonic        : VPERMIL2PD
// ISA extensions  : XOP
// Supported forms : (6 forms)
//
//    * VPERMIL2PD imm4, xmm, xmm, xmm, xmm
//    * VPERMIL2PD imm4, m128, xmm, xmm, xmm
//    * VPERMIL2PD imm4, xmm, m128, xmm, xmm
//    * VPERMIL2PD imm4, ymm, ymm, ymm, ymm
//    * VPERMIL2PD imm4, m256, ymm, ymm, ymm
//    * VPERMIL2PD imm4, ymm, m256, ymm, ymm
//
func (self *Program) VPERMIL2PD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}, v4 interface{}) *Instruction {
    p := self.alloc(5, Operands{v0, v1, v2, v3, v4})
    // VPERMIL2PD imm4, xmm, xmm, xmm, xmm
    if isImm4(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) && isXMM(v4) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[4]) << 7) ^ (hcode(v[2]) << 5))
            m.emit(0x79 ^ (hlcode(v[3]) << 3))
            m.emit(0x49)
            m.emit(0xc0 | lcode(v[4]) << 3 | lcode(v[2]))
            m.emit((hlcode(v[1]) << 4) | imml(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[4]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0xf9 ^ (hlcode(v[3]) << 3))
            m.emit(0x49)
            m.emit(0xc0 | lcode(v[4]) << 3 | lcode(v[1]))
            m.emit((hlcode(v[2]) << 4) | imml(v[0]))
        })
    }
    // VPERMIL2PD imm4, m128, xmm, xmm, xmm
    if isImm4(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) && isXMM(v4) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x81, hcode(v[4]), addr(v[1]), hlcode(v[3]))
            m.emit(0x49)
            m.mrsd(lcode(v[4]), addr(v[1]), 1)
            m.emit((hlcode(v[2]) << 4) | imml(v[0]))
        })
    }
    // VPERMIL2PD imm4, xmm, m128, xmm, xmm
    if isImm4(v0) && isXMM(v1) && isM128(v2) && isXMM(v3) && isXMM(v4) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[4]), addr(v[2]), hlcode(v[3]))
            m.emit(0x49)
            m.mrsd(lcode(v[4]), addr(v[2]), 1)
            m.emit((hlcode(v[1]) << 4) | imml(v[0]))
        })
    }
    // VPERMIL2PD imm4, ymm, ymm, ymm, ymm
    if isImm4(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) && isYMM(v4) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[4]) << 7) ^ (hcode(v[2]) << 5))
            m.emit(0x7d ^ (hlcode(v[3]) << 3))
            m.emit(0x49)
            m.emit(0xc0 | lcode(v[4]) << 3 | lcode(v[2]))
            m.emit((hlcode(v[1]) << 4) | imml(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[4]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0xfd ^ (hlcode(v[3]) << 3))
            m.emit(0x49)
            m.emit(0xc0 | lcode(v[4]) << 3 | lcode(v[1]))
            m.emit((hlcode(v[2]) << 4) | imml(v[0]))
        })
    }
    // VPERMIL2PD imm4, m256, ymm, ymm, ymm
    if isImm4(v0) && isM256(v1) && isYMM(v2) && isYMM(v3) && isYMM(v4) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x85, hcode(v[4]), addr(v[1]), hlcode(v[3]))
            m.emit(0x49)
            m.mrsd(lcode(v[4]), addr(v[1]), 1)
            m.emit((hlcode(v[2]) << 4) | imml(v[0]))
        })
    }
    // VPERMIL2PD imm4, ymm, m256, ymm, ymm
    if isImm4(v0) && isYMM(v1) && isM256(v2) && isYMM(v3) && isYMM(v4) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[4]), addr(v[2]), hlcode(v[3]))
            m.emit(0x49)
            m.mrsd(lcode(v[4]), addr(v[2]), 1)
            m.emit((hlcode(v[1]) << 4) | imml(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPERMIL2PD")
    }
    return p
}

// VPERMIL2PS performs "Permute Two-Source Single-Precision Floating-Point Vectors".
//
// Mnemonic        : VPERMIL2PS
// ISA extensions  : XOP
// Supported forms : (6 forms)
//
//    * VPERMIL2PS imm4, xmm, xmm, xmm, xmm
//    * VPERMIL2PS imm4, m128, xmm, xmm, xmm
//    * VPERMIL2PS imm4, xmm, m128, xmm, xmm
//    * VPERMIL2PS imm4, ymm, ymm, ymm, ymm
//    * VPERMIL2PS imm4, m256, ymm, ymm, ymm
//    * VPERMIL2PS imm4, ymm, m256, ymm, ymm
//
func (self *Program) VPERMIL2PS(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}, v4 interface{}) *Instruction {
    p := self.alloc(5, Operands{v0, v1, v2, v3, v4})
    // VPERMIL2PS imm4, xmm, xmm, xmm, xmm
    if isImm4(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) && isXMM(v4) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[4]) << 7) ^ (hcode(v[2]) << 5))
            m.emit(0x79 ^ (hlcode(v[3]) << 3))
            m.emit(0x48)
            m.emit(0xc0 | lcode(v[4]) << 3 | lcode(v[2]))
            m.emit((hlcode(v[1]) << 4) | imml(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[4]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0xf9 ^ (hlcode(v[3]) << 3))
            m.emit(0x48)
            m.emit(0xc0 | lcode(v[4]) << 3 | lcode(v[1]))
            m.emit((hlcode(v[2]) << 4) | imml(v[0]))
        })
    }
    // VPERMIL2PS imm4, m128, xmm, xmm, xmm
    if isImm4(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) && isXMM(v4) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x81, hcode(v[4]), addr(v[1]), hlcode(v[3]))
            m.emit(0x48)
            m.mrsd(lcode(v[4]), addr(v[1]), 1)
            m.emit((hlcode(v[2]) << 4) | imml(v[0]))
        })
    }
    // VPERMIL2PS imm4, xmm, m128, xmm, xmm
    if isImm4(v0) && isXMM(v1) && isM128(v2) && isXMM(v3) && isXMM(v4) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[4]), addr(v[2]), hlcode(v[3]))
            m.emit(0x48)
            m.mrsd(lcode(v[4]), addr(v[2]), 1)
            m.emit((hlcode(v[1]) << 4) | imml(v[0]))
        })
    }
    // VPERMIL2PS imm4, ymm, ymm, ymm, ymm
    if isImm4(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) && isYMM(v4) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[4]) << 7) ^ (hcode(v[2]) << 5))
            m.emit(0x7d ^ (hlcode(v[3]) << 3))
            m.emit(0x48)
            m.emit(0xc0 | lcode(v[4]) << 3 | lcode(v[2]))
            m.emit((hlcode(v[1]) << 4) | imml(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[4]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0xfd ^ (hlcode(v[3]) << 3))
            m.emit(0x48)
            m.emit(0xc0 | lcode(v[4]) << 3 | lcode(v[1]))
            m.emit((hlcode(v[2]) << 4) | imml(v[0]))
        })
    }
    // VPERMIL2PS imm4, m256, ymm, ymm, ymm
    if isImm4(v0) && isM256(v1) && isYMM(v2) && isYMM(v3) && isYMM(v4) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x85, hcode(v[4]), addr(v[1]), hlcode(v[3]))
            m.emit(0x48)
            m.mrsd(lcode(v[4]), addr(v[1]), 1)
            m.emit((hlcode(v[2]) << 4) | imml(v[0]))
        })
    }
    // VPERMIL2PS imm4, ymm, m256, ymm, ymm
    if isImm4(v0) && isYMM(v1) && isM256(v2) && isYMM(v3) && isYMM(v4) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[4]), addr(v[2]), hlcode(v[3]))
            m.emit(0x48)
            m.mrsd(lcode(v[4]), addr(v[2]), 1)
            m.emit((hlcode(v[1]) << 4) | imml(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPERMIL2PS")
    }
    return p
}

// VPERMILPD performs "Permute Double-Precision Floating-Point Values".
//
// Mnemonic        : VPERMILPD
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (20 forms)
//
//    * VPERMILPD imm8, m128/m64bcst, xmm{k}{z}
//    * VPERMILPD imm8, m256/m64bcst, ymm{k}{z}
//    * VPERMILPD imm8, m512/m64bcst, zmm{k}{z}
//    * VPERMILPD m128/m64bcst, xmm, xmm{k}{z}
//    * VPERMILPD imm8, xmm, xmm{k}{z}
//    * VPERMILPD xmm, xmm, xmm{k}{z}
//    * VPERMILPD m256/m64bcst, ymm, ymm{k}{z}
//    * VPERMILPD imm8, ymm, ymm{k}{z}
//    * VPERMILPD ymm, ymm, ymm{k}{z}
//    * VPERMILPD m512/m64bcst, zmm, zmm{k}{z}
//    * VPERMILPD imm8, zmm, zmm{k}{z}
//    * VPERMILPD zmm, zmm, zmm{k}{z}
//    * VPERMILPD imm8, xmm, xmm
//    * VPERMILPD xmm, xmm, xmm
//    * VPERMILPD m128, xmm, xmm
//    * VPERMILPD imm8, m128, xmm
//    * VPERMILPD imm8, ymm, ymm
//    * VPERMILPD ymm, ymm, ymm
//    * VPERMILPD m256, ymm, ymm
//    * VPERMILPD imm8, m256, ymm
//
func (self *Program) VPERMILPD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPERMILPD imm8, m128/m64bcst, xmm{k}{z}
    if isImm8(v0) && isM128M64bcst(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b00, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x05)
            m.mrsd(lcode(v[2]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPERMILPD imm8, m256/m64bcst, ymm{k}{z}
    if isImm8(v0) && isM256M64bcst(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b01, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x05)
            m.mrsd(lcode(v[2]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPERMILPD imm8, m512/m64bcst, zmm{k}{z}
    if isImm8(v0) && isM512M64bcst(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b10, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x05)
            m.mrsd(lcode(v[2]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPERMILPD m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x0d)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPERMILPD imm8, xmm, xmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x08)
            m.emit(0x05)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPERMILPD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x0d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMILPD m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x0d)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPERMILPD imm8, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x28)
            m.emit(0x05)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPERMILPD ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x0d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMILPD m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x0d)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPERMILPD imm8, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x48)
            m.emit(0x05)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPERMILPD zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x0d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMILPD imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79)
            m.emit(0x05)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPERMILPD xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x0d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMILPD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x0d)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPERMILPD imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[2]), addr(v[1]), 0)
            m.emit(0x05)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPERMILPD imm8, ymm, ymm
    if isImm8(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7d)
            m.emit(0x05)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPERMILPD ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x0d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMILPD m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x0d)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPERMILPD imm8, m256, ymm
    if isImm8(v0) && isM256(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[2]), addr(v[1]), 0)
            m.emit(0x05)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPERMILPD")
    }
    return p
}

// VPERMILPS performs "Permute Single-Precision Floating-Point Values".
//
// Mnemonic        : VPERMILPS
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (20 forms)
//
//    * VPERMILPS imm8, m128/m32bcst, xmm{k}{z}
//    * VPERMILPS imm8, m256/m32bcst, ymm{k}{z}
//    * VPERMILPS imm8, m512/m32bcst, zmm{k}{z}
//    * VPERMILPS m128/m32bcst, xmm, xmm{k}{z}
//    * VPERMILPS imm8, xmm, xmm{k}{z}
//    * VPERMILPS xmm, xmm, xmm{k}{z}
//    * VPERMILPS m256/m32bcst, ymm, ymm{k}{z}
//    * VPERMILPS imm8, ymm, ymm{k}{z}
//    * VPERMILPS ymm, ymm, ymm{k}{z}
//    * VPERMILPS m512/m32bcst, zmm, zmm{k}{z}
//    * VPERMILPS imm8, zmm, zmm{k}{z}
//    * VPERMILPS zmm, zmm, zmm{k}{z}
//    * VPERMILPS imm8, xmm, xmm
//    * VPERMILPS xmm, xmm, xmm
//    * VPERMILPS m128, xmm, xmm
//    * VPERMILPS imm8, m128, xmm
//    * VPERMILPS imm8, ymm, ymm
//    * VPERMILPS ymm, ymm, ymm
//    * VPERMILPS m256, ymm, ymm
//    * VPERMILPS imm8, m256, ymm
//
func (self *Program) VPERMILPS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPERMILPS imm8, m128/m32bcst, xmm{k}{z}
    if isImm8(v0) && isM128M32bcst(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b00, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x04)
            m.mrsd(lcode(v[2]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPERMILPS imm8, m256/m32bcst, ymm{k}{z}
    if isImm8(v0) && isM256M32bcst(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b01, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x04)
            m.mrsd(lcode(v[2]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPERMILPS imm8, m512/m32bcst, zmm{k}{z}
    if isImm8(v0) && isM512M32bcst(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b10, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x04)
            m.mrsd(lcode(v[2]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPERMILPS m128/m32bcst, xmm, xmm{k}{z}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x0c)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPERMILPS imm8, xmm, xmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x08)
            m.emit(0x04)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPERMILPS xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x0c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMILPS m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x0c)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPERMILPS imm8, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x28)
            m.emit(0x04)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPERMILPS ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x0c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMILPS m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x0c)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPERMILPS imm8, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x48)
            m.emit(0x04)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPERMILPS zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x0c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMILPS imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79)
            m.emit(0x04)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPERMILPS xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x0c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMILPS m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x0c)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPERMILPS imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[2]), addr(v[1]), 0)
            m.emit(0x04)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPERMILPS imm8, ymm, ymm
    if isImm8(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7d)
            m.emit(0x04)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPERMILPS ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x0c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMILPS m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x0c)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPERMILPS imm8, m256, ymm
    if isImm8(v0) && isM256(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[2]), addr(v[1]), 0)
            m.emit(0x04)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPERMILPS")
    }
    return p
}

// VPERMPD performs "Permute Double-Precision Floating-Point Elements".
//
// Mnemonic        : VPERMPD
// ISA extensions  : AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPERMPD imm8, m256/m64bcst, ymm{k}{z}
//    * VPERMPD imm8, m512/m64bcst, zmm{k}{z}
//    * VPERMPD m256/m64bcst, ymm, ymm{k}{z}
//    * VPERMPD imm8, ymm, ymm{k}{z}
//    * VPERMPD ymm, ymm, ymm{k}{z}
//    * VPERMPD m512/m64bcst, zmm, zmm{k}{z}
//    * VPERMPD imm8, zmm, zmm{k}{z}
//    * VPERMPD zmm, zmm, zmm{k}{z}
//    * VPERMPD imm8, ymm, ymm
//    * VPERMPD imm8, m256, ymm
//
func (self *Program) VPERMPD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPERMPD imm8, m256/m64bcst, ymm{k}{z}
    if isImm8(v0) && isM256M64bcst(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b01, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x01)
            m.mrsd(lcode(v[2]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPERMPD imm8, m512/m64bcst, zmm{k}{z}
    if isImm8(v0) && isM512M64bcst(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b10, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x01)
            m.mrsd(lcode(v[2]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPERMPD m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x16)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPERMPD imm8, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x28)
            m.emit(0x01)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPERMPD ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x16)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMPD m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x16)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPERMPD imm8, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x48)
            m.emit(0x01)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPERMPD zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x16)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMPD imm8, ymm, ymm
    if isImm8(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0xfd)
            m.emit(0x01)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPERMPD imm8, m256, ymm
    if isImm8(v0) && isM256(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x85, hcode(v[2]), addr(v[1]), 0)
            m.emit(0x01)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPERMPD")
    }
    return p
}

// VPERMPS performs "Permute Single-Precision Floating-Point Elements".
//
// Mnemonic        : VPERMPS
// ISA extensions  : AVX2, AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPERMPS m256/m32bcst, ymm, ymm{k}{z}
//    * VPERMPS ymm, ymm, ymm{k}{z}
//    * VPERMPS m512/m32bcst, zmm, zmm{k}{z}
//    * VPERMPS zmm, zmm, zmm{k}{z}
//    * VPERMPS ymm, ymm, ymm
//    * VPERMPS m256, ymm, ymm
//
func (self *Program) VPERMPS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPERMPS m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x16)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPERMPS ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x16)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMPS m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x16)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPERMPS zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x16)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMPS ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x16)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMPS m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x16)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPERMPS")
    }
    return p
}

// VPERMQ performs "Permute Quadword Integers".
//
// Mnemonic        : VPERMQ
// ISA extensions  : AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPERMQ imm8, m256/m64bcst, ymm{k}{z}
//    * VPERMQ imm8, m512/m64bcst, zmm{k}{z}
//    * VPERMQ m256/m64bcst, ymm, ymm{k}{z}
//    * VPERMQ imm8, ymm, ymm{k}{z}
//    * VPERMQ ymm, ymm, ymm{k}{z}
//    * VPERMQ m512/m64bcst, zmm, zmm{k}{z}
//    * VPERMQ imm8, zmm, zmm{k}{z}
//    * VPERMQ zmm, zmm, zmm{k}{z}
//    * VPERMQ imm8, ymm, ymm
//    * VPERMQ imm8, m256, ymm
//
func (self *Program) VPERMQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPERMQ imm8, m256/m64bcst, ymm{k}{z}
    if isImm8(v0) && isM256M64bcst(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b01, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x00)
            m.mrsd(lcode(v[2]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPERMQ imm8, m512/m64bcst, zmm{k}{z}
    if isImm8(v0) && isM512M64bcst(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b10, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x00)
            m.mrsd(lcode(v[2]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPERMQ m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x36)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPERMQ imm8, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x28)
            m.emit(0x00)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPERMQ ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x36)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMQ m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x36)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPERMQ imm8, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x48)
            m.emit(0x00)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPERMQ zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x36)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMQ imm8, ymm, ymm
    if isImm8(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0xfd)
            m.emit(0x00)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPERMQ imm8, m256, ymm
    if isImm8(v0) && isM256(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x85, hcode(v[2]), addr(v[1]), 0)
            m.emit(0x00)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPERMQ")
    }
    return p
}

// VPERMT2B performs "Full Permute of Bytes From Two Tables Overwriting a Table".
//
// Mnemonic        : VPERMT2B
// ISA extensions  : AVX512VBMI, AVX512VL
// Supported forms : (6 forms)
//
//    * VPERMT2B xmm, xmm, xmm{k}{z}
//    * VPERMT2B m128, xmm, xmm{k}{z}
//    * VPERMT2B ymm, ymm, ymm{k}{z}
//    * VPERMT2B m256, ymm, ymm{k}{z}
//    * VPERMT2B zmm, zmm, zmm{k}{z}
//    * VPERMT2B m512, zmm, zmm{k}{z}
//
func (self *Program) VPERMT2B(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPERMT2B xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512VBMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x7d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMT2B m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512VBMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x7d)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPERMT2B ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512VBMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x7d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMT2B m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512VBMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x7d)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPERMT2B zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512VBMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x7d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMT2B m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512VBMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x7d)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPERMT2B")
    }
    return p
}

// VPERMT2D performs "Full Permute of Doublewords From Two Tables Overwriting a Table".
//
// Mnemonic        : VPERMT2D
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPERMT2D m128/m32bcst, xmm, xmm{k}{z}
//    * VPERMT2D xmm, xmm, xmm{k}{z}
//    * VPERMT2D m256/m32bcst, ymm, ymm{k}{z}
//    * VPERMT2D ymm, ymm, ymm{k}{z}
//    * VPERMT2D m512/m32bcst, zmm, zmm{k}{z}
//    * VPERMT2D zmm, zmm, zmm{k}{z}
//
func (self *Program) VPERMT2D(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPERMT2D m128/m32bcst, xmm, xmm{k}{z}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x7e)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPERMT2D xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x7e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMT2D m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x7e)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPERMT2D ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x7e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMT2D m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x7e)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPERMT2D zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x7e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPERMT2D")
    }
    return p
}

// VPERMT2PD performs "Full Permute of Double-Precision Floating-Point Values From Two Tables Overwriting a Table".
//
// Mnemonic        : VPERMT2PD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPERMT2PD m128/m64bcst, xmm, xmm{k}{z}
//    * VPERMT2PD xmm, xmm, xmm{k}{z}
//    * VPERMT2PD m256/m64bcst, ymm, ymm{k}{z}
//    * VPERMT2PD ymm, ymm, ymm{k}{z}
//    * VPERMT2PD m512/m64bcst, zmm, zmm{k}{z}
//    * VPERMT2PD zmm, zmm, zmm{k}{z}
//
func (self *Program) VPERMT2PD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPERMT2PD m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x7f)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPERMT2PD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x7f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMT2PD m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x7f)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPERMT2PD ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x7f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMT2PD m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x7f)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPERMT2PD zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x7f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPERMT2PD")
    }
    return p
}

// VPERMT2PS performs "Full Permute of Single-Precision Floating-Point Values From Two Tables Overwriting a Table".
//
// Mnemonic        : VPERMT2PS
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPERMT2PS m128/m32bcst, xmm, xmm{k}{z}
//    * VPERMT2PS xmm, xmm, xmm{k}{z}
//    * VPERMT2PS m256/m32bcst, ymm, ymm{k}{z}
//    * VPERMT2PS ymm, ymm, ymm{k}{z}
//    * VPERMT2PS m512/m32bcst, zmm, zmm{k}{z}
//    * VPERMT2PS zmm, zmm, zmm{k}{z}
//
func (self *Program) VPERMT2PS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPERMT2PS m128/m32bcst, xmm, xmm{k}{z}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x7f)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPERMT2PS xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x7f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMT2PS m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x7f)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPERMT2PS ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x7f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMT2PS m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x7f)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPERMT2PS zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x7f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPERMT2PS")
    }
    return p
}

// VPERMT2Q performs "Full Permute of Quadwords From Two Tables Overwriting a Table".
//
// Mnemonic        : VPERMT2Q
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPERMT2Q m128/m64bcst, xmm, xmm{k}{z}
//    * VPERMT2Q xmm, xmm, xmm{k}{z}
//    * VPERMT2Q m256/m64bcst, ymm, ymm{k}{z}
//    * VPERMT2Q ymm, ymm, ymm{k}{z}
//    * VPERMT2Q m512/m64bcst, zmm, zmm{k}{z}
//    * VPERMT2Q zmm, zmm, zmm{k}{z}
//
func (self *Program) VPERMT2Q(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPERMT2Q m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x7e)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPERMT2Q xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x7e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMT2Q m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x7e)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPERMT2Q ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x7e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMT2Q m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x7e)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPERMT2Q zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x7e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPERMT2Q")
    }
    return p
}

// VPERMT2W performs "Full Permute of Words From Two Tables Overwriting a Table".
//
// Mnemonic        : VPERMT2W
// ISA extensions  : AVX512BW, AVX512VL
// Supported forms : (6 forms)
//
//    * VPERMT2W xmm, xmm, xmm{k}{z}
//    * VPERMT2W m128, xmm, xmm{k}{z}
//    * VPERMT2W ymm, ymm, ymm{k}{z}
//    * VPERMT2W m256, ymm, ymm{k}{z}
//    * VPERMT2W zmm, zmm, zmm{k}{z}
//    * VPERMT2W m512, zmm, zmm{k}{z}
//
func (self *Program) VPERMT2W(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPERMT2W xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x7d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMT2W m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x7d)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPERMT2W ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x7d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMT2W m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x7d)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPERMT2W zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x7d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMT2W m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x7d)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPERMT2W")
    }
    return p
}

// VPERMW performs "Permute Word Integers".
//
// Mnemonic        : VPERMW
// ISA extensions  : AVX512BW, AVX512VL
// Supported forms : (6 forms)
//
//    * VPERMW xmm, xmm, xmm{k}{z}
//    * VPERMW m128, xmm, xmm{k}{z}
//    * VPERMW ymm, ymm, ymm{k}{z}
//    * VPERMW m256, ymm, ymm{k}{z}
//    * VPERMW zmm, zmm, zmm{k}{z}
//    * VPERMW m512, zmm, zmm{k}{z}
//
func (self *Program) VPERMW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPERMW xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x8d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMW m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x8d)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPERMW ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x8d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMW m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x8d)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPERMW zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x8d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPERMW m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x8d)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPERMW")
    }
    return p
}

// VPEXPANDD performs "Load Sparse Packed Doubleword Integer Values from Dense Memory/Register".
//
// Mnemonic        : VPEXPANDD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPEXPANDD xmm, xmm{k}{z}
//    * VPEXPANDD ymm, ymm{k}{z}
//    * VPEXPANDD zmm, zmm{k}{z}
//    * VPEXPANDD m128, xmm{k}{z}
//    * VPEXPANDD m256, ymm{k}{z}
//    * VPEXPANDD m512, zmm{k}{z}
//
func (self *Program) VPEXPANDD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPEXPANDD xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x89)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPEXPANDD ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x89)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPEXPANDD zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x89)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPEXPANDD m128, xmm{k}{z}
    if isM128(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x89)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VPEXPANDD m256, ymm{k}{z}
    if isM256(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x89)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VPEXPANDD m512, zmm{k}{z}
    if isM512(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x89)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPEXPANDD")
    }
    return p
}

// VPEXPANDQ performs "Load Sparse Packed Quadword Integer Values from Dense Memory/Register".
//
// Mnemonic        : VPEXPANDQ
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPEXPANDQ xmm, xmm{k}{z}
//    * VPEXPANDQ ymm, ymm{k}{z}
//    * VPEXPANDQ zmm, zmm{k}{z}
//    * VPEXPANDQ m128, xmm{k}{z}
//    * VPEXPANDQ m256, ymm{k}{z}
//    * VPEXPANDQ m512, zmm{k}{z}
//
func (self *Program) VPEXPANDQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPEXPANDQ xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x89)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPEXPANDQ ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x89)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPEXPANDQ zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x89)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPEXPANDQ m128, xmm{k}{z}
    if isM128(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x89)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VPEXPANDQ m256, ymm{k}{z}
    if isM256(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x89)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VPEXPANDQ m512, zmm{k}{z}
    if isM512(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x89)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPEXPANDQ")
    }
    return p
}

// VPEXTRB performs "Extract Byte".
//
// Mnemonic        : VPEXTRB
// ISA extensions  : AVX, AVX512BW
// Supported forms : (4 forms)
//
//    * VPEXTRB imm8, xmm, r32
//    * VPEXTRB imm8, xmm, r32
//    * VPEXTRB imm8, xmm, m8
//    * VPEXTRB imm8, xmm, m8
//
func (self *Program) VPEXTRB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPEXTRB imm8, xmm, r32
    if isImm8(v0) && isXMM(v1) && isReg32(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[1]) << 7) ^ (hcode(v[2]) << 5))
            m.emit(0x79)
            m.emit(0x14)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPEXTRB imm8, xmm, r32
    if isImm8(v0) && isEVEXXMM(v1) && isReg32(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[1]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit(0x08)
            m.emit(0x14)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPEXTRB imm8, xmm, m8
    if isImm8(v0) && isXMM(v1) && isM8(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[1]), addr(v[2]), 0)
            m.emit(0x14)
            m.mrsd(lcode(v[1]), addr(v[2]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPEXTRB imm8, xmm, m8
    if isImm8(v0) && isEVEXXMM(v1) && isM8(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b00, ehcode(v[1]), addr(v[2]), 0, 0, 0, 0)
            m.emit(0x14)
            m.mrsd(lcode(v[1]), addr(v[2]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPEXTRB")
    }
    return p
}

// VPEXTRD performs "Extract Doubleword".
//
// Mnemonic        : VPEXTRD
// ISA extensions  : AVX, AVX512DQ
// Supported forms : (4 forms)
//
//    * VPEXTRD imm8, xmm, r32
//    * VPEXTRD imm8, xmm, r32
//    * VPEXTRD imm8, xmm, m32
//    * VPEXTRD imm8, xmm, m32
//
func (self *Program) VPEXTRD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPEXTRD imm8, xmm, r32
    if isImm8(v0) && isXMM(v1) && isReg32(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[1]) << 7) ^ (hcode(v[2]) << 5))
            m.emit(0x79)
            m.emit(0x16)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPEXTRD imm8, xmm, r32
    if isImm8(v0) && isEVEXXMM(v1) && isReg32(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[1]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit(0x08)
            m.emit(0x16)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPEXTRD imm8, xmm, m32
    if isImm8(v0) && isXMM(v1) && isM32(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[1]), addr(v[2]), 0)
            m.emit(0x16)
            m.mrsd(lcode(v[1]), addr(v[2]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPEXTRD imm8, xmm, m32
    if isImm8(v0) && isEVEXXMM(v1) && isM32(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b00, ehcode(v[1]), addr(v[2]), 0, 0, 0, 0)
            m.emit(0x16)
            m.mrsd(lcode(v[1]), addr(v[2]), 4)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPEXTRD")
    }
    return p
}

// VPEXTRQ performs "Extract Quadword".
//
// Mnemonic        : VPEXTRQ
// ISA extensions  : AVX, AVX512DQ
// Supported forms : (4 forms)
//
//    * VPEXTRQ imm8, xmm, r64
//    * VPEXTRQ imm8, xmm, r64
//    * VPEXTRQ imm8, xmm, m64
//    * VPEXTRQ imm8, xmm, m64
//
func (self *Program) VPEXTRQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPEXTRQ imm8, xmm, r64
    if isImm8(v0) && isXMM(v1) && isReg64(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[1]) << 7) ^ (hcode(v[2]) << 5))
            m.emit(0xf9)
            m.emit(0x16)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPEXTRQ imm8, xmm, r64
    if isImm8(v0) && isEVEXXMM(v1) && isReg64(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[1]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit(0x08)
            m.emit(0x16)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPEXTRQ imm8, xmm, m64
    if isImm8(v0) && isXMM(v1) && isM64(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x81, hcode(v[1]), addr(v[2]), 0)
            m.emit(0x16)
            m.mrsd(lcode(v[1]), addr(v[2]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPEXTRQ imm8, xmm, m64
    if isImm8(v0) && isEVEXXMM(v1) && isM64(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b00, ehcode(v[1]), addr(v[2]), 0, 0, 0, 0)
            m.emit(0x16)
            m.mrsd(lcode(v[1]), addr(v[2]), 8)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPEXTRQ")
    }
    return p
}

// VPEXTRW performs "Extract Word".
//
// Mnemonic        : VPEXTRW
// ISA extensions  : AVX, AVX512BW
// Supported forms : (4 forms)
//
//    * VPEXTRW imm8, xmm, r32
//    * VPEXTRW imm8, xmm, r32
//    * VPEXTRW imm8, xmm, m16
//    * VPEXTRW imm8, xmm, m16
//
func (self *Program) VPEXTRW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPEXTRW imm8, xmm, r32
    if isImm8(v0) && isXMM(v1) && isReg32(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[1], 0)
            m.emit(0xc5)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[1]) << 7) ^ (hcode(v[2]) << 5))
            m.emit(0x79)
            m.emit(0x15)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPEXTRW imm8, xmm, r32
    if isImm8(v0) && isEVEXXMM(v1) && isReg32(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[1]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit(0x08)
            m.emit(0x15)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d)
            m.emit(0x08)
            m.emit(0xc5)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPEXTRW imm8, xmm, m16
    if isImm8(v0) && isXMM(v1) && isM16(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[1]), addr(v[2]), 0)
            m.emit(0x15)
            m.mrsd(lcode(v[1]), addr(v[2]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPEXTRW imm8, xmm, m16
    if isImm8(v0) && isEVEXXMM(v1) && isM16(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b00, ehcode(v[1]), addr(v[2]), 0, 0, 0, 0)
            m.emit(0x15)
            m.mrsd(lcode(v[1]), addr(v[2]), 2)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPEXTRW")
    }
    return p
}

// VPGATHERDD performs "Gather Packed Doubleword Values Using Signed Doubleword Indices".
//
// Mnemonic        : VPGATHERDD
// ISA extensions  : AVX2, AVX512F, AVX512VL
// Supported forms : (5 forms)
//
//    * VPGATHERDD vm32x, xmm{k}
//    * VPGATHERDD vm32y, ymm{k}
//    * VPGATHERDD vm32z, zmm{k}
//    * VPGATHERDD xmm, vm32x, xmm
//    * VPGATHERDD ymm, vm32y, ymm
//
func (self *Program) VPGATHERDD(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VPGATHERDD takes 2 or 3 operands")
    }
    // VPGATHERDD vm32x, xmm{k}
    if len(vv) == 0 && isEVEXVMX(v0) && isXMMk(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), 0, 0)
            m.emit(0x90)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VPGATHERDD vm32y, ymm{k}
    if len(vv) == 0 && isEVEXVMY(v0) && isYMMk(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), 0, 0)
            m.emit(0x90)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VPGATHERDD vm32z, zmm{k}
    if len(vv) == 0 && isVMZ(v0) && isZMMk(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), 0, 0)
            m.emit(0x90)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VPGATHERDD xmm, vm32x, xmm
    if len(vv) == 1 && isXMM(v0) && isVMX(v1) && isXMM(vv[0]) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0x90)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    // VPGATHERDD ymm, vm32y, ymm
    if len(vv) == 1 && isYMM(v0) && isVMY(v1) && isYMM(vv[0]) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0x90)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPGATHERDD")
    }
    return p
}

// VPGATHERDQ performs "Gather Packed Quadword Values Using Signed Doubleword Indices".
//
// Mnemonic        : VPGATHERDQ
// ISA extensions  : AVX2, AVX512F, AVX512VL
// Supported forms : (5 forms)
//
//    * VPGATHERDQ vm32x, xmm{k}
//    * VPGATHERDQ vm32x, ymm{k}
//    * VPGATHERDQ vm32y, zmm{k}
//    * VPGATHERDQ xmm, vm32x, xmm
//    * VPGATHERDQ ymm, vm32x, ymm
//
func (self *Program) VPGATHERDQ(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VPGATHERDQ takes 2 or 3 operands")
    }
    // VPGATHERDQ vm32x, xmm{k}
    if len(vv) == 0 && isEVEXVMX(v0) && isXMMk(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), 0, 0)
            m.emit(0x90)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VPGATHERDQ vm32x, ymm{k}
    if len(vv) == 0 && isEVEXVMX(v0) && isYMMk(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), 0, 0)
            m.emit(0x90)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VPGATHERDQ vm32y, zmm{k}
    if len(vv) == 0 && isEVEXVMY(v0) && isZMMk(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), 0, 0)
            m.emit(0x90)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VPGATHERDQ xmm, vm32x, xmm
    if len(vv) == 1 && isXMM(v0) && isVMX(v1) && isXMM(vv[0]) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0x90)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    // VPGATHERDQ ymm, vm32x, ymm
    if len(vv) == 1 && isYMM(v0) && isVMX(v1) && isYMM(vv[0]) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x85, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0x90)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPGATHERDQ")
    }
    return p
}

// VPGATHERQD performs "Gather Packed Doubleword Values Using Signed Quadword Indices".
//
// Mnemonic        : VPGATHERQD
// ISA extensions  : AVX2, AVX512F, AVX512VL
// Supported forms : (5 forms)
//
//    * VPGATHERQD vm64x, xmm{k}
//    * VPGATHERQD vm64y, xmm{k}
//    * VPGATHERQD vm64z, ymm{k}
//    * VPGATHERQD xmm, vm64x, xmm
//    * VPGATHERQD xmm, vm64y, xmm
//
func (self *Program) VPGATHERQD(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VPGATHERQD takes 2 or 3 operands")
    }
    // VPGATHERQD vm64x, xmm{k}
    if len(vv) == 0 && isEVEXVMX(v0) && isXMMk(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), 0, 0)
            m.emit(0x91)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VPGATHERQD vm64y, xmm{k}
    if len(vv) == 0 && isEVEXVMY(v0) && isXMMk(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), 0, 0)
            m.emit(0x91)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VPGATHERQD vm64z, ymm{k}
    if len(vv) == 0 && isVMZ(v0) && isYMMk(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), 0, 0)
            m.emit(0x91)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VPGATHERQD xmm, vm64x, xmm
    if len(vv) == 1 && isXMM(v0) && isVMX(v1) && isXMM(vv[0]) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0x91)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    // VPGATHERQD xmm, vm64y, xmm
    if len(vv) == 1 && isXMM(v0) && isVMY(v1) && isXMM(vv[0]) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0x91)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPGATHERQD")
    }
    return p
}

// VPGATHERQQ performs "Gather Packed Quadword Values Using Signed Quadword Indices".
//
// Mnemonic        : VPGATHERQQ
// ISA extensions  : AVX2, AVX512F, AVX512VL
// Supported forms : (5 forms)
//
//    * VPGATHERQQ vm64x, xmm{k}
//    * VPGATHERQQ vm64y, ymm{k}
//    * VPGATHERQQ vm64z, zmm{k}
//    * VPGATHERQQ xmm, vm64x, xmm
//    * VPGATHERQQ ymm, vm64y, ymm
//
func (self *Program) VPGATHERQQ(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VPGATHERQQ takes 2 or 3 operands")
    }
    // VPGATHERQQ vm64x, xmm{k}
    if len(vv) == 0 && isEVEXVMX(v0) && isXMMk(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), 0, 0)
            m.emit(0x91)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VPGATHERQQ vm64y, ymm{k}
    if len(vv) == 0 && isEVEXVMY(v0) && isYMMk(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), 0, 0)
            m.emit(0x91)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VPGATHERQQ vm64z, zmm{k}
    if len(vv) == 0 && isVMZ(v0) && isZMMk(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), 0, 0)
            m.emit(0x91)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VPGATHERQQ xmm, vm64x, xmm
    if len(vv) == 1 && isXMM(v0) && isVMX(v1) && isXMM(vv[0]) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0x91)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    // VPGATHERQQ ymm, vm64y, ymm
    if len(vv) == 1 && isYMM(v0) && isVMY(v1) && isYMM(vv[0]) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x85, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0x91)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPGATHERQQ")
    }
    return p
}

// VPHADDBD performs "Packed Horizontal Add Signed Byte to Signed Doubleword".
//
// Mnemonic        : VPHADDBD
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPHADDBD xmm, xmm
//    * VPHADDBD m128, xmm
//
func (self *Program) VPHADDBD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPHADDBD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x78)
            m.emit(0xc2)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPHADDBD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, hcode(v[1]), addr(v[0]), 0)
            m.emit(0xc2)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPHADDBD")
    }
    return p
}

// VPHADDBQ performs "Packed Horizontal Add Signed Byte to Signed Quadword".
//
// Mnemonic        : VPHADDBQ
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPHADDBQ xmm, xmm
//    * VPHADDBQ m128, xmm
//
func (self *Program) VPHADDBQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPHADDBQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x78)
            m.emit(0xc3)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPHADDBQ m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, hcode(v[1]), addr(v[0]), 0)
            m.emit(0xc3)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPHADDBQ")
    }
    return p
}

// VPHADDBW performs "Packed Horizontal Add Signed Byte to Signed Word".
//
// Mnemonic        : VPHADDBW
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPHADDBW xmm, xmm
//    * VPHADDBW m128, xmm
//
func (self *Program) VPHADDBW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPHADDBW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x78)
            m.emit(0xc1)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPHADDBW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, hcode(v[1]), addr(v[0]), 0)
            m.emit(0xc1)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPHADDBW")
    }
    return p
}

// VPHADDD performs "Packed Horizontal Add Doubleword Integer".
//
// Mnemonic        : VPHADDD
// ISA extensions  : AVX, AVX2
// Supported forms : (4 forms)
//
//    * VPHADDD xmm, xmm, xmm
//    * VPHADDD m128, xmm, xmm
//    * VPHADDD ymm, ymm, ymm
//    * VPHADDD m256, ymm, ymm
//
func (self *Program) VPHADDD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPHADDD xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x02)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPHADDD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x02)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPHADDD ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x02)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPHADDD m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x02)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPHADDD")
    }
    return p
}

// VPHADDDQ performs "Packed Horizontal Add Signed Doubleword to Signed Quadword".
//
// Mnemonic        : VPHADDDQ
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPHADDDQ xmm, xmm
//    * VPHADDDQ m128, xmm
//
func (self *Program) VPHADDDQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPHADDDQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x78)
            m.emit(0xcb)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPHADDDQ m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, hcode(v[1]), addr(v[0]), 0)
            m.emit(0xcb)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPHADDDQ")
    }
    return p
}

// VPHADDSW performs "Packed Horizontal Add Signed Word Integers with Signed Saturation".
//
// Mnemonic        : VPHADDSW
// ISA extensions  : AVX, AVX2
// Supported forms : (4 forms)
//
//    * VPHADDSW xmm, xmm, xmm
//    * VPHADDSW m128, xmm, xmm
//    * VPHADDSW ymm, ymm, ymm
//    * VPHADDSW m256, ymm, ymm
//
func (self *Program) VPHADDSW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPHADDSW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x03)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPHADDSW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x03)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPHADDSW ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x03)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPHADDSW m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x03)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPHADDSW")
    }
    return p
}

// VPHADDUBD performs "Packed Horizontal Add Unsigned Byte to Doubleword".
//
// Mnemonic        : VPHADDUBD
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPHADDUBD xmm, xmm
//    * VPHADDUBD m128, xmm
//
func (self *Program) VPHADDUBD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPHADDUBD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x78)
            m.emit(0xd2)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPHADDUBD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, hcode(v[1]), addr(v[0]), 0)
            m.emit(0xd2)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPHADDUBD")
    }
    return p
}

// VPHADDUBQ performs "Packed Horizontal Add Unsigned Byte to Quadword".
//
// Mnemonic        : VPHADDUBQ
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPHADDUBQ xmm, xmm
//    * VPHADDUBQ m128, xmm
//
func (self *Program) VPHADDUBQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPHADDUBQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x78)
            m.emit(0xd3)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPHADDUBQ m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, hcode(v[1]), addr(v[0]), 0)
            m.emit(0xd3)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPHADDUBQ")
    }
    return p
}

// VPHADDUBW performs "Packed Horizontal Add Unsigned Byte to Word".
//
// Mnemonic        : VPHADDUBW
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPHADDUBW xmm, xmm
//    * VPHADDUBW m128, xmm
//
func (self *Program) VPHADDUBW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPHADDUBW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x78)
            m.emit(0xd1)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPHADDUBW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, hcode(v[1]), addr(v[0]), 0)
            m.emit(0xd1)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPHADDUBW")
    }
    return p
}

// VPHADDUDQ performs "Packed Horizontal Add Unsigned Doubleword to Quadword".
//
// Mnemonic        : VPHADDUDQ
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPHADDUDQ xmm, xmm
//    * VPHADDUDQ m128, xmm
//
func (self *Program) VPHADDUDQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPHADDUDQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x78)
            m.emit(0xdb)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPHADDUDQ m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, hcode(v[1]), addr(v[0]), 0)
            m.emit(0xdb)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPHADDUDQ")
    }
    return p
}

// VPHADDUWD performs "Packed Horizontal Add Unsigned Word to Doubleword".
//
// Mnemonic        : VPHADDUWD
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPHADDUWD xmm, xmm
//    * VPHADDUWD m128, xmm
//
func (self *Program) VPHADDUWD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPHADDUWD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x78)
            m.emit(0xd6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPHADDUWD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, hcode(v[1]), addr(v[0]), 0)
            m.emit(0xd6)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPHADDUWD")
    }
    return p
}

// VPHADDUWQ performs "Packed Horizontal Add Unsigned Word to Quadword".
//
// Mnemonic        : VPHADDUWQ
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPHADDUWQ xmm, xmm
//    * VPHADDUWQ m128, xmm
//
func (self *Program) VPHADDUWQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPHADDUWQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x78)
            m.emit(0xd7)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPHADDUWQ m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, hcode(v[1]), addr(v[0]), 0)
            m.emit(0xd7)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPHADDUWQ")
    }
    return p
}

// VPHADDW performs "Packed Horizontal Add Word Integers".
//
// Mnemonic        : VPHADDW
// ISA extensions  : AVX, AVX2
// Supported forms : (4 forms)
//
//    * VPHADDW xmm, xmm, xmm
//    * VPHADDW m128, xmm, xmm
//    * VPHADDW ymm, ymm, ymm
//    * VPHADDW m256, ymm, ymm
//
func (self *Program) VPHADDW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPHADDW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x01)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPHADDW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x01)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPHADDW ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x01)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPHADDW m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x01)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPHADDW")
    }
    return p
}

// VPHADDWD performs "Packed Horizontal Add Signed Word to Signed Doubleword".
//
// Mnemonic        : VPHADDWD
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPHADDWD xmm, xmm
//    * VPHADDWD m128, xmm
//
func (self *Program) VPHADDWD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPHADDWD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x78)
            m.emit(0xc6)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPHADDWD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, hcode(v[1]), addr(v[0]), 0)
            m.emit(0xc6)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPHADDWD")
    }
    return p
}

// VPHADDWQ performs "Packed Horizontal Add Signed Word to Signed Quadword".
//
// Mnemonic        : VPHADDWQ
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPHADDWQ xmm, xmm
//    * VPHADDWQ m128, xmm
//
func (self *Program) VPHADDWQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPHADDWQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x78)
            m.emit(0xc7)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPHADDWQ m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, hcode(v[1]), addr(v[0]), 0)
            m.emit(0xc7)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPHADDWQ")
    }
    return p
}

// VPHMINPOSUW performs "Packed Horizontal Minimum of Unsigned Word Integers".
//
// Mnemonic        : VPHMINPOSUW
// ISA extensions  : AVX
// Supported forms : (2 forms)
//
//    * VPHMINPOSUW xmm, xmm
//    * VPHMINPOSUW m128, xmm
//
func (self *Program) VPHMINPOSUW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPHMINPOSUW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79)
            m.emit(0x41)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPHMINPOSUW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x41)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPHMINPOSUW")
    }
    return p
}

// VPHSUBBW performs "Packed Horizontal Subtract Signed Byte to Signed Word".
//
// Mnemonic        : VPHSUBBW
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPHSUBBW xmm, xmm
//    * VPHSUBBW m128, xmm
//
func (self *Program) VPHSUBBW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPHSUBBW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x78)
            m.emit(0xe1)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPHSUBBW m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, hcode(v[1]), addr(v[0]), 0)
            m.emit(0xe1)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPHSUBBW")
    }
    return p
}

// VPHSUBD performs "Packed Horizontal Subtract Doubleword Integers".
//
// Mnemonic        : VPHSUBD
// ISA extensions  : AVX, AVX2
// Supported forms : (4 forms)
//
//    * VPHSUBD xmm, xmm, xmm
//    * VPHSUBD m128, xmm, xmm
//    * VPHSUBD ymm, ymm, ymm
//    * VPHSUBD m256, ymm, ymm
//
func (self *Program) VPHSUBD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPHSUBD xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x06)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPHSUBD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x06)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPHSUBD ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x06)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPHSUBD m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x06)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPHSUBD")
    }
    return p
}

// VPHSUBDQ performs "Packed Horizontal Subtract Signed Doubleword to Signed Quadword".
//
// Mnemonic        : VPHSUBDQ
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPHSUBDQ xmm, xmm
//    * VPHSUBDQ m128, xmm
//
func (self *Program) VPHSUBDQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPHSUBDQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x78)
            m.emit(0xe3)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPHSUBDQ m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, hcode(v[1]), addr(v[0]), 0)
            m.emit(0xe3)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPHSUBDQ")
    }
    return p
}

// VPHSUBSW performs "Packed Horizontal Subtract Signed Word Integers with Signed Saturation".
//
// Mnemonic        : VPHSUBSW
// ISA extensions  : AVX, AVX2
// Supported forms : (4 forms)
//
//    * VPHSUBSW xmm, xmm, xmm
//    * VPHSUBSW m128, xmm, xmm
//    * VPHSUBSW ymm, ymm, ymm
//    * VPHSUBSW m256, ymm, ymm
//
func (self *Program) VPHSUBSW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPHSUBSW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x07)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPHSUBSW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x07)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPHSUBSW ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x07)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPHSUBSW m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x07)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPHSUBSW")
    }
    return p
}

// VPHSUBW performs "Packed Horizontal Subtract Word Integers".
//
// Mnemonic        : VPHSUBW
// ISA extensions  : AVX, AVX2
// Supported forms : (4 forms)
//
//    * VPHSUBW xmm, xmm, xmm
//    * VPHSUBW m128, xmm, xmm
//    * VPHSUBW ymm, ymm, ymm
//    * VPHSUBW m256, ymm, ymm
//
func (self *Program) VPHSUBW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPHSUBW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x05)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPHSUBW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x05)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPHSUBW ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x05)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPHSUBW m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x05)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPHSUBW")
    }
    return p
}

// VPHSUBWD performs "Packed Horizontal Subtract Signed Word to Signed Doubleword".
//
// Mnemonic        : VPHSUBWD
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPHSUBWD xmm, xmm
//    * VPHSUBWD m128, xmm
//
func (self *Program) VPHSUBWD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPHSUBWD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x78)
            m.emit(0xe2)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPHSUBWD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, hcode(v[1]), addr(v[0]), 0)
            m.emit(0xe2)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPHSUBWD")
    }
    return p
}

// VPINSRB performs "Insert Byte".
//
// Mnemonic        : VPINSRB
// ISA extensions  : AVX, AVX512BW
// Supported forms : (4 forms)
//
//    * VPINSRB imm8, r32, xmm, xmm
//    * VPINSRB imm8, r32, xmm, xmm
//    * VPINSRB imm8, m8, xmm, xmm
//    * VPINSRB imm8, m8, xmm, xmm
//
func (self *Program) VPINSRB(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPINSRB imm8, r32, xmm, xmm
    if isImm8(v0) && isReg32(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x20)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPINSRB imm8, r32, xmm, xmm
    if isImm8(v0) && isReg32(v1) && isEVEXXMM(v2) && isEVEXXMM(v3) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | 0x00)
            m.emit(0x20)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPINSRB imm8, m8, xmm, xmm
    if isImm8(v0) && isM8(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x20)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPINSRB imm8, m8, xmm, xmm
    if isImm8(v0) && isM8(v1) && isEVEXXMM(v2) && isEVEXXMM(v3) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), 0, 0, 0)
            m.emit(0x20)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPINSRB")
    }
    return p
}

// VPINSRD performs "Insert Doubleword".
//
// Mnemonic        : VPINSRD
// ISA extensions  : AVX, AVX512DQ
// Supported forms : (4 forms)
//
//    * VPINSRD imm8, r32, xmm, xmm
//    * VPINSRD imm8, r32, xmm, xmm
//    * VPINSRD imm8, m32, xmm, xmm
//    * VPINSRD imm8, m32, xmm, xmm
//
func (self *Program) VPINSRD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPINSRD imm8, r32, xmm, xmm
    if isImm8(v0) && isReg32(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x22)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPINSRD imm8, r32, xmm, xmm
    if isImm8(v0) && isReg32(v1) && isEVEXXMM(v2) && isEVEXXMM(v3) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | 0x00)
            m.emit(0x22)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPINSRD imm8, m32, xmm, xmm
    if isImm8(v0) && isM32(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x22)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPINSRD imm8, m32, xmm, xmm
    if isImm8(v0) && isM32(v1) && isEVEXXMM(v2) && isEVEXXMM(v3) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), 0, 0, 0)
            m.emit(0x22)
            m.mrsd(lcode(v[3]), addr(v[1]), 4)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPINSRD")
    }
    return p
}

// VPINSRQ performs "Insert Quadword".
//
// Mnemonic        : VPINSRQ
// ISA extensions  : AVX, AVX512DQ
// Supported forms : (4 forms)
//
//    * VPINSRQ imm8, r64, xmm, xmm
//    * VPINSRQ imm8, r64, xmm, xmm
//    * VPINSRQ imm8, m64, xmm, xmm
//    * VPINSRQ imm8, m64, xmm, xmm
//
func (self *Program) VPINSRQ(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPINSRQ imm8, r64, xmm, xmm
    if isImm8(v0) && isReg64(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0xf9 ^ (hlcode(v[2]) << 3))
            m.emit(0x22)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPINSRQ imm8, r64, xmm, xmm
    if isImm8(v0) && isReg64(v1) && isEVEXXMM(v2) && isEVEXXMM(v3) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | 0x00)
            m.emit(0x22)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPINSRQ imm8, m64, xmm, xmm
    if isImm8(v0) && isM64(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x81, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x22)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPINSRQ imm8, m64, xmm, xmm
    if isImm8(v0) && isM64(v1) && isEVEXXMM(v2) && isEVEXXMM(v3) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), 0, 0, 0)
            m.emit(0x22)
            m.mrsd(lcode(v[3]), addr(v[1]), 8)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPINSRQ")
    }
    return p
}

// VPINSRW performs "Insert Word".
//
// Mnemonic        : VPINSRW
// ISA extensions  : AVX, AVX512BW
// Supported forms : (4 forms)
//
//    * VPINSRW imm8, r32, xmm, xmm
//    * VPINSRW imm8, r32, xmm, xmm
//    * VPINSRW imm8, m16, xmm, xmm
//    * VPINSRW imm8, m16, xmm, xmm
//
func (self *Program) VPINSRW(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPINSRW imm8, r32, xmm, xmm
    if isImm8(v0) && isReg32(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[3]), v[1], hlcode(v[2]))
            m.emit(0xc4)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPINSRW imm8, r32, xmm, xmm
    if isImm8(v0) && isReg32(v1) && isEVEXXMM(v2) && isEVEXXMM(v3) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | 0x00)
            m.emit(0xc4)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPINSRW imm8, m16, xmm, xmm
    if isImm8(v0) && isM16(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0xc4)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPINSRW imm8, m16, xmm, xmm
    if isImm8(v0) && isM16(v1) && isEVEXXMM(v2) && isEVEXXMM(v3) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), 0, 0, 0)
            m.emit(0xc4)
            m.mrsd(lcode(v[3]), addr(v[1]), 2)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPINSRW")
    }
    return p
}

// VPLZCNTD performs "Count the Number of Leading Zero Bits for Packed Doubleword Values".
//
// Mnemonic        : VPLZCNTD
// ISA extensions  : AVX512CD, AVX512VL
// Supported forms : (6 forms)
//
//    * VPLZCNTD m128/m32bcst, xmm{k}{z}
//    * VPLZCNTD m256/m32bcst, ymm{k}{z}
//    * VPLZCNTD m512/m32bcst, zmm{k}{z}
//    * VPLZCNTD xmm, xmm{k}{z}
//    * VPLZCNTD ymm, ymm{k}{z}
//    * VPLZCNTD zmm, zmm{k}{z}
//
func (self *Program) VPLZCNTD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPLZCNTD m128/m32bcst, xmm{k}{z}
    if isM128M32bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512CD)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x44)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VPLZCNTD m256/m32bcst, ymm{k}{z}
    if isM256M32bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512CD)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x44)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VPLZCNTD m512/m32bcst, zmm{k}{z}
    if isM512M32bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512CD)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x44)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VPLZCNTD xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512CD)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x44)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPLZCNTD ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512CD)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x44)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPLZCNTD zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512CD)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x44)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPLZCNTD")
    }
    return p
}

// VPLZCNTQ performs "Count the Number of Leading Zero Bits for Packed Quadword Values".
//
// Mnemonic        : VPLZCNTQ
// ISA extensions  : AVX512CD, AVX512VL
// Supported forms : (6 forms)
//
//    * VPLZCNTQ m128/m64bcst, xmm{k}{z}
//    * VPLZCNTQ m256/m64bcst, ymm{k}{z}
//    * VPLZCNTQ m512/m64bcst, zmm{k}{z}
//    * VPLZCNTQ xmm, xmm{k}{z}
//    * VPLZCNTQ ymm, ymm{k}{z}
//    * VPLZCNTQ zmm, zmm{k}{z}
//
func (self *Program) VPLZCNTQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPLZCNTQ m128/m64bcst, xmm{k}{z}
    if isM128M64bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512CD)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x44)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VPLZCNTQ m256/m64bcst, ymm{k}{z}
    if isM256M64bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512CD)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x44)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VPLZCNTQ m512/m64bcst, zmm{k}{z}
    if isM512M64bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512CD)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x44)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VPLZCNTQ xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512CD)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x44)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPLZCNTQ ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512CD)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x44)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPLZCNTQ zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512CD)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x44)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPLZCNTQ")
    }
    return p
}

// VPMACSDD performs "Packed Multiply Accumulate Signed Doubleword to Signed Doubleword".
//
// Mnemonic        : VPMACSDD
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPMACSDD xmm, xmm, xmm, xmm
//    * VPMACSDD xmm, m128, xmm, xmm
//
func (self *Program) VPMACSDD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPMACSDD xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe8 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[2]) << 3))
            m.emit(0x9e)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VPMACSDD xmm, m128, xmm, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1000, 0x00, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x9e)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMACSDD")
    }
    return p
}

// VPMACSDQH performs "Packed Multiply Accumulate Signed High Doubleword to Signed Quadword".
//
// Mnemonic        : VPMACSDQH
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPMACSDQH xmm, xmm, xmm, xmm
//    * VPMACSDQH xmm, m128, xmm, xmm
//
func (self *Program) VPMACSDQH(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPMACSDQH xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe8 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[2]) << 3))
            m.emit(0x9f)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VPMACSDQH xmm, m128, xmm, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1000, 0x00, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x9f)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMACSDQH")
    }
    return p
}

// VPMACSDQL performs "Packed Multiply Accumulate Signed Low Doubleword to Signed Quadword".
//
// Mnemonic        : VPMACSDQL
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPMACSDQL xmm, xmm, xmm, xmm
//    * VPMACSDQL xmm, m128, xmm, xmm
//
func (self *Program) VPMACSDQL(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPMACSDQL xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe8 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[2]) << 3))
            m.emit(0x97)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VPMACSDQL xmm, m128, xmm, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1000, 0x00, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x97)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMACSDQL")
    }
    return p
}

// VPMACSSDD performs "Packed Multiply Accumulate with Saturation Signed Doubleword to Signed Doubleword".
//
// Mnemonic        : VPMACSSDD
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPMACSSDD xmm, xmm, xmm, xmm
//    * VPMACSSDD xmm, m128, xmm, xmm
//
func (self *Program) VPMACSSDD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPMACSSDD xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe8 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[2]) << 3))
            m.emit(0x8e)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VPMACSSDD xmm, m128, xmm, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1000, 0x00, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x8e)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMACSSDD")
    }
    return p
}

// VPMACSSDQH performs "Packed Multiply Accumulate with Saturation Signed High Doubleword to Signed Quadword".
//
// Mnemonic        : VPMACSSDQH
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPMACSSDQH xmm, xmm, xmm, xmm
//    * VPMACSSDQH xmm, m128, xmm, xmm
//
func (self *Program) VPMACSSDQH(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPMACSSDQH xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe8 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[2]) << 3))
            m.emit(0x8f)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VPMACSSDQH xmm, m128, xmm, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1000, 0x00, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x8f)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMACSSDQH")
    }
    return p
}

// VPMACSSDQL performs "Packed Multiply Accumulate with Saturation Signed Low Doubleword to Signed Quadword".
//
// Mnemonic        : VPMACSSDQL
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPMACSSDQL xmm, xmm, xmm, xmm
//    * VPMACSSDQL xmm, m128, xmm, xmm
//
func (self *Program) VPMACSSDQL(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPMACSSDQL xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe8 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[2]) << 3))
            m.emit(0x87)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VPMACSSDQL xmm, m128, xmm, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1000, 0x00, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x87)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMACSSDQL")
    }
    return p
}

// VPMACSSWD performs "Packed Multiply Accumulate with Saturation Signed Word to Signed Doubleword".
//
// Mnemonic        : VPMACSSWD
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPMACSSWD xmm, xmm, xmm, xmm
//    * VPMACSSWD xmm, m128, xmm, xmm
//
func (self *Program) VPMACSSWD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPMACSSWD xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe8 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[2]) << 3))
            m.emit(0x86)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VPMACSSWD xmm, m128, xmm, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1000, 0x00, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x86)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMACSSWD")
    }
    return p
}

// VPMACSSWW performs "Packed Multiply Accumulate with Saturation Signed Word to Signed Word".
//
// Mnemonic        : VPMACSSWW
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPMACSSWW xmm, xmm, xmm, xmm
//    * VPMACSSWW xmm, m128, xmm, xmm
//
func (self *Program) VPMACSSWW(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPMACSSWW xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe8 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[2]) << 3))
            m.emit(0x85)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VPMACSSWW xmm, m128, xmm, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1000, 0x00, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x85)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMACSSWW")
    }
    return p
}

// VPMACSWD performs "Packed Multiply Accumulate Signed Word to Signed Doubleword".
//
// Mnemonic        : VPMACSWD
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPMACSWD xmm, xmm, xmm, xmm
//    * VPMACSWD xmm, m128, xmm, xmm
//
func (self *Program) VPMACSWD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPMACSWD xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe8 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[2]) << 3))
            m.emit(0x96)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VPMACSWD xmm, m128, xmm, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1000, 0x00, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x96)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMACSWD")
    }
    return p
}

// VPMACSWW performs "Packed Multiply Accumulate Signed Word to Signed Word".
//
// Mnemonic        : VPMACSWW
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPMACSWW xmm, xmm, xmm, xmm
//    * VPMACSWW xmm, m128, xmm, xmm
//
func (self *Program) VPMACSWW(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPMACSWW xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe8 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[2]) << 3))
            m.emit(0x95)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VPMACSWW xmm, m128, xmm, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1000, 0x00, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x95)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMACSWW")
    }
    return p
}

// VPMADCSSWD performs "Packed Multiply Add Accumulate with Saturation Signed Word to Signed Doubleword".
//
// Mnemonic        : VPMADCSSWD
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPMADCSSWD xmm, xmm, xmm, xmm
//    * VPMADCSSWD xmm, m128, xmm, xmm
//
func (self *Program) VPMADCSSWD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPMADCSSWD xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe8 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[2]) << 3))
            m.emit(0xa6)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VPMADCSSWD xmm, m128, xmm, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1000, 0x00, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0xa6)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMADCSSWD")
    }
    return p
}

// VPMADCSWD performs "Packed Multiply Add Accumulate Signed Word to Signed Doubleword".
//
// Mnemonic        : VPMADCSWD
// ISA extensions  : XOP
// Supported forms : (2 forms)
//
//    * VPMADCSWD xmm, xmm, xmm, xmm
//    * VPMADCSWD xmm, m128, xmm, xmm
//
func (self *Program) VPMADCSWD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPMADCSWD xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe8 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[2]) << 3))
            m.emit(0xb6)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
    }
    // VPMADCSWD xmm, m128, xmm, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1000, 0x00, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0xb6)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMADCSWD")
    }
    return p
}

// VPMADD52HUQ performs "Packed Multiply of Unsigned 52-bit Unsigned Integers and Add High 52-bit Products to Quadword Accumulators".
//
// Mnemonic        : VPMADD52HUQ
// ISA extensions  : AVX512IFMA, AVX512VL
// Supported forms : (6 forms)
//
//    * VPMADD52HUQ m128/m64bcst, xmm, xmm{k}{z}
//    * VPMADD52HUQ xmm, xmm, xmm{k}{z}
//    * VPMADD52HUQ m256/m64bcst, ymm, ymm{k}{z}
//    * VPMADD52HUQ ymm, ymm, ymm{k}{z}
//    * VPMADD52HUQ m512/m64bcst, zmm, zmm{k}{z}
//    * VPMADD52HUQ zmm, zmm, zmm{k}{z}
//
func (self *Program) VPMADD52HUQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPMADD52HUQ m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512IFMA | ISA_AVX512VL)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xb5)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPMADD52HUQ xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512IFMA | ISA_AVX512VL)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xb5)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMADD52HUQ m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512IFMA | ISA_AVX512VL)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xb5)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPMADD52HUQ ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512IFMA | ISA_AVX512VL)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xb5)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMADD52HUQ m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512IFMA)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xb5)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPMADD52HUQ zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512IFMA)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xb5)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMADD52HUQ")
    }
    return p
}

// VPMADD52LUQ performs "Packed Multiply of Unsigned 52-bit Integers and Add the Low 52-bit Products to Quadword Accumulators".
//
// Mnemonic        : VPMADD52LUQ
// ISA extensions  : AVX512IFMA, AVX512VL
// Supported forms : (6 forms)
//
//    * VPMADD52LUQ m128/m64bcst, xmm, xmm{k}{z}
//    * VPMADD52LUQ xmm, xmm, xmm{k}{z}
//    * VPMADD52LUQ m256/m64bcst, ymm, ymm{k}{z}
//    * VPMADD52LUQ ymm, ymm, ymm{k}{z}
//    * VPMADD52LUQ m512/m64bcst, zmm, zmm{k}{z}
//    * VPMADD52LUQ zmm, zmm, zmm{k}{z}
//
func (self *Program) VPMADD52LUQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPMADD52LUQ m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512IFMA | ISA_AVX512VL)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xb4)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPMADD52LUQ xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512IFMA | ISA_AVX512VL)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xb4)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMADD52LUQ m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512IFMA | ISA_AVX512VL)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xb4)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPMADD52LUQ ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512IFMA | ISA_AVX512VL)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xb4)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMADD52LUQ m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512IFMA)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xb4)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPMADD52LUQ zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512IFMA)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xb4)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMADD52LUQ")
    }
    return p
}

// VPMADDUBSW performs "Multiply and Add Packed Signed and Unsigned Byte Integers".
//
// Mnemonic        : VPMADDUBSW
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPMADDUBSW xmm, xmm, xmm{k}{z}
//    * VPMADDUBSW m128, xmm, xmm{k}{z}
//    * VPMADDUBSW ymm, ymm, ymm{k}{z}
//    * VPMADDUBSW m256, ymm, ymm{k}{z}
//    * VPMADDUBSW zmm, zmm, zmm{k}{z}
//    * VPMADDUBSW m512, zmm, zmm{k}{z}
//    * VPMADDUBSW xmm, xmm, xmm
//    * VPMADDUBSW m128, xmm, xmm
//    * VPMADDUBSW ymm, ymm, ymm
//    * VPMADDUBSW m256, ymm, ymm
//
func (self *Program) VPMADDUBSW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPMADDUBSW xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x04)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMADDUBSW m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x04)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPMADDUBSW ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x04)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMADDUBSW m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x04)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPMADDUBSW zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x04)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMADDUBSW m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x04)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPMADDUBSW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x04)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMADDUBSW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x04)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPMADDUBSW ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x04)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMADDUBSW m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x04)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMADDUBSW")
    }
    return p
}

// VPMADDWD performs "Multiply and Add Packed Signed Word Integers".
//
// Mnemonic        : VPMADDWD
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPMADDWD xmm, xmm, xmm{k}{z}
//    * VPMADDWD m128, xmm, xmm{k}{z}
//    * VPMADDWD ymm, ymm, ymm{k}{z}
//    * VPMADDWD m256, ymm, ymm{k}{z}
//    * VPMADDWD zmm, zmm, zmm{k}{z}
//    * VPMADDWD m512, zmm, zmm{k}{z}
//    * VPMADDWD xmm, xmm, xmm
//    * VPMADDWD m128, xmm, xmm
//    * VPMADDWD ymm, ymm, ymm
//    * VPMADDWD m256, ymm, ymm
//
func (self *Program) VPMADDWD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPMADDWD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xf5)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMADDWD m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xf5)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPMADDWD ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xf5)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMADDWD m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xf5)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPMADDWD zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xf5)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMADDWD m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xf5)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPMADDWD xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xf5)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMADDWD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xf5)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPMADDWD ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xf5)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMADDWD m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xf5)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMADDWD")
    }
    return p
}

// VPMASKMOVD performs "Conditional Move Packed Doubleword Integers".
//
// Mnemonic        : VPMASKMOVD
// ISA extensions  : AVX2
// Supported forms : (4 forms)
//
//    * VPMASKMOVD m128, xmm, xmm
//    * VPMASKMOVD m256, ymm, ymm
//    * VPMASKMOVD xmm, xmm, m128
//    * VPMASKMOVD ymm, ymm, m256
//
func (self *Program) VPMASKMOVD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPMASKMOVD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x8c)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPMASKMOVD m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x8c)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPMASKMOVD xmm, xmm, m128
    if isXMM(v0) && isXMM(v1) && isM128(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[0]), addr(v[2]), hlcode(v[1]))
            m.emit(0x8e)
            m.mrsd(lcode(v[0]), addr(v[2]), 1)
        })
    }
    // VPMASKMOVD ymm, ymm, m256
    if isYMM(v0) && isYMM(v1) && isM256(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[0]), addr(v[2]), hlcode(v[1]))
            m.emit(0x8e)
            m.mrsd(lcode(v[0]), addr(v[2]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMASKMOVD")
    }
    return p
}

// VPMASKMOVQ performs "Conditional Move Packed Quadword Integers".
//
// Mnemonic        : VPMASKMOVQ
// ISA extensions  : AVX2
// Supported forms : (4 forms)
//
//    * VPMASKMOVQ m128, xmm, xmm
//    * VPMASKMOVQ m256, ymm, ymm
//    * VPMASKMOVQ xmm, xmm, m128
//    * VPMASKMOVQ ymm, ymm, m256
//
func (self *Program) VPMASKMOVQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPMASKMOVQ m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x8c)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPMASKMOVQ m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x85, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x8c)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPMASKMOVQ xmm, xmm, m128
    if isXMM(v0) && isXMM(v1) && isM128(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[0]), addr(v[2]), hlcode(v[1]))
            m.emit(0x8e)
            m.mrsd(lcode(v[0]), addr(v[2]), 1)
        })
    }
    // VPMASKMOVQ ymm, ymm, m256
    if isYMM(v0) && isYMM(v1) && isM256(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x85, hcode(v[0]), addr(v[2]), hlcode(v[1]))
            m.emit(0x8e)
            m.mrsd(lcode(v[0]), addr(v[2]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMASKMOVQ")
    }
    return p
}

// VPMAXSB performs "Maximum of Packed Signed Byte Integers".
//
// Mnemonic        : VPMAXSB
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPMAXSB xmm, xmm, xmm{k}{z}
//    * VPMAXSB m128, xmm, xmm{k}{z}
//    * VPMAXSB ymm, ymm, ymm{k}{z}
//    * VPMAXSB m256, ymm, ymm{k}{z}
//    * VPMAXSB zmm, zmm, zmm{k}{z}
//    * VPMAXSB m512, zmm, zmm{k}{z}
//    * VPMAXSB xmm, xmm, xmm
//    * VPMAXSB m128, xmm, xmm
//    * VPMAXSB ymm, ymm, ymm
//    * VPMAXSB m256, ymm, ymm
//
func (self *Program) VPMAXSB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPMAXSB xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x3c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXSB m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x3c)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPMAXSB ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x3c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXSB m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x3c)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPMAXSB zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x3c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXSB m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x3c)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPMAXSB xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x3c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXSB m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x3c)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPMAXSB ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x3c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXSB m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x3c)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMAXSB")
    }
    return p
}

// VPMAXSD performs "Maximum of Packed Signed Doubleword Integers".
//
// Mnemonic        : VPMAXSD
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPMAXSD m128/m32bcst, xmm, xmm{k}{z}
//    * VPMAXSD xmm, xmm, xmm{k}{z}
//    * VPMAXSD m256/m32bcst, ymm, ymm{k}{z}
//    * VPMAXSD ymm, ymm, ymm{k}{z}
//    * VPMAXSD m512/m32bcst, zmm, zmm{k}{z}
//    * VPMAXSD zmm, zmm, zmm{k}{z}
//    * VPMAXSD xmm, xmm, xmm
//    * VPMAXSD m128, xmm, xmm
//    * VPMAXSD ymm, ymm, ymm
//    * VPMAXSD m256, ymm, ymm
//
func (self *Program) VPMAXSD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPMAXSD m128/m32bcst, xmm, xmm{k}{z}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x3d)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPMAXSD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x3d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXSD m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x3d)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPMAXSD ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x3d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXSD m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x3d)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPMAXSD zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x3d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXSD xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x3d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXSD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x3d)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPMAXSD ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x3d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXSD m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x3d)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMAXSD")
    }
    return p
}

// VPMAXSQ performs "Maximum of Packed Signed Quadword Integers".
//
// Mnemonic        : VPMAXSQ
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPMAXSQ m128/m64bcst, xmm, xmm{k}{z}
//    * VPMAXSQ xmm, xmm, xmm{k}{z}
//    * VPMAXSQ m256/m64bcst, ymm, ymm{k}{z}
//    * VPMAXSQ ymm, ymm, ymm{k}{z}
//    * VPMAXSQ m512/m64bcst, zmm, zmm{k}{z}
//    * VPMAXSQ zmm, zmm, zmm{k}{z}
//
func (self *Program) VPMAXSQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPMAXSQ m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x3d)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPMAXSQ xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x3d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXSQ m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x3d)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPMAXSQ ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x3d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXSQ m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x3d)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPMAXSQ zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x3d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMAXSQ")
    }
    return p
}

// VPMAXSW performs "Maximum of Packed Signed Word Integers".
//
// Mnemonic        : VPMAXSW
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPMAXSW xmm, xmm, xmm{k}{z}
//    * VPMAXSW m128, xmm, xmm{k}{z}
//    * VPMAXSW ymm, ymm, ymm{k}{z}
//    * VPMAXSW m256, ymm, ymm{k}{z}
//    * VPMAXSW zmm, zmm, zmm{k}{z}
//    * VPMAXSW m512, zmm, zmm{k}{z}
//    * VPMAXSW xmm, xmm, xmm
//    * VPMAXSW m128, xmm, xmm
//    * VPMAXSW ymm, ymm, ymm
//    * VPMAXSW m256, ymm, ymm
//
func (self *Program) VPMAXSW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPMAXSW xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xee)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXSW m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xee)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPMAXSW ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xee)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXSW m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xee)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPMAXSW zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xee)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXSW m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xee)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPMAXSW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xee)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXSW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xee)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPMAXSW ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xee)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXSW m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xee)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMAXSW")
    }
    return p
}

// VPMAXUB performs "Maximum of Packed Unsigned Byte Integers".
//
// Mnemonic        : VPMAXUB
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPMAXUB xmm, xmm, xmm{k}{z}
//    * VPMAXUB m128, xmm, xmm{k}{z}
//    * VPMAXUB ymm, ymm, ymm{k}{z}
//    * VPMAXUB m256, ymm, ymm{k}{z}
//    * VPMAXUB zmm, zmm, zmm{k}{z}
//    * VPMAXUB m512, zmm, zmm{k}{z}
//    * VPMAXUB xmm, xmm, xmm
//    * VPMAXUB m128, xmm, xmm
//    * VPMAXUB ymm, ymm, ymm
//    * VPMAXUB m256, ymm, ymm
//
func (self *Program) VPMAXUB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPMAXUB xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xde)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXUB m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xde)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPMAXUB ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xde)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXUB m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xde)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPMAXUB zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xde)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXUB m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xde)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPMAXUB xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xde)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXUB m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xde)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPMAXUB ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xde)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXUB m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xde)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMAXUB")
    }
    return p
}

// VPMAXUD performs "Maximum of Packed Unsigned Doubleword Integers".
//
// Mnemonic        : VPMAXUD
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPMAXUD m128/m32bcst, xmm, xmm{k}{z}
//    * VPMAXUD xmm, xmm, xmm{k}{z}
//    * VPMAXUD m256/m32bcst, ymm, ymm{k}{z}
//    * VPMAXUD ymm, ymm, ymm{k}{z}
//    * VPMAXUD m512/m32bcst, zmm, zmm{k}{z}
//    * VPMAXUD zmm, zmm, zmm{k}{z}
//    * VPMAXUD xmm, xmm, xmm
//    * VPMAXUD m128, xmm, xmm
//    * VPMAXUD ymm, ymm, ymm
//    * VPMAXUD m256, ymm, ymm
//
func (self *Program) VPMAXUD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPMAXUD m128/m32bcst, xmm, xmm{k}{z}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x3f)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPMAXUD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x3f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXUD m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x3f)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPMAXUD ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x3f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXUD m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x3f)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPMAXUD zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x3f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXUD xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x3f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXUD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x3f)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPMAXUD ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x3f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXUD m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x3f)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMAXUD")
    }
    return p
}

// VPMAXUQ performs "Maximum of Packed Unsigned Quadword Integers".
//
// Mnemonic        : VPMAXUQ
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPMAXUQ m128/m64bcst, xmm, xmm{k}{z}
//    * VPMAXUQ xmm, xmm, xmm{k}{z}
//    * VPMAXUQ m256/m64bcst, ymm, ymm{k}{z}
//    * VPMAXUQ ymm, ymm, ymm{k}{z}
//    * VPMAXUQ m512/m64bcst, zmm, zmm{k}{z}
//    * VPMAXUQ zmm, zmm, zmm{k}{z}
//
func (self *Program) VPMAXUQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPMAXUQ m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x3f)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPMAXUQ xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x3f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXUQ m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x3f)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPMAXUQ ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x3f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXUQ m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x3f)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPMAXUQ zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x3f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMAXUQ")
    }
    return p
}

// VPMAXUW performs "Maximum of Packed Unsigned Word Integers".
//
// Mnemonic        : VPMAXUW
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPMAXUW xmm, xmm, xmm{k}{z}
//    * VPMAXUW m128, xmm, xmm{k}{z}
//    * VPMAXUW ymm, ymm, ymm{k}{z}
//    * VPMAXUW m256, ymm, ymm{k}{z}
//    * VPMAXUW zmm, zmm, zmm{k}{z}
//    * VPMAXUW m512, zmm, zmm{k}{z}
//    * VPMAXUW xmm, xmm, xmm
//    * VPMAXUW m128, xmm, xmm
//    * VPMAXUW ymm, ymm, ymm
//    * VPMAXUW m256, ymm, ymm
//
func (self *Program) VPMAXUW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPMAXUW xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x3e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXUW m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x3e)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPMAXUW ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x3e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXUW m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x3e)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPMAXUW zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x3e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXUW m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x3e)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPMAXUW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x3e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXUW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x3e)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPMAXUW ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x3e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMAXUW m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x3e)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMAXUW")
    }
    return p
}

// VPMINSB performs "Minimum of Packed Signed Byte Integers".
//
// Mnemonic        : VPMINSB
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPMINSB xmm, xmm, xmm{k}{z}
//    * VPMINSB m128, xmm, xmm{k}{z}
//    * VPMINSB ymm, ymm, ymm{k}{z}
//    * VPMINSB m256, ymm, ymm{k}{z}
//    * VPMINSB zmm, zmm, zmm{k}{z}
//    * VPMINSB m512, zmm, zmm{k}{z}
//    * VPMINSB xmm, xmm, xmm
//    * VPMINSB m128, xmm, xmm
//    * VPMINSB ymm, ymm, ymm
//    * VPMINSB m256, ymm, ymm
//
func (self *Program) VPMINSB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPMINSB xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x38)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINSB m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x38)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPMINSB ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x38)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINSB m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x38)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPMINSB zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x38)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINSB m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x38)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPMINSB xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x38)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINSB m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x38)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPMINSB ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x38)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINSB m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x38)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMINSB")
    }
    return p
}

// VPMINSD performs "Minimum of Packed Signed Doubleword Integers".
//
// Mnemonic        : VPMINSD
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPMINSD m128/m32bcst, xmm, xmm{k}{z}
//    * VPMINSD xmm, xmm, xmm{k}{z}
//    * VPMINSD m256/m32bcst, ymm, ymm{k}{z}
//    * VPMINSD ymm, ymm, ymm{k}{z}
//    * VPMINSD m512/m32bcst, zmm, zmm{k}{z}
//    * VPMINSD zmm, zmm, zmm{k}{z}
//    * VPMINSD xmm, xmm, xmm
//    * VPMINSD m128, xmm, xmm
//    * VPMINSD ymm, ymm, ymm
//    * VPMINSD m256, ymm, ymm
//
func (self *Program) VPMINSD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPMINSD m128/m32bcst, xmm, xmm{k}{z}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x39)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPMINSD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x39)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINSD m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x39)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPMINSD ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x39)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINSD m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x39)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPMINSD zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x39)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINSD xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x39)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINSD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x39)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPMINSD ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x39)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINSD m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x39)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMINSD")
    }
    return p
}

// VPMINSQ performs "Minimum of Packed Signed Quadword Integers".
//
// Mnemonic        : VPMINSQ
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPMINSQ m128/m64bcst, xmm, xmm{k}{z}
//    * VPMINSQ xmm, xmm, xmm{k}{z}
//    * VPMINSQ m256/m64bcst, ymm, ymm{k}{z}
//    * VPMINSQ ymm, ymm, ymm{k}{z}
//    * VPMINSQ m512/m64bcst, zmm, zmm{k}{z}
//    * VPMINSQ zmm, zmm, zmm{k}{z}
//
func (self *Program) VPMINSQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPMINSQ m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x39)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPMINSQ xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x39)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINSQ m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x39)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPMINSQ ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x39)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINSQ m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x39)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPMINSQ zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x39)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMINSQ")
    }
    return p
}

// VPMINSW performs "Minimum of Packed Signed Word Integers".
//
// Mnemonic        : VPMINSW
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPMINSW xmm, xmm, xmm{k}{z}
//    * VPMINSW m128, xmm, xmm{k}{z}
//    * VPMINSW ymm, ymm, ymm{k}{z}
//    * VPMINSW m256, ymm, ymm{k}{z}
//    * VPMINSW zmm, zmm, zmm{k}{z}
//    * VPMINSW m512, zmm, zmm{k}{z}
//    * VPMINSW xmm, xmm, xmm
//    * VPMINSW m128, xmm, xmm
//    * VPMINSW ymm, ymm, ymm
//    * VPMINSW m256, ymm, ymm
//
func (self *Program) VPMINSW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPMINSW xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xea)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINSW m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xea)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPMINSW ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xea)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINSW m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xea)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPMINSW zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xea)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINSW m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xea)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPMINSW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xea)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINSW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xea)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPMINSW ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xea)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINSW m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xea)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMINSW")
    }
    return p
}

// VPMINUB performs "Minimum of Packed Unsigned Byte Integers".
//
// Mnemonic        : VPMINUB
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPMINUB xmm, xmm, xmm{k}{z}
//    * VPMINUB m128, xmm, xmm{k}{z}
//    * VPMINUB ymm, ymm, ymm{k}{z}
//    * VPMINUB m256, ymm, ymm{k}{z}
//    * VPMINUB zmm, zmm, zmm{k}{z}
//    * VPMINUB m512, zmm, zmm{k}{z}
//    * VPMINUB xmm, xmm, xmm
//    * VPMINUB m128, xmm, xmm
//    * VPMINUB ymm, ymm, ymm
//    * VPMINUB m256, ymm, ymm
//
func (self *Program) VPMINUB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPMINUB xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xda)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINUB m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xda)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPMINUB ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xda)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINUB m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xda)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPMINUB zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xda)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINUB m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xda)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPMINUB xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xda)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINUB m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xda)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPMINUB ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xda)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINUB m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xda)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMINUB")
    }
    return p
}

// VPMINUD performs "Minimum of Packed Unsigned Doubleword Integers".
//
// Mnemonic        : VPMINUD
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPMINUD m128/m32bcst, xmm, xmm{k}{z}
//    * VPMINUD xmm, xmm, xmm{k}{z}
//    * VPMINUD m256/m32bcst, ymm, ymm{k}{z}
//    * VPMINUD ymm, ymm, ymm{k}{z}
//    * VPMINUD m512/m32bcst, zmm, zmm{k}{z}
//    * VPMINUD zmm, zmm, zmm{k}{z}
//    * VPMINUD xmm, xmm, xmm
//    * VPMINUD m128, xmm, xmm
//    * VPMINUD ymm, ymm, ymm
//    * VPMINUD m256, ymm, ymm
//
func (self *Program) VPMINUD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPMINUD m128/m32bcst, xmm, xmm{k}{z}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x3b)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPMINUD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x3b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINUD m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x3b)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPMINUD ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x3b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINUD m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x3b)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPMINUD zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x3b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINUD xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x3b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINUD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x3b)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPMINUD ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x3b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINUD m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x3b)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMINUD")
    }
    return p
}

// VPMINUQ performs "Minimum of Packed Unsigned Quadword Integers".
//
// Mnemonic        : VPMINUQ
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPMINUQ m128/m64bcst, xmm, xmm{k}{z}
//    * VPMINUQ xmm, xmm, xmm{k}{z}
//    * VPMINUQ m256/m64bcst, ymm, ymm{k}{z}
//    * VPMINUQ ymm, ymm, ymm{k}{z}
//    * VPMINUQ m512/m64bcst, zmm, zmm{k}{z}
//    * VPMINUQ zmm, zmm, zmm{k}{z}
//
func (self *Program) VPMINUQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPMINUQ m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x3b)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPMINUQ xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x3b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINUQ m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x3b)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPMINUQ ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x3b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINUQ m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x3b)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPMINUQ zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x3b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMINUQ")
    }
    return p
}

// VPMINUW performs "Minimum of Packed Unsigned Word Integers".
//
// Mnemonic        : VPMINUW
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPMINUW xmm, xmm, xmm{k}{z}
//    * VPMINUW m128, xmm, xmm{k}{z}
//    * VPMINUW ymm, ymm, ymm{k}{z}
//    * VPMINUW m256, ymm, ymm{k}{z}
//    * VPMINUW zmm, zmm, zmm{k}{z}
//    * VPMINUW m512, zmm, zmm{k}{z}
//    * VPMINUW xmm, xmm, xmm
//    * VPMINUW m128, xmm, xmm
//    * VPMINUW ymm, ymm, ymm
//    * VPMINUW m256, ymm, ymm
//
func (self *Program) VPMINUW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPMINUW xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x3a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINUW m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x3a)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPMINUW ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x3a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINUW m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x3a)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPMINUW zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x3a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINUW m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x3a)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPMINUW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x3a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINUW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x3a)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPMINUW ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x3a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMINUW m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x3a)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMINUW")
    }
    return p
}

// VPMOVB2M performs "Move Signs of Packed Byte Integers to Mask Register".
//
// Mnemonic        : VPMOVB2M
// ISA extensions  : AVX512BW, AVX512VL
// Supported forms : (3 forms)
//
//    * VPMOVB2M xmm, k
//    * VPMOVB2M ymm, k
//    * VPMOVB2M zmm, k
//
func (self *Program) VPMOVB2M(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVB2M xmm, k
    if isEVEXXMM(v0) && isK(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit(0x08)
            m.emit(0x29)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVB2M ymm, k
    if isEVEXYMM(v0) && isK(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit(0x28)
            m.emit(0x29)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVB2M zmm, k
    if isZMM(v0) && isK(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit(0x48)
            m.emit(0x29)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVB2M")
    }
    return p
}

// VPMOVD2M performs "Move Signs of Packed Doubleword Integers to Mask Register".
//
// Mnemonic        : VPMOVD2M
// ISA extensions  : AVX512DQ, AVX512VL
// Supported forms : (3 forms)
//
//    * VPMOVD2M xmm, k
//    * VPMOVD2M ymm, k
//    * VPMOVD2M zmm, k
//
func (self *Program) VPMOVD2M(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVD2M xmm, k
    if isEVEXXMM(v0) && isK(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit(0x08)
            m.emit(0x39)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVD2M ymm, k
    if isEVEXYMM(v0) && isK(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit(0x28)
            m.emit(0x39)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVD2M zmm, k
    if isZMM(v0) && isK(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit(0x48)
            m.emit(0x39)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVD2M")
    }
    return p
}

// VPMOVDB performs "Down Convert Packed Doubleword Values to Byte Values with Truncation".
//
// Mnemonic        : VPMOVDB
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPMOVDB xmm, xmm{k}{z}
//    * VPMOVDB xmm, m32{k}{z}
//    * VPMOVDB ymm, xmm{k}{z}
//    * VPMOVDB ymm, m64{k}{z}
//    * VPMOVDB zmm, xmm{k}{z}
//    * VPMOVDB zmm, m128{k}{z}
//
func (self *Program) VPMOVDB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVDB xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x31)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVDB xmm, m32{k}{z}
    if isEVEXXMM(v0) && isM32kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x31)
            m.mrsd(lcode(v[0]), addr(v[1]), 4)
        })
    }
    // VPMOVDB ymm, xmm{k}{z}
    if isEVEXYMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x31)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVDB ymm, m64{k}{z}
    if isEVEXYMM(v0) && isM64kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x31)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    // VPMOVDB zmm, xmm{k}{z}
    if isZMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x31)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVDB zmm, m128{k}{z}
    if isZMM(v0) && isM128kz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x31)
            m.mrsd(lcode(v[0]), addr(v[1]), 16)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVDB")
    }
    return p
}

// VPMOVDW performs "Down Convert Packed Doubleword Values to Word Values with Truncation".
//
// Mnemonic        : VPMOVDW
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPMOVDW xmm, xmm{k}{z}
//    * VPMOVDW xmm, m64{k}{z}
//    * VPMOVDW ymm, xmm{k}{z}
//    * VPMOVDW ymm, m128{k}{z}
//    * VPMOVDW zmm, ymm{k}{z}
//    * VPMOVDW zmm, m256{k}{z}
//
func (self *Program) VPMOVDW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVDW xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x33)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVDW xmm, m64{k}{z}
    if isEVEXXMM(v0) && isM64kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x33)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    // VPMOVDW ymm, xmm{k}{z}
    if isEVEXYMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x33)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVDW ymm, m128{k}{z}
    if isEVEXYMM(v0) && isM128kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x33)
            m.mrsd(lcode(v[0]), addr(v[1]), 16)
        })
    }
    // VPMOVDW zmm, ymm{k}{z}
    if isZMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x33)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVDW zmm, m256{k}{z}
    if isZMM(v0) && isM256kz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x33)
            m.mrsd(lcode(v[0]), addr(v[1]), 32)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVDW")
    }
    return p
}

// VPMOVM2B performs "Expand Bits of Mask Register to Packed Byte Integers".
//
// Mnemonic        : VPMOVM2B
// ISA extensions  : AVX512BW, AVX512VL
// Supported forms : (3 forms)
//
//    * VPMOVM2B k, xmm
//    * VPMOVM2B k, ymm
//    * VPMOVM2B k, zmm
//
func (self *Program) VPMOVM2B(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVM2B k, xmm
    if isK(v0) && isEVEXXMM(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit(0x08)
            m.emit(0x28)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVM2B k, ymm
    if isK(v0) && isEVEXYMM(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit(0x28)
            m.emit(0x28)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVM2B k, zmm
    if isK(v0) && isZMM(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit(0x48)
            m.emit(0x28)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVM2B")
    }
    return p
}

// VPMOVM2D performs "Expand Bits of Mask Register to Packed Doubleword Integers".
//
// Mnemonic        : VPMOVM2D
// ISA extensions  : AVX512DQ, AVX512VL
// Supported forms : (3 forms)
//
//    * VPMOVM2D k, xmm
//    * VPMOVM2D k, ymm
//    * VPMOVM2D k, zmm
//
func (self *Program) VPMOVM2D(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVM2D k, xmm
    if isK(v0) && isEVEXXMM(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit(0x08)
            m.emit(0x38)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVM2D k, ymm
    if isK(v0) && isEVEXYMM(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit(0x28)
            m.emit(0x38)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVM2D k, zmm
    if isK(v0) && isZMM(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7e)
            m.emit(0x48)
            m.emit(0x38)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVM2D")
    }
    return p
}

// VPMOVM2Q performs "Expand Bits of Mask Register to Packed Quadword Integers".
//
// Mnemonic        : VPMOVM2Q
// ISA extensions  : AVX512DQ, AVX512VL
// Supported forms : (3 forms)
//
//    * VPMOVM2Q k, xmm
//    * VPMOVM2Q k, ymm
//    * VPMOVM2Q k, zmm
//
func (self *Program) VPMOVM2Q(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVM2Q k, xmm
    if isK(v0) && isEVEXXMM(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfe)
            m.emit(0x08)
            m.emit(0x38)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVM2Q k, ymm
    if isK(v0) && isEVEXYMM(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfe)
            m.emit(0x28)
            m.emit(0x38)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVM2Q k, zmm
    if isK(v0) && isZMM(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfe)
            m.emit(0x48)
            m.emit(0x38)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVM2Q")
    }
    return p
}

// VPMOVM2W performs "Expand Bits of Mask Register to Packed Word Integers".
//
// Mnemonic        : VPMOVM2W
// ISA extensions  : AVX512BW, AVX512VL
// Supported forms : (3 forms)
//
//    * VPMOVM2W k, xmm
//    * VPMOVM2W k, ymm
//    * VPMOVM2W k, zmm
//
func (self *Program) VPMOVM2W(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVM2W k, xmm
    if isK(v0) && isEVEXXMM(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfe)
            m.emit(0x08)
            m.emit(0x28)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVM2W k, ymm
    if isK(v0) && isEVEXYMM(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfe)
            m.emit(0x28)
            m.emit(0x28)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVM2W k, zmm
    if isK(v0) && isZMM(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfe)
            m.emit(0x48)
            m.emit(0x28)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVM2W")
    }
    return p
}

// VPMOVMSKB performs "Move Byte Mask".
//
// Mnemonic        : VPMOVMSKB
// ISA extensions  : AVX, AVX2
// Supported forms : (2 forms)
//
//    * VPMOVMSKB xmm, r32
//    * VPMOVMSKB ymm, r32
//
func (self *Program) VPMOVMSKB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVMSKB xmm, r32
    if isXMM(v0) && isReg32(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[1]), v[0], 0)
            m.emit(0xd7)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVMSKB ymm, r32
    if isYMM(v0) && isReg32(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[1]), v[0], 0)
            m.emit(0xd7)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVMSKB")
    }
    return p
}

// VPMOVQ2M performs "Move Signs of Packed Quadword Integers to Mask Register".
//
// Mnemonic        : VPMOVQ2M
// ISA extensions  : AVX512DQ, AVX512VL
// Supported forms : (3 forms)
//
//    * VPMOVQ2M xmm, k
//    * VPMOVQ2M ymm, k
//    * VPMOVQ2M zmm, k
//
func (self *Program) VPMOVQ2M(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVQ2M xmm, k
    if isEVEXXMM(v0) && isK(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfe)
            m.emit(0x08)
            m.emit(0x39)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVQ2M ymm, k
    if isEVEXYMM(v0) && isK(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfe)
            m.emit(0x28)
            m.emit(0x39)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVQ2M zmm, k
    if isZMM(v0) && isK(v1) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfe)
            m.emit(0x48)
            m.emit(0x39)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVQ2M")
    }
    return p
}

// VPMOVQB performs "Down Convert Packed Quadword Values to Byte Values with Truncation".
//
// Mnemonic        : VPMOVQB
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPMOVQB xmm, xmm{k}{z}
//    * VPMOVQB xmm, m16{k}{z}
//    * VPMOVQB ymm, xmm{k}{z}
//    * VPMOVQB ymm, m32{k}{z}
//    * VPMOVQB zmm, xmm{k}{z}
//    * VPMOVQB zmm, m64{k}{z}
//
func (self *Program) VPMOVQB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVQB xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x32)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVQB xmm, m16{k}{z}
    if isEVEXXMM(v0) && isM16kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x32)
            m.mrsd(lcode(v[0]), addr(v[1]), 2)
        })
    }
    // VPMOVQB ymm, xmm{k}{z}
    if isEVEXYMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x32)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVQB ymm, m32{k}{z}
    if isEVEXYMM(v0) && isM32kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x32)
            m.mrsd(lcode(v[0]), addr(v[1]), 4)
        })
    }
    // VPMOVQB zmm, xmm{k}{z}
    if isZMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x32)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVQB zmm, m64{k}{z}
    if isZMM(v0) && isM64kz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x32)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVQB")
    }
    return p
}

// VPMOVQD performs "Down Convert Packed Quadword Values to Doubleword Values with Truncation".
//
// Mnemonic        : VPMOVQD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPMOVQD xmm, xmm{k}{z}
//    * VPMOVQD xmm, m64{k}{z}
//    * VPMOVQD ymm, xmm{k}{z}
//    * VPMOVQD ymm, m128{k}{z}
//    * VPMOVQD zmm, ymm{k}{z}
//    * VPMOVQD zmm, m256{k}{z}
//
func (self *Program) VPMOVQD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVQD xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x35)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVQD xmm, m64{k}{z}
    if isEVEXXMM(v0) && isM64kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x35)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    // VPMOVQD ymm, xmm{k}{z}
    if isEVEXYMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x35)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVQD ymm, m128{k}{z}
    if isEVEXYMM(v0) && isM128kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x35)
            m.mrsd(lcode(v[0]), addr(v[1]), 16)
        })
    }
    // VPMOVQD zmm, ymm{k}{z}
    if isZMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x35)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVQD zmm, m256{k}{z}
    if isZMM(v0) && isM256kz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x35)
            m.mrsd(lcode(v[0]), addr(v[1]), 32)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVQD")
    }
    return p
}

// VPMOVQW performs "Down Convert Packed Quadword Values to Word Values with Truncation".
//
// Mnemonic        : VPMOVQW
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPMOVQW xmm, xmm{k}{z}
//    * VPMOVQW xmm, m32{k}{z}
//    * VPMOVQW ymm, xmm{k}{z}
//    * VPMOVQW ymm, m64{k}{z}
//    * VPMOVQW zmm, xmm{k}{z}
//    * VPMOVQW zmm, m128{k}{z}
//
func (self *Program) VPMOVQW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVQW xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x34)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVQW xmm, m32{k}{z}
    if isEVEXXMM(v0) && isM32kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x34)
            m.mrsd(lcode(v[0]), addr(v[1]), 4)
        })
    }
    // VPMOVQW ymm, xmm{k}{z}
    if isEVEXYMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x34)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVQW ymm, m64{k}{z}
    if isEVEXYMM(v0) && isM64kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x34)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    // VPMOVQW zmm, xmm{k}{z}
    if isZMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x34)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVQW zmm, m128{k}{z}
    if isZMM(v0) && isM128kz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x34)
            m.mrsd(lcode(v[0]), addr(v[1]), 16)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVQW")
    }
    return p
}

// VPMOVSDB performs "Down Convert Packed Doubleword Values to Byte Values with Signed Saturation".
//
// Mnemonic        : VPMOVSDB
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPMOVSDB xmm, xmm{k}{z}
//    * VPMOVSDB xmm, m32{k}{z}
//    * VPMOVSDB ymm, xmm{k}{z}
//    * VPMOVSDB ymm, m64{k}{z}
//    * VPMOVSDB zmm, xmm{k}{z}
//    * VPMOVSDB zmm, m128{k}{z}
//
func (self *Program) VPMOVSDB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVSDB xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x21)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVSDB xmm, m32{k}{z}
    if isEVEXXMM(v0) && isM32kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x21)
            m.mrsd(lcode(v[0]), addr(v[1]), 4)
        })
    }
    // VPMOVSDB ymm, xmm{k}{z}
    if isEVEXYMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x21)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVSDB ymm, m64{k}{z}
    if isEVEXYMM(v0) && isM64kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x21)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    // VPMOVSDB zmm, xmm{k}{z}
    if isZMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x21)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVSDB zmm, m128{k}{z}
    if isZMM(v0) && isM128kz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x21)
            m.mrsd(lcode(v[0]), addr(v[1]), 16)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVSDB")
    }
    return p
}

// VPMOVSDW performs "Down Convert Packed Doubleword Values to Word Values with Signed Saturation".
//
// Mnemonic        : VPMOVSDW
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPMOVSDW xmm, xmm{k}{z}
//    * VPMOVSDW xmm, m64{k}{z}
//    * VPMOVSDW ymm, xmm{k}{z}
//    * VPMOVSDW ymm, m128{k}{z}
//    * VPMOVSDW zmm, ymm{k}{z}
//    * VPMOVSDW zmm, m256{k}{z}
//
func (self *Program) VPMOVSDW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVSDW xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x23)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVSDW xmm, m64{k}{z}
    if isEVEXXMM(v0) && isM64kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x23)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    // VPMOVSDW ymm, xmm{k}{z}
    if isEVEXYMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x23)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVSDW ymm, m128{k}{z}
    if isEVEXYMM(v0) && isM128kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x23)
            m.mrsd(lcode(v[0]), addr(v[1]), 16)
        })
    }
    // VPMOVSDW zmm, ymm{k}{z}
    if isZMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x23)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVSDW zmm, m256{k}{z}
    if isZMM(v0) && isM256kz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x23)
            m.mrsd(lcode(v[0]), addr(v[1]), 32)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVSDW")
    }
    return p
}

// VPMOVSQB performs "Down Convert Packed Quadword Values to Byte Values with Signed Saturation".
//
// Mnemonic        : VPMOVSQB
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPMOVSQB xmm, xmm{k}{z}
//    * VPMOVSQB xmm, m16{k}{z}
//    * VPMOVSQB ymm, xmm{k}{z}
//    * VPMOVSQB ymm, m32{k}{z}
//    * VPMOVSQB zmm, xmm{k}{z}
//    * VPMOVSQB zmm, m64{k}{z}
//
func (self *Program) VPMOVSQB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVSQB xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x22)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVSQB xmm, m16{k}{z}
    if isEVEXXMM(v0) && isM16kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x22)
            m.mrsd(lcode(v[0]), addr(v[1]), 2)
        })
    }
    // VPMOVSQB ymm, xmm{k}{z}
    if isEVEXYMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x22)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVSQB ymm, m32{k}{z}
    if isEVEXYMM(v0) && isM32kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x22)
            m.mrsd(lcode(v[0]), addr(v[1]), 4)
        })
    }
    // VPMOVSQB zmm, xmm{k}{z}
    if isZMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x22)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVSQB zmm, m64{k}{z}
    if isZMM(v0) && isM64kz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x22)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVSQB")
    }
    return p
}

// VPMOVSQD performs "Down Convert Packed Quadword Values to Doubleword Values with Signed Saturation".
//
// Mnemonic        : VPMOVSQD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPMOVSQD xmm, xmm{k}{z}
//    * VPMOVSQD xmm, m64{k}{z}
//    * VPMOVSQD ymm, xmm{k}{z}
//    * VPMOVSQD ymm, m128{k}{z}
//    * VPMOVSQD zmm, ymm{k}{z}
//    * VPMOVSQD zmm, m256{k}{z}
//
func (self *Program) VPMOVSQD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVSQD xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x25)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVSQD xmm, m64{k}{z}
    if isEVEXXMM(v0) && isM64kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x25)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    // VPMOVSQD ymm, xmm{k}{z}
    if isEVEXYMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x25)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVSQD ymm, m128{k}{z}
    if isEVEXYMM(v0) && isM128kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x25)
            m.mrsd(lcode(v[0]), addr(v[1]), 16)
        })
    }
    // VPMOVSQD zmm, ymm{k}{z}
    if isZMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x25)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVSQD zmm, m256{k}{z}
    if isZMM(v0) && isM256kz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x25)
            m.mrsd(lcode(v[0]), addr(v[1]), 32)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVSQD")
    }
    return p
}

// VPMOVSQW performs "Down Convert Packed Quadword Values to Word Values with Signed Saturation".
//
// Mnemonic        : VPMOVSQW
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPMOVSQW xmm, xmm{k}{z}
//    * VPMOVSQW xmm, m32{k}{z}
//    * VPMOVSQW ymm, xmm{k}{z}
//    * VPMOVSQW ymm, m64{k}{z}
//    * VPMOVSQW zmm, xmm{k}{z}
//    * VPMOVSQW zmm, m128{k}{z}
//
func (self *Program) VPMOVSQW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVSQW xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x24)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVSQW xmm, m32{k}{z}
    if isEVEXXMM(v0) && isM32kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x24)
            m.mrsd(lcode(v[0]), addr(v[1]), 4)
        })
    }
    // VPMOVSQW ymm, xmm{k}{z}
    if isEVEXYMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x24)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVSQW ymm, m64{k}{z}
    if isEVEXYMM(v0) && isM64kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x24)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    // VPMOVSQW zmm, xmm{k}{z}
    if isZMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x24)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVSQW zmm, m128{k}{z}
    if isZMM(v0) && isM128kz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x24)
            m.mrsd(lcode(v[0]), addr(v[1]), 16)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVSQW")
    }
    return p
}

// VPMOVSWB performs "Down Convert Packed Word Values to Byte Values with Signed Saturation".
//
// Mnemonic        : VPMOVSWB
// ISA extensions  : AVX512BW, AVX512VL
// Supported forms : (6 forms)
//
//    * VPMOVSWB xmm, xmm{k}{z}
//    * VPMOVSWB xmm, m64{k}{z}
//    * VPMOVSWB ymm, xmm{k}{z}
//    * VPMOVSWB ymm, m128{k}{z}
//    * VPMOVSWB zmm, ymm{k}{z}
//    * VPMOVSWB zmm, m256{k}{z}
//
func (self *Program) VPMOVSWB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVSWB xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x20)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVSWB xmm, m64{k}{z}
    if isEVEXXMM(v0) && isM64kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x20)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    // VPMOVSWB ymm, xmm{k}{z}
    if isEVEXYMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x20)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVSWB ymm, m128{k}{z}
    if isEVEXYMM(v0) && isM128kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x20)
            m.mrsd(lcode(v[0]), addr(v[1]), 16)
        })
    }
    // VPMOVSWB zmm, ymm{k}{z}
    if isZMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x20)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVSWB zmm, m256{k}{z}
    if isZMM(v0) && isM256kz(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x20)
            m.mrsd(lcode(v[0]), addr(v[1]), 32)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVSWB")
    }
    return p
}

// VPMOVSXBD performs "Move Packed Byte Integers to Doubleword Integers with Sign Extension".
//
// Mnemonic        : VPMOVSXBD
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPMOVSXBD xmm, xmm{k}{z}
//    * VPMOVSXBD xmm, ymm{k}{z}
//    * VPMOVSXBD xmm, zmm{k}{z}
//    * VPMOVSXBD m32, xmm{k}{z}
//    * VPMOVSXBD m64, ymm{k}{z}
//    * VPMOVSXBD m128, zmm{k}{z}
//    * VPMOVSXBD xmm, xmm
//    * VPMOVSXBD m32, xmm
//    * VPMOVSXBD xmm, ymm
//    * VPMOVSXBD m64, ymm
//
func (self *Program) VPMOVSXBD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVSXBD xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x21)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVSXBD xmm, ymm{k}{z}
    if isEVEXXMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x21)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVSXBD xmm, zmm{k}{z}
    if isEVEXXMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x21)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVSXBD m32, xmm{k}{z}
    if isM32(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x21)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VPMOVSXBD m64, ymm{k}{z}
    if isM64(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x21)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VPMOVSXBD m128, zmm{k}{z}
    if isM128(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x21)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VPMOVSXBD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79)
            m.emit(0x21)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVSXBD m32, xmm
    if isM32(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x21)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VPMOVSXBD xmm, ymm
    if isXMM(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d)
            m.emit(0x21)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVSXBD m64, ymm
    if isM64(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x21)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVSXBD")
    }
    return p
}

// VPMOVSXBQ performs "Move Packed Byte Integers to Quadword Integers with Sign Extension".
//
// Mnemonic        : VPMOVSXBQ
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPMOVSXBQ xmm, xmm{k}{z}
//    * VPMOVSXBQ xmm, ymm{k}{z}
//    * VPMOVSXBQ xmm, zmm{k}{z}
//    * VPMOVSXBQ m16, xmm{k}{z}
//    * VPMOVSXBQ m32, ymm{k}{z}
//    * VPMOVSXBQ m64, zmm{k}{z}
//    * VPMOVSXBQ xmm, xmm
//    * VPMOVSXBQ m16, xmm
//    * VPMOVSXBQ xmm, ymm
//    * VPMOVSXBQ m32, ymm
//
func (self *Program) VPMOVSXBQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVSXBQ xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x22)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVSXBQ xmm, ymm{k}{z}
    if isEVEXXMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x22)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVSXBQ xmm, zmm{k}{z}
    if isEVEXXMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x22)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVSXBQ m16, xmm{k}{z}
    if isM16(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x22)
            m.mrsd(lcode(v[1]), addr(v[0]), 2)
        })
    }
    // VPMOVSXBQ m32, ymm{k}{z}
    if isM32(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x22)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VPMOVSXBQ m64, zmm{k}{z}
    if isM64(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x22)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VPMOVSXBQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79)
            m.emit(0x22)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVSXBQ m16, xmm
    if isM16(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x22)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VPMOVSXBQ xmm, ymm
    if isXMM(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d)
            m.emit(0x22)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVSXBQ m32, ymm
    if isM32(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x22)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVSXBQ")
    }
    return p
}

// VPMOVSXBW performs "Move Packed Byte Integers to Word Integers with Sign Extension".
//
// Mnemonic        : VPMOVSXBW
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPMOVSXBW xmm, xmm{k}{z}
//    * VPMOVSXBW xmm, ymm{k}{z}
//    * VPMOVSXBW ymm, zmm{k}{z}
//    * VPMOVSXBW m64, xmm{k}{z}
//    * VPMOVSXBW m128, ymm{k}{z}
//    * VPMOVSXBW m256, zmm{k}{z}
//    * VPMOVSXBW xmm, xmm
//    * VPMOVSXBW m64, xmm
//    * VPMOVSXBW xmm, ymm
//    * VPMOVSXBW m128, ymm
//
func (self *Program) VPMOVSXBW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVSXBW xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x20)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVSXBW xmm, ymm{k}{z}
    if isEVEXXMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x20)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVSXBW ymm, zmm{k}{z}
    if isEVEXYMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x20)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVSXBW m64, xmm{k}{z}
    if isM64(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x20)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VPMOVSXBW m128, ymm{k}{z}
    if isM128(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x20)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VPMOVSXBW m256, zmm{k}{z}
    if isM256(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x20)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VPMOVSXBW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79)
            m.emit(0x20)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVSXBW m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x20)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VPMOVSXBW xmm, ymm
    if isXMM(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d)
            m.emit(0x20)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVSXBW m128, ymm
    if isM128(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x20)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVSXBW")
    }
    return p
}

// VPMOVSXDQ performs "Move Packed Doubleword Integers to Quadword Integers with Sign Extension".
//
// Mnemonic        : VPMOVSXDQ
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPMOVSXDQ xmm, xmm{k}{z}
//    * VPMOVSXDQ xmm, ymm{k}{z}
//    * VPMOVSXDQ ymm, zmm{k}{z}
//    * VPMOVSXDQ m64, xmm{k}{z}
//    * VPMOVSXDQ m128, ymm{k}{z}
//    * VPMOVSXDQ m256, zmm{k}{z}
//    * VPMOVSXDQ xmm, xmm
//    * VPMOVSXDQ m64, xmm
//    * VPMOVSXDQ xmm, ymm
//    * VPMOVSXDQ m128, ymm
//
func (self *Program) VPMOVSXDQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVSXDQ xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x25)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVSXDQ xmm, ymm{k}{z}
    if isEVEXXMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x25)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVSXDQ ymm, zmm{k}{z}
    if isEVEXYMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x25)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVSXDQ m64, xmm{k}{z}
    if isM64(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x25)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VPMOVSXDQ m128, ymm{k}{z}
    if isM128(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x25)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VPMOVSXDQ m256, zmm{k}{z}
    if isM256(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x25)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VPMOVSXDQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79)
            m.emit(0x25)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVSXDQ m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x25)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VPMOVSXDQ xmm, ymm
    if isXMM(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d)
            m.emit(0x25)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVSXDQ m128, ymm
    if isM128(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x25)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVSXDQ")
    }
    return p
}

// VPMOVSXWD performs "Move Packed Word Integers to Doubleword Integers with Sign Extension".
//
// Mnemonic        : VPMOVSXWD
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPMOVSXWD xmm, xmm{k}{z}
//    * VPMOVSXWD xmm, ymm{k}{z}
//    * VPMOVSXWD ymm, zmm{k}{z}
//    * VPMOVSXWD m64, xmm{k}{z}
//    * VPMOVSXWD m128, ymm{k}{z}
//    * VPMOVSXWD m256, zmm{k}{z}
//    * VPMOVSXWD xmm, xmm
//    * VPMOVSXWD m64, xmm
//    * VPMOVSXWD xmm, ymm
//    * VPMOVSXWD m128, ymm
//
func (self *Program) VPMOVSXWD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVSXWD xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x23)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVSXWD xmm, ymm{k}{z}
    if isEVEXXMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x23)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVSXWD ymm, zmm{k}{z}
    if isEVEXYMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x23)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVSXWD m64, xmm{k}{z}
    if isM64(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x23)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VPMOVSXWD m128, ymm{k}{z}
    if isM128(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x23)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VPMOVSXWD m256, zmm{k}{z}
    if isM256(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x23)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VPMOVSXWD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79)
            m.emit(0x23)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVSXWD m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x23)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VPMOVSXWD xmm, ymm
    if isXMM(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d)
            m.emit(0x23)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVSXWD m128, ymm
    if isM128(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x23)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVSXWD")
    }
    return p
}

// VPMOVSXWQ performs "Move Packed Word Integers to Quadword Integers with Sign Extension".
//
// Mnemonic        : VPMOVSXWQ
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPMOVSXWQ xmm, xmm{k}{z}
//    * VPMOVSXWQ xmm, ymm{k}{z}
//    * VPMOVSXWQ xmm, zmm{k}{z}
//    * VPMOVSXWQ m32, xmm{k}{z}
//    * VPMOVSXWQ m64, ymm{k}{z}
//    * VPMOVSXWQ m128, zmm{k}{z}
//    * VPMOVSXWQ xmm, xmm
//    * VPMOVSXWQ m32, xmm
//    * VPMOVSXWQ xmm, ymm
//    * VPMOVSXWQ m64, ymm
//
func (self *Program) VPMOVSXWQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVSXWQ xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x24)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVSXWQ xmm, ymm{k}{z}
    if isEVEXXMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x24)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVSXWQ xmm, zmm{k}{z}
    if isEVEXXMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x24)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVSXWQ m32, xmm{k}{z}
    if isM32(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x24)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VPMOVSXWQ m64, ymm{k}{z}
    if isM64(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x24)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VPMOVSXWQ m128, zmm{k}{z}
    if isM128(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x24)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VPMOVSXWQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79)
            m.emit(0x24)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVSXWQ m32, xmm
    if isM32(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x24)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VPMOVSXWQ xmm, ymm
    if isXMM(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d)
            m.emit(0x24)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVSXWQ m64, ymm
    if isM64(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x24)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVSXWQ")
    }
    return p
}

// VPMOVUSDB performs "Down Convert Packed Doubleword Values to Byte Values with Unsigned Saturation".
//
// Mnemonic        : VPMOVUSDB
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPMOVUSDB xmm, xmm{k}{z}
//    * VPMOVUSDB xmm, m32{k}{z}
//    * VPMOVUSDB ymm, xmm{k}{z}
//    * VPMOVUSDB ymm, m64{k}{z}
//    * VPMOVUSDB zmm, xmm{k}{z}
//    * VPMOVUSDB zmm, m128{k}{z}
//
func (self *Program) VPMOVUSDB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVUSDB xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x11)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVUSDB xmm, m32{k}{z}
    if isEVEXXMM(v0) && isM32kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x11)
            m.mrsd(lcode(v[0]), addr(v[1]), 4)
        })
    }
    // VPMOVUSDB ymm, xmm{k}{z}
    if isEVEXYMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x11)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVUSDB ymm, m64{k}{z}
    if isEVEXYMM(v0) && isM64kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x11)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    // VPMOVUSDB zmm, xmm{k}{z}
    if isZMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x11)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVUSDB zmm, m128{k}{z}
    if isZMM(v0) && isM128kz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x11)
            m.mrsd(lcode(v[0]), addr(v[1]), 16)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVUSDB")
    }
    return p
}

// VPMOVUSDW performs "Down Convert Packed Doubleword Values to Word Values with Unsigned Saturation".
//
// Mnemonic        : VPMOVUSDW
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPMOVUSDW xmm, xmm{k}{z}
//    * VPMOVUSDW xmm, m64{k}{z}
//    * VPMOVUSDW ymm, xmm{k}{z}
//    * VPMOVUSDW ymm, m128{k}{z}
//    * VPMOVUSDW zmm, ymm{k}{z}
//    * VPMOVUSDW zmm, m256{k}{z}
//
func (self *Program) VPMOVUSDW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVUSDW xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x13)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVUSDW xmm, m64{k}{z}
    if isEVEXXMM(v0) && isM64kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x13)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    // VPMOVUSDW ymm, xmm{k}{z}
    if isEVEXYMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x13)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVUSDW ymm, m128{k}{z}
    if isEVEXYMM(v0) && isM128kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x13)
            m.mrsd(lcode(v[0]), addr(v[1]), 16)
        })
    }
    // VPMOVUSDW zmm, ymm{k}{z}
    if isZMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x13)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVUSDW zmm, m256{k}{z}
    if isZMM(v0) && isM256kz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x13)
            m.mrsd(lcode(v[0]), addr(v[1]), 32)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVUSDW")
    }
    return p
}

// VPMOVUSQB performs "Down Convert Packed Quadword Values to Byte Values with Unsigned Saturation".
//
// Mnemonic        : VPMOVUSQB
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPMOVUSQB xmm, xmm{k}{z}
//    * VPMOVUSQB xmm, m16{k}{z}
//    * VPMOVUSQB ymm, xmm{k}{z}
//    * VPMOVUSQB ymm, m32{k}{z}
//    * VPMOVUSQB zmm, xmm{k}{z}
//    * VPMOVUSQB zmm, m64{k}{z}
//
func (self *Program) VPMOVUSQB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVUSQB xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x12)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVUSQB xmm, m16{k}{z}
    if isEVEXXMM(v0) && isM16kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x12)
            m.mrsd(lcode(v[0]), addr(v[1]), 2)
        })
    }
    // VPMOVUSQB ymm, xmm{k}{z}
    if isEVEXYMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x12)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVUSQB ymm, m32{k}{z}
    if isEVEXYMM(v0) && isM32kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x12)
            m.mrsd(lcode(v[0]), addr(v[1]), 4)
        })
    }
    // VPMOVUSQB zmm, xmm{k}{z}
    if isZMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x12)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVUSQB zmm, m64{k}{z}
    if isZMM(v0) && isM64kz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x12)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVUSQB")
    }
    return p
}

// VPMOVUSQD performs "Down Convert Packed Quadword Values to Doubleword Values with Unsigned Saturation".
//
// Mnemonic        : VPMOVUSQD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPMOVUSQD xmm, xmm{k}{z}
//    * VPMOVUSQD xmm, m64{k}{z}
//    * VPMOVUSQD ymm, xmm{k}{z}
//    * VPMOVUSQD ymm, m128{k}{z}
//    * VPMOVUSQD zmm, ymm{k}{z}
//    * VPMOVUSQD zmm, m256{k}{z}
//
func (self *Program) VPMOVUSQD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVUSQD xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x15)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVUSQD xmm, m64{k}{z}
    if isEVEXXMM(v0) && isM64kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x15)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    // VPMOVUSQD ymm, xmm{k}{z}
    if isEVEXYMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x15)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVUSQD ymm, m128{k}{z}
    if isEVEXYMM(v0) && isM128kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x15)
            m.mrsd(lcode(v[0]), addr(v[1]), 16)
        })
    }
    // VPMOVUSQD zmm, ymm{k}{z}
    if isZMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x15)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVUSQD zmm, m256{k}{z}
    if isZMM(v0) && isM256kz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x15)
            m.mrsd(lcode(v[0]), addr(v[1]), 32)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVUSQD")
    }
    return p
}

// VPMOVUSQW performs "Down Convert Packed Quadword Values to Word Values with Unsigned Saturation".
//
// Mnemonic        : VPMOVUSQW
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPMOVUSQW xmm, xmm{k}{z}
//    * VPMOVUSQW xmm, m32{k}{z}
//    * VPMOVUSQW ymm, xmm{k}{z}
//    * VPMOVUSQW ymm, m64{k}{z}
//    * VPMOVUSQW zmm, xmm{k}{z}
//    * VPMOVUSQW zmm, m128{k}{z}
//
func (self *Program) VPMOVUSQW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVUSQW xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x14)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVUSQW xmm, m32{k}{z}
    if isEVEXXMM(v0) && isM32kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x14)
            m.mrsd(lcode(v[0]), addr(v[1]), 4)
        })
    }
    // VPMOVUSQW ymm, xmm{k}{z}
    if isEVEXYMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x14)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVUSQW ymm, m64{k}{z}
    if isEVEXYMM(v0) && isM64kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x14)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    // VPMOVUSQW zmm, xmm{k}{z}
    if isZMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x14)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVUSQW zmm, m128{k}{z}
    if isZMM(v0) && isM128kz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x14)
            m.mrsd(lcode(v[0]), addr(v[1]), 16)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVUSQW")
    }
    return p
}

// VPMOVUSWB performs "Down Convert Packed Word Values to Byte Values with Unsigned Saturation".
//
// Mnemonic        : VPMOVUSWB
// ISA extensions  : AVX512BW, AVX512VL
// Supported forms : (6 forms)
//
//    * VPMOVUSWB xmm, xmm{k}{z}
//    * VPMOVUSWB xmm, m64{k}{z}
//    * VPMOVUSWB ymm, xmm{k}{z}
//    * VPMOVUSWB ymm, m128{k}{z}
//    * VPMOVUSWB zmm, ymm{k}{z}
//    * VPMOVUSWB zmm, m256{k}{z}
//
func (self *Program) VPMOVUSWB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVUSWB xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x10)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVUSWB xmm, m64{k}{z}
    if isEVEXXMM(v0) && isM64kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x10)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    // VPMOVUSWB ymm, xmm{k}{z}
    if isEVEXYMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x10)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVUSWB ymm, m128{k}{z}
    if isEVEXYMM(v0) && isM128kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x10)
            m.mrsd(lcode(v[0]), addr(v[1]), 16)
        })
    }
    // VPMOVUSWB zmm, ymm{k}{z}
    if isZMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x10)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVUSWB zmm, m256{k}{z}
    if isZMM(v0) && isM256kz(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x10)
            m.mrsd(lcode(v[0]), addr(v[1]), 32)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVUSWB")
    }
    return p
}

// VPMOVW2M performs "Move Signs of Packed Word Integers to Mask Register".
//
// Mnemonic        : VPMOVW2M
// ISA extensions  : AVX512BW, AVX512VL
// Supported forms : (3 forms)
//
//    * VPMOVW2M xmm, k
//    * VPMOVW2M ymm, k
//    * VPMOVW2M zmm, k
//
func (self *Program) VPMOVW2M(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVW2M xmm, k
    if isEVEXXMM(v0) && isK(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfe)
            m.emit(0x08)
            m.emit(0x29)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVW2M ymm, k
    if isEVEXYMM(v0) && isK(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfe)
            m.emit(0x28)
            m.emit(0x29)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVW2M zmm, k
    if isZMM(v0) && isK(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfe)
            m.emit(0x48)
            m.emit(0x29)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVW2M")
    }
    return p
}

// VPMOVWB performs "Down Convert Packed Word Values to Byte Values with Truncation".
//
// Mnemonic        : VPMOVWB
// ISA extensions  : AVX512BW, AVX512VL
// Supported forms : (6 forms)
//
//    * VPMOVWB xmm, xmm{k}{z}
//    * VPMOVWB xmm, m64{k}{z}
//    * VPMOVWB ymm, xmm{k}{z}
//    * VPMOVWB ymm, m128{k}{z}
//    * VPMOVWB zmm, ymm{k}{z}
//    * VPMOVWB zmm, m256{k}{z}
//
func (self *Program) VPMOVWB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVWB xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x30)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVWB xmm, m64{k}{z}
    if isEVEXXMM(v0) && isM64kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x30)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    // VPMOVWB ymm, xmm{k}{z}
    if isEVEXYMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x30)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVWB ymm, m128{k}{z}
    if isEVEXYMM(v0) && isM128kz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x30)
            m.mrsd(lcode(v[0]), addr(v[1]), 16)
        })
    }
    // VPMOVWB zmm, ymm{k}{z}
    if isZMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[0]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[0]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x30)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // VPMOVWB zmm, m256{k}{z}
    if isZMM(v0) && isM256kz(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x30)
            m.mrsd(lcode(v[0]), addr(v[1]), 32)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVWB")
    }
    return p
}

// VPMOVZXBD performs "Move Packed Byte Integers to Doubleword Integers with Zero Extension".
//
// Mnemonic        : VPMOVZXBD
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPMOVZXBD xmm, xmm{k}{z}
//    * VPMOVZXBD xmm, ymm{k}{z}
//    * VPMOVZXBD xmm, zmm{k}{z}
//    * VPMOVZXBD m32, xmm{k}{z}
//    * VPMOVZXBD m64, ymm{k}{z}
//    * VPMOVZXBD m128, zmm{k}{z}
//    * VPMOVZXBD xmm, xmm
//    * VPMOVZXBD m32, xmm
//    * VPMOVZXBD xmm, ymm
//    * VPMOVZXBD m64, ymm
//
func (self *Program) VPMOVZXBD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVZXBD xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x31)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVZXBD xmm, ymm{k}{z}
    if isEVEXXMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x31)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVZXBD xmm, zmm{k}{z}
    if isEVEXXMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x31)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVZXBD m32, xmm{k}{z}
    if isM32(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x31)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VPMOVZXBD m64, ymm{k}{z}
    if isM64(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x31)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VPMOVZXBD m128, zmm{k}{z}
    if isM128(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x31)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VPMOVZXBD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79)
            m.emit(0x31)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVZXBD m32, xmm
    if isM32(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x31)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VPMOVZXBD xmm, ymm
    if isXMM(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d)
            m.emit(0x31)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVZXBD m64, ymm
    if isM64(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x31)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVZXBD")
    }
    return p
}

// VPMOVZXBQ performs "Move Packed Byte Integers to Quadword Integers with Zero Extension".
//
// Mnemonic        : VPMOVZXBQ
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPMOVZXBQ xmm, xmm{k}{z}
//    * VPMOVZXBQ xmm, ymm{k}{z}
//    * VPMOVZXBQ xmm, zmm{k}{z}
//    * VPMOVZXBQ m16, xmm{k}{z}
//    * VPMOVZXBQ m32, ymm{k}{z}
//    * VPMOVZXBQ m64, zmm{k}{z}
//    * VPMOVZXBQ xmm, xmm
//    * VPMOVZXBQ m16, xmm
//    * VPMOVZXBQ xmm, ymm
//    * VPMOVZXBQ m32, ymm
//
func (self *Program) VPMOVZXBQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVZXBQ xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x32)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVZXBQ xmm, ymm{k}{z}
    if isEVEXXMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x32)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVZXBQ xmm, zmm{k}{z}
    if isEVEXXMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x32)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVZXBQ m16, xmm{k}{z}
    if isM16(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x32)
            m.mrsd(lcode(v[1]), addr(v[0]), 2)
        })
    }
    // VPMOVZXBQ m32, ymm{k}{z}
    if isM32(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x32)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VPMOVZXBQ m64, zmm{k}{z}
    if isM64(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x32)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VPMOVZXBQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79)
            m.emit(0x32)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVZXBQ m16, xmm
    if isM16(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x32)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VPMOVZXBQ xmm, ymm
    if isXMM(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d)
            m.emit(0x32)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVZXBQ m32, ymm
    if isM32(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x32)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVZXBQ")
    }
    return p
}

// VPMOVZXBW performs "Move Packed Byte Integers to Word Integers with Zero Extension".
//
// Mnemonic        : VPMOVZXBW
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPMOVZXBW xmm, xmm{k}{z}
//    * VPMOVZXBW xmm, ymm{k}{z}
//    * VPMOVZXBW ymm, zmm{k}{z}
//    * VPMOVZXBW m64, xmm{k}{z}
//    * VPMOVZXBW m128, ymm{k}{z}
//    * VPMOVZXBW m256, zmm{k}{z}
//    * VPMOVZXBW xmm, xmm
//    * VPMOVZXBW m64, xmm
//    * VPMOVZXBW xmm, ymm
//    * VPMOVZXBW m128, ymm
//
func (self *Program) VPMOVZXBW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVZXBW xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x30)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVZXBW xmm, ymm{k}{z}
    if isEVEXXMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x30)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVZXBW ymm, zmm{k}{z}
    if isEVEXYMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x30)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVZXBW m64, xmm{k}{z}
    if isM64(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x30)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VPMOVZXBW m128, ymm{k}{z}
    if isM128(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x30)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VPMOVZXBW m256, zmm{k}{z}
    if isM256(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x30)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VPMOVZXBW xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79)
            m.emit(0x30)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVZXBW m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x30)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VPMOVZXBW xmm, ymm
    if isXMM(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d)
            m.emit(0x30)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVZXBW m128, ymm
    if isM128(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x30)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVZXBW")
    }
    return p
}

// VPMOVZXDQ performs "Move Packed Doubleword Integers to Quadword Integers with Zero Extension".
//
// Mnemonic        : VPMOVZXDQ
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPMOVZXDQ xmm, xmm{k}{z}
//    * VPMOVZXDQ xmm, ymm{k}{z}
//    * VPMOVZXDQ ymm, zmm{k}{z}
//    * VPMOVZXDQ m64, xmm{k}{z}
//    * VPMOVZXDQ m128, ymm{k}{z}
//    * VPMOVZXDQ m256, zmm{k}{z}
//    * VPMOVZXDQ xmm, xmm
//    * VPMOVZXDQ m64, xmm
//    * VPMOVZXDQ xmm, ymm
//    * VPMOVZXDQ m128, ymm
//
func (self *Program) VPMOVZXDQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVZXDQ xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x35)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVZXDQ xmm, ymm{k}{z}
    if isEVEXXMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x35)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVZXDQ ymm, zmm{k}{z}
    if isEVEXYMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x35)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVZXDQ m64, xmm{k}{z}
    if isM64(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x35)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VPMOVZXDQ m128, ymm{k}{z}
    if isM128(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x35)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VPMOVZXDQ m256, zmm{k}{z}
    if isM256(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x35)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VPMOVZXDQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79)
            m.emit(0x35)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVZXDQ m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x35)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VPMOVZXDQ xmm, ymm
    if isXMM(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d)
            m.emit(0x35)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVZXDQ m128, ymm
    if isM128(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x35)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVZXDQ")
    }
    return p
}

// VPMOVZXWD performs "Move Packed Word Integers to Doubleword Integers with Zero Extension".
//
// Mnemonic        : VPMOVZXWD
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPMOVZXWD xmm, xmm{k}{z}
//    * VPMOVZXWD xmm, ymm{k}{z}
//    * VPMOVZXWD ymm, zmm{k}{z}
//    * VPMOVZXWD m64, xmm{k}{z}
//    * VPMOVZXWD m128, ymm{k}{z}
//    * VPMOVZXWD m256, zmm{k}{z}
//    * VPMOVZXWD xmm, xmm
//    * VPMOVZXWD m64, xmm
//    * VPMOVZXWD xmm, ymm
//    * VPMOVZXWD m128, ymm
//
func (self *Program) VPMOVZXWD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVZXWD xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x33)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVZXWD xmm, ymm{k}{z}
    if isEVEXXMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x33)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVZXWD ymm, zmm{k}{z}
    if isEVEXYMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x33)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVZXWD m64, xmm{k}{z}
    if isM64(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x33)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VPMOVZXWD m128, ymm{k}{z}
    if isM128(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x33)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VPMOVZXWD m256, zmm{k}{z}
    if isM256(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x33)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VPMOVZXWD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79)
            m.emit(0x33)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVZXWD m64, xmm
    if isM64(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x33)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VPMOVZXWD xmm, ymm
    if isXMM(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d)
            m.emit(0x33)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVZXWD m128, ymm
    if isM128(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x33)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVZXWD")
    }
    return p
}

// VPMOVZXWQ performs "Move Packed Word Integers to Quadword Integers with Zero Extension".
//
// Mnemonic        : VPMOVZXWQ
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPMOVZXWQ xmm, xmm{k}{z}
//    * VPMOVZXWQ xmm, ymm{k}{z}
//    * VPMOVZXWQ xmm, zmm{k}{z}
//    * VPMOVZXWQ m32, xmm{k}{z}
//    * VPMOVZXWQ m64, ymm{k}{z}
//    * VPMOVZXWQ m128, zmm{k}{z}
//    * VPMOVZXWQ xmm, xmm
//    * VPMOVZXWQ m32, xmm
//    * VPMOVZXWQ xmm, ymm
//    * VPMOVZXWQ m64, ymm
//
func (self *Program) VPMOVZXWQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPMOVZXWQ xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x34)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVZXWQ xmm, ymm{k}{z}
    if isEVEXXMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x34)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVZXWQ xmm, zmm{k}{z}
    if isEVEXXMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x34)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVZXWQ m32, xmm{k}{z}
    if isM32(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x34)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VPMOVZXWQ m64, ymm{k}{z}
    if isM64(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x34)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VPMOVZXWQ m128, zmm{k}{z}
    if isM128(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), 0)
            m.emit(0x34)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VPMOVZXWQ xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79)
            m.emit(0x34)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVZXWQ m32, xmm
    if isM32(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x34)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VPMOVZXWQ xmm, ymm
    if isXMM(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d)
            m.emit(0x34)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPMOVZXWQ m64, ymm
    if isM64(v0) && isYMM(v1) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x34)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMOVZXWQ")
    }
    return p
}

// VPMULDQ performs "Multiply Packed Signed Doubleword Integers and Store Quadword Result".
//
// Mnemonic        : VPMULDQ
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPMULDQ m128/m64bcst, xmm, xmm{k}{z}
//    * VPMULDQ xmm, xmm, xmm{k}{z}
//    * VPMULDQ m256/m64bcst, ymm, ymm{k}{z}
//    * VPMULDQ ymm, ymm, ymm{k}{z}
//    * VPMULDQ m512/m64bcst, zmm, zmm{k}{z}
//    * VPMULDQ zmm, zmm, zmm{k}{z}
//    * VPMULDQ xmm, xmm, xmm
//    * VPMULDQ m128, xmm, xmm
//    * VPMULDQ ymm, ymm, ymm
//    * VPMULDQ m256, ymm, ymm
//
func (self *Program) VPMULDQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPMULDQ m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x28)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPMULDQ xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x28)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULDQ m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x28)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPMULDQ ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x28)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULDQ m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x28)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPMULDQ zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x28)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULDQ xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x28)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULDQ m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x28)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPMULDQ ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x28)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULDQ m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x28)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMULDQ")
    }
    return p
}

// VPMULHRSW performs "Packed Multiply Signed Word Integers and Store High Result with Round and Scale".
//
// Mnemonic        : VPMULHRSW
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPMULHRSW xmm, xmm, xmm{k}{z}
//    * VPMULHRSW m128, xmm, xmm{k}{z}
//    * VPMULHRSW ymm, ymm, ymm{k}{z}
//    * VPMULHRSW m256, ymm, ymm{k}{z}
//    * VPMULHRSW zmm, zmm, zmm{k}{z}
//    * VPMULHRSW m512, zmm, zmm{k}{z}
//    * VPMULHRSW xmm, xmm, xmm
//    * VPMULHRSW m128, xmm, xmm
//    * VPMULHRSW ymm, ymm, ymm
//    * VPMULHRSW m256, ymm, ymm
//
func (self *Program) VPMULHRSW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPMULHRSW xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x0b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULHRSW m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x0b)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPMULHRSW ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x0b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULHRSW m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x0b)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPMULHRSW zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x0b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULHRSW m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x0b)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPMULHRSW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x0b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULHRSW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x0b)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPMULHRSW ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x0b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULHRSW m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x0b)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMULHRSW")
    }
    return p
}

// VPMULHUW performs "Multiply Packed Unsigned Word Integers and Store High Result".
//
// Mnemonic        : VPMULHUW
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPMULHUW xmm, xmm, xmm{k}{z}
//    * VPMULHUW m128, xmm, xmm{k}{z}
//    * VPMULHUW ymm, ymm, ymm{k}{z}
//    * VPMULHUW m256, ymm, ymm{k}{z}
//    * VPMULHUW zmm, zmm, zmm{k}{z}
//    * VPMULHUW m512, zmm, zmm{k}{z}
//    * VPMULHUW xmm, xmm, xmm
//    * VPMULHUW m128, xmm, xmm
//    * VPMULHUW ymm, ymm, ymm
//    * VPMULHUW m256, ymm, ymm
//
func (self *Program) VPMULHUW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPMULHUW xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xe4)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULHUW m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xe4)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPMULHUW ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xe4)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULHUW m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xe4)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPMULHUW zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xe4)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULHUW m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xe4)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPMULHUW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xe4)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULHUW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xe4)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPMULHUW ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xe4)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULHUW m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xe4)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMULHUW")
    }
    return p
}

// VPMULHW performs "Multiply Packed Signed Word Integers and Store High Result".
//
// Mnemonic        : VPMULHW
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPMULHW xmm, xmm, xmm{k}{z}
//    * VPMULHW m128, xmm, xmm{k}{z}
//    * VPMULHW ymm, ymm, ymm{k}{z}
//    * VPMULHW m256, ymm, ymm{k}{z}
//    * VPMULHW zmm, zmm, zmm{k}{z}
//    * VPMULHW m512, zmm, zmm{k}{z}
//    * VPMULHW xmm, xmm, xmm
//    * VPMULHW m128, xmm, xmm
//    * VPMULHW ymm, ymm, ymm
//    * VPMULHW m256, ymm, ymm
//
func (self *Program) VPMULHW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPMULHW xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xe5)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULHW m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xe5)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPMULHW ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xe5)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULHW m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xe5)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPMULHW zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xe5)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULHW m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xe5)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPMULHW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xe5)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULHW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xe5)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPMULHW ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xe5)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULHW m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xe5)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMULHW")
    }
    return p
}

// VPMULLD performs "Multiply Packed Signed Doubleword Integers and Store Low Result".
//
// Mnemonic        : VPMULLD
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPMULLD m128/m32bcst, xmm, xmm{k}{z}
//    * VPMULLD xmm, xmm, xmm{k}{z}
//    * VPMULLD m256/m32bcst, ymm, ymm{k}{z}
//    * VPMULLD ymm, ymm, ymm{k}{z}
//    * VPMULLD m512/m32bcst, zmm, zmm{k}{z}
//    * VPMULLD zmm, zmm, zmm{k}{z}
//    * VPMULLD xmm, xmm, xmm
//    * VPMULLD m128, xmm, xmm
//    * VPMULLD ymm, ymm, ymm
//    * VPMULLD m256, ymm, ymm
//
func (self *Program) VPMULLD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPMULLD m128/m32bcst, xmm, xmm{k}{z}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x40)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPMULLD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x40)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULLD m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x40)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPMULLD ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x40)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULLD m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x40)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPMULLD zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x40)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULLD xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x40)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULLD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x40)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPMULLD ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x40)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULLD m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x40)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMULLD")
    }
    return p
}

// VPMULLQ performs "Multiply Packed Signed Quadword Integers and Store Low Result".
//
// Mnemonic        : VPMULLQ
// ISA extensions  : AVX512DQ, AVX512VL
// Supported forms : (6 forms)
//
//    * VPMULLQ m128/m64bcst, xmm, xmm{k}{z}
//    * VPMULLQ xmm, xmm, xmm{k}{z}
//    * VPMULLQ m256/m64bcst, ymm, ymm{k}{z}
//    * VPMULLQ ymm, ymm, ymm{k}{z}
//    * VPMULLQ m512/m64bcst, zmm, zmm{k}{z}
//    * VPMULLQ zmm, zmm, zmm{k}{z}
//
func (self *Program) VPMULLQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPMULLQ m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x40)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPMULLQ xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x40)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULLQ m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x40)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPMULLQ ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x40)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULLQ m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x40)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPMULLQ zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x40)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMULLQ")
    }
    return p
}

// VPMULLW performs "Multiply Packed Signed Word Integers and Store Low Result".
//
// Mnemonic        : VPMULLW
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPMULLW xmm, xmm, xmm{k}{z}
//    * VPMULLW m128, xmm, xmm{k}{z}
//    * VPMULLW ymm, ymm, ymm{k}{z}
//    * VPMULLW m256, ymm, ymm{k}{z}
//    * VPMULLW zmm, zmm, zmm{k}{z}
//    * VPMULLW m512, zmm, zmm{k}{z}
//    * VPMULLW xmm, xmm, xmm
//    * VPMULLW m128, xmm, xmm
//    * VPMULLW ymm, ymm, ymm
//    * VPMULLW m256, ymm, ymm
//
func (self *Program) VPMULLW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPMULLW xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xd5)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULLW m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xd5)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPMULLW ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xd5)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULLW m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xd5)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPMULLW zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xd5)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULLW m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xd5)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPMULLW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xd5)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULLW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xd5)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPMULLW ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xd5)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULLW m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xd5)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMULLW")
    }
    return p
}

// VPMULTISHIFTQB performs "Select Packed Unaligned Bytes from Quadword Sources".
//
// Mnemonic        : VPMULTISHIFTQB
// ISA extensions  : AVX512VBMI, AVX512VL
// Supported forms : (6 forms)
//
//    * VPMULTISHIFTQB m128/m64bcst, xmm, xmm{k}{z}
//    * VPMULTISHIFTQB xmm, xmm, xmm{k}{z}
//    * VPMULTISHIFTQB m256/m64bcst, ymm, ymm{k}{z}
//    * VPMULTISHIFTQB ymm, ymm, ymm{k}{z}
//    * VPMULTISHIFTQB m512/m64bcst, zmm, zmm{k}{z}
//    * VPMULTISHIFTQB zmm, zmm, zmm{k}{z}
//
func (self *Program) VPMULTISHIFTQB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPMULTISHIFTQB m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VBMI | ISA_AVX512VL)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x83)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPMULTISHIFTQB xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VBMI | ISA_AVX512VL)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x83)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULTISHIFTQB m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VBMI | ISA_AVX512VL)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x83)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPMULTISHIFTQB ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VBMI | ISA_AVX512VL)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x83)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULTISHIFTQB m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512VBMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x83)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPMULTISHIFTQB zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512VBMI)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x83)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMULTISHIFTQB")
    }
    return p
}

// VPMULUDQ performs "Multiply Packed Unsigned Doubleword Integers".
//
// Mnemonic        : VPMULUDQ
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPMULUDQ m128/m64bcst, xmm, xmm{k}{z}
//    * VPMULUDQ xmm, xmm, xmm{k}{z}
//    * VPMULUDQ m256/m64bcst, ymm, ymm{k}{z}
//    * VPMULUDQ ymm, ymm, ymm{k}{z}
//    * VPMULUDQ m512/m64bcst, zmm, zmm{k}{z}
//    * VPMULUDQ zmm, zmm, zmm{k}{z}
//    * VPMULUDQ xmm, xmm, xmm
//    * VPMULUDQ m128, xmm, xmm
//    * VPMULUDQ ymm, ymm, ymm
//    * VPMULUDQ m256, ymm, ymm
//
func (self *Program) VPMULUDQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPMULUDQ m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xf4)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPMULUDQ xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xf4)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULUDQ m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xf4)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPMULUDQ ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xf4)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULUDQ m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xf4)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPMULUDQ zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xf4)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULUDQ xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xf4)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULUDQ m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xf4)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPMULUDQ ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xf4)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPMULUDQ m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xf4)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPMULUDQ")
    }
    return p
}

// VPOPCNTD performs "Packed Population Count for Doubleword Integers".
//
// Mnemonic        : VPOPCNTD
// ISA extensions  : AVX512VPOPCNTDQ
// Supported forms : (2 forms)
//
//    * VPOPCNTD m512/m32bcst, zmm{k}{z}
//    * VPOPCNTD zmm, zmm{k}{z}
//
func (self *Program) VPOPCNTD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPOPCNTD m512/m32bcst, zmm{k}{z}
    if isM512M32bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512VPOPCNTDQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x55)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VPOPCNTD zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512VPOPCNTDQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x55)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPOPCNTD")
    }
    return p
}

// VPOPCNTQ performs "Packed Population Count for Quadword Integers".
//
// Mnemonic        : VPOPCNTQ
// ISA extensions  : AVX512VPOPCNTDQ
// Supported forms : (2 forms)
//
//    * VPOPCNTQ m512/m64bcst, zmm{k}{z}
//    * VPOPCNTQ zmm, zmm{k}{z}
//
func (self *Program) VPOPCNTQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPOPCNTQ m512/m64bcst, zmm{k}{z}
    if isM512M64bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512VPOPCNTDQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x55)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VPOPCNTQ zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512VPOPCNTDQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x55)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPOPCNTQ")
    }
    return p
}

// VPOR performs "Packed Bitwise Logical OR".
//
// Mnemonic        : VPOR
// ISA extensions  : AVX, AVX2
// Supported forms : (4 forms)
//
//    * VPOR xmm, xmm, xmm
//    * VPOR m128, xmm, xmm
//    * VPOR ymm, ymm, ymm
//    * VPOR m256, ymm, ymm
//
func (self *Program) VPOR(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPOR xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xeb)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPOR m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xeb)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPOR ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xeb)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPOR m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xeb)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPOR")
    }
    return p
}

// VPORD performs "Bitwise Logical OR of Packed Doubleword Integers".
//
// Mnemonic        : VPORD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPORD m128/m32bcst, xmm, xmm{k}{z}
//    * VPORD xmm, xmm, xmm{k}{z}
//    * VPORD m256/m32bcst, ymm, ymm{k}{z}
//    * VPORD ymm, ymm, ymm{k}{z}
//    * VPORD m512/m32bcst, zmm, zmm{k}{z}
//    * VPORD zmm, zmm, zmm{k}{z}
//
func (self *Program) VPORD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPORD m128/m32bcst, xmm, xmm{k}{z}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xeb)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPORD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xeb)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPORD m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xeb)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPORD ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xeb)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPORD m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xeb)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPORD zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xeb)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPORD")
    }
    return p
}

// VPORQ performs "Bitwise Logical OR of Packed Quadword Integers".
//
// Mnemonic        : VPORQ
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPORQ m128/m64bcst, xmm, xmm{k}{z}
//    * VPORQ xmm, xmm, xmm{k}{z}
//    * VPORQ m256/m64bcst, ymm, ymm{k}{z}
//    * VPORQ ymm, ymm, ymm{k}{z}
//    * VPORQ m512/m64bcst, zmm, zmm{k}{z}
//    * VPORQ zmm, zmm, zmm{k}{z}
//
func (self *Program) VPORQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPORQ m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xeb)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPORQ xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xeb)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPORQ m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xeb)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPORQ ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xeb)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPORQ m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xeb)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPORQ zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xeb)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPORQ")
    }
    return p
}

// VPPERM performs "Packed Permute Bytes".
//
// Mnemonic        : VPPERM
// ISA extensions  : XOP
// Supported forms : (3 forms)
//
//    * VPPERM xmm, xmm, xmm, xmm
//    * VPPERM m128, xmm, xmm, xmm
//    * VPPERM xmm, m128, xmm, xmm
//
func (self *Program) VPPERM(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPPERM xmm, xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe8 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[2]) << 3))
            m.emit(0xa3)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.emit(hlcode(v[0]) << 4)
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe8 ^ (hcode(v[3]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf8 ^ (hlcode(v[2]) << 3))
            m.emit(0xa3)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[0]))
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VPPERM m128, xmm, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1000, 0x80, hcode(v[3]), addr(v[0]), hlcode(v[2]))
            m.emit(0xa3)
            m.mrsd(lcode(v[3]), addr(v[0]), 1)
            m.emit(hlcode(v[1]) << 4)
        })
    }
    // VPPERM xmm, m128, xmm, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1000, 0x00, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0xa3)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.emit(hlcode(v[0]) << 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPPERM")
    }
    return p
}

// VPROLD performs "Rotate Packed Doubleword Left".
//
// Mnemonic        : VPROLD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPROLD imm8, m128/m32bcst, xmm{k}{z}
//    * VPROLD imm8, m256/m32bcst, ymm{k}{z}
//    * VPROLD imm8, m512/m32bcst, zmm{k}{z}
//    * VPROLD imm8, xmm, xmm{k}{z}
//    * VPROLD imm8, ymm, ymm{k}{z}
//    * VPROLD imm8, zmm, zmm{k}{z}
//
func (self *Program) VPROLD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPROLD imm8, m128/m32bcst, xmm{k}{z}
    if isImm8(v0) && isM128M32bcst(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x72)
            m.mrsd(1, addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPROLD imm8, m256/m32bcst, ymm{k}{z}
    if isImm8(v0) && isM256M32bcst(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x72)
            m.mrsd(1, addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPROLD imm8, m512/m32bcst, zmm{k}{z}
    if isImm8(v0) && isM512M32bcst(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x72)
            m.mrsd(1, addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPROLD imm8, xmm, xmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x72)
            m.emit(0xc8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPROLD imm8, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x72)
            m.emit(0xc8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPROLD imm8, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x72)
            m.emit(0xc8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPROLD")
    }
    return p
}

// VPROLQ performs "Rotate Packed Quadword Left".
//
// Mnemonic        : VPROLQ
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPROLQ imm8, m128/m64bcst, xmm{k}{z}
//    * VPROLQ imm8, m256/m64bcst, ymm{k}{z}
//    * VPROLQ imm8, m512/m64bcst, zmm{k}{z}
//    * VPROLQ imm8, xmm, xmm{k}{z}
//    * VPROLQ imm8, ymm, ymm{k}{z}
//    * VPROLQ imm8, zmm, zmm{k}{z}
//
func (self *Program) VPROLQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPROLQ imm8, m128/m64bcst, xmm{k}{z}
    if isImm8(v0) && isM128M64bcst(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x72)
            m.mrsd(1, addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPROLQ imm8, m256/m64bcst, ymm{k}{z}
    if isImm8(v0) && isM256M64bcst(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x72)
            m.mrsd(1, addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPROLQ imm8, m512/m64bcst, zmm{k}{z}
    if isImm8(v0) && isM512M64bcst(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x72)
            m.mrsd(1, addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPROLQ imm8, xmm, xmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x72)
            m.emit(0xc8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPROLQ imm8, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x72)
            m.emit(0xc8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPROLQ imm8, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x72)
            m.emit(0xc8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPROLQ")
    }
    return p
}

// VPROLVD performs "Variable Rotate Packed Doubleword Left".
//
// Mnemonic        : VPROLVD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPROLVD m128/m32bcst, xmm, xmm{k}{z}
//    * VPROLVD xmm, xmm, xmm{k}{z}
//    * VPROLVD m256/m32bcst, ymm, ymm{k}{z}
//    * VPROLVD ymm, ymm, ymm{k}{z}
//    * VPROLVD m512/m32bcst, zmm, zmm{k}{z}
//    * VPROLVD zmm, zmm, zmm{k}{z}
//
func (self *Program) VPROLVD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPROLVD m128/m32bcst, xmm, xmm{k}{z}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x15)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPROLVD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x15)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPROLVD m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x15)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPROLVD ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x15)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPROLVD m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x15)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPROLVD zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x15)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPROLVD")
    }
    return p
}

// VPROLVQ performs "Variable Rotate Packed Quadword Left".
//
// Mnemonic        : VPROLVQ
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPROLVQ m128/m64bcst, xmm, xmm{k}{z}
//    * VPROLVQ xmm, xmm, xmm{k}{z}
//    * VPROLVQ m256/m64bcst, ymm, ymm{k}{z}
//    * VPROLVQ ymm, ymm, ymm{k}{z}
//    * VPROLVQ m512/m64bcst, zmm, zmm{k}{z}
//    * VPROLVQ zmm, zmm, zmm{k}{z}
//
func (self *Program) VPROLVQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPROLVQ m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x15)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPROLVQ xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x15)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPROLVQ m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x15)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPROLVQ ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x15)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPROLVQ m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x15)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPROLVQ zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x15)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPROLVQ")
    }
    return p
}

// VPRORD performs "Rotate Packed Doubleword Right".
//
// Mnemonic        : VPRORD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPRORD imm8, m128/m32bcst, xmm{k}{z}
//    * VPRORD imm8, m256/m32bcst, ymm{k}{z}
//    * VPRORD imm8, m512/m32bcst, zmm{k}{z}
//    * VPRORD imm8, xmm, xmm{k}{z}
//    * VPRORD imm8, ymm, ymm{k}{z}
//    * VPRORD imm8, zmm, zmm{k}{z}
//
func (self *Program) VPRORD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPRORD imm8, m128/m32bcst, xmm{k}{z}
    if isImm8(v0) && isM128M32bcst(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x72)
            m.mrsd(0, addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPRORD imm8, m256/m32bcst, ymm{k}{z}
    if isImm8(v0) && isM256M32bcst(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x72)
            m.mrsd(0, addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPRORD imm8, m512/m32bcst, zmm{k}{z}
    if isImm8(v0) && isM512M32bcst(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x72)
            m.mrsd(0, addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPRORD imm8, xmm, xmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x72)
            m.emit(0xc0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPRORD imm8, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x72)
            m.emit(0xc0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPRORD imm8, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x72)
            m.emit(0xc0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPRORD")
    }
    return p
}

// VPRORQ performs "Rotate Packed Quadword Right".
//
// Mnemonic        : VPRORQ
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPRORQ imm8, m128/m64bcst, xmm{k}{z}
//    * VPRORQ imm8, m256/m64bcst, ymm{k}{z}
//    * VPRORQ imm8, m512/m64bcst, zmm{k}{z}
//    * VPRORQ imm8, xmm, xmm{k}{z}
//    * VPRORQ imm8, ymm, ymm{k}{z}
//    * VPRORQ imm8, zmm, zmm{k}{z}
//
func (self *Program) VPRORQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPRORQ imm8, m128/m64bcst, xmm{k}{z}
    if isImm8(v0) && isM128M64bcst(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x72)
            m.mrsd(0, addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPRORQ imm8, m256/m64bcst, ymm{k}{z}
    if isImm8(v0) && isM256M64bcst(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x72)
            m.mrsd(0, addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPRORQ imm8, m512/m64bcst, zmm{k}{z}
    if isImm8(v0) && isM512M64bcst(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x72)
            m.mrsd(0, addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPRORQ imm8, xmm, xmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x72)
            m.emit(0xc0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPRORQ imm8, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x72)
            m.emit(0xc0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPRORQ imm8, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x72)
            m.emit(0xc0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPRORQ")
    }
    return p
}

// VPRORVD performs "Variable Rotate Packed Doubleword Right".
//
// Mnemonic        : VPRORVD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPRORVD m128/m32bcst, xmm, xmm{k}{z}
//    * VPRORVD xmm, xmm, xmm{k}{z}
//    * VPRORVD m256/m32bcst, ymm, ymm{k}{z}
//    * VPRORVD ymm, ymm, ymm{k}{z}
//    * VPRORVD m512/m32bcst, zmm, zmm{k}{z}
//    * VPRORVD zmm, zmm, zmm{k}{z}
//
func (self *Program) VPRORVD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPRORVD m128/m32bcst, xmm, xmm{k}{z}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x14)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPRORVD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x14)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPRORVD m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x14)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPRORVD ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x14)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPRORVD m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x14)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPRORVD zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x14)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPRORVD")
    }
    return p
}

// VPRORVQ performs "Variable Rotate Packed Quadword Right".
//
// Mnemonic        : VPRORVQ
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPRORVQ m128/m64bcst, xmm, xmm{k}{z}
//    * VPRORVQ xmm, xmm, xmm{k}{z}
//    * VPRORVQ m256/m64bcst, ymm, ymm{k}{z}
//    * VPRORVQ ymm, ymm, ymm{k}{z}
//    * VPRORVQ m512/m64bcst, zmm, zmm{k}{z}
//    * VPRORVQ zmm, zmm, zmm{k}{z}
//
func (self *Program) VPRORVQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPRORVQ m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x14)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPRORVQ xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x14)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPRORVQ m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x14)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPRORVQ ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x14)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPRORVQ m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x14)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPRORVQ zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x14)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPRORVQ")
    }
    return p
}

// VPROTB performs "Packed Rotate Bytes".
//
// Mnemonic        : VPROTB
// ISA extensions  : XOP
// Supported forms : (5 forms)
//
//    * VPROTB imm8, xmm, xmm
//    * VPROTB xmm, xmm, xmm
//    * VPROTB m128, xmm, xmm
//    * VPROTB imm8, m128, xmm
//    * VPROTB xmm, m128, xmm
//
func (self *Program) VPROTB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPROTB imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe8 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78)
            m.emit(0xc0)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPROTB xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[0]) << 3))
            m.emit(0x90)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf8 ^ (hlcode(v[1]) << 3))
            m.emit(0x90)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPROTB m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x80, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x90)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPROTB imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1000, 0x00, hcode(v[2]), addr(v[1]), 0)
            m.emit(0xc0)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPROTB xmm, m128, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0x90)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPROTB")
    }
    return p
}

// VPROTD performs "Packed Rotate Doublewords".
//
// Mnemonic        : VPROTD
// ISA extensions  : XOP
// Supported forms : (5 forms)
//
//    * VPROTD imm8, xmm, xmm
//    * VPROTD xmm, xmm, xmm
//    * VPROTD m128, xmm, xmm
//    * VPROTD imm8, m128, xmm
//    * VPROTD xmm, m128, xmm
//
func (self *Program) VPROTD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPROTD imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe8 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78)
            m.emit(0xc2)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPROTD xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[0]) << 3))
            m.emit(0x92)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf8 ^ (hlcode(v[1]) << 3))
            m.emit(0x92)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPROTD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x80, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x92)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPROTD imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1000, 0x00, hcode(v[2]), addr(v[1]), 0)
            m.emit(0xc2)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPROTD xmm, m128, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0x92)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPROTD")
    }
    return p
}

// VPROTQ performs "Packed Rotate Quadwords".
//
// Mnemonic        : VPROTQ
// ISA extensions  : XOP
// Supported forms : (5 forms)
//
//    * VPROTQ imm8, xmm, xmm
//    * VPROTQ xmm, xmm, xmm
//    * VPROTQ m128, xmm, xmm
//    * VPROTQ imm8, m128, xmm
//    * VPROTQ xmm, m128, xmm
//
func (self *Program) VPROTQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPROTQ imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe8 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78)
            m.emit(0xc3)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPROTQ xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[0]) << 3))
            m.emit(0x93)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf8 ^ (hlcode(v[1]) << 3))
            m.emit(0x93)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPROTQ m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x80, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x93)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPROTQ imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1000, 0x00, hcode(v[2]), addr(v[1]), 0)
            m.emit(0xc3)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPROTQ xmm, m128, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0x93)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPROTQ")
    }
    return p
}

// VPROTW performs "Packed Rotate Words".
//
// Mnemonic        : VPROTW
// ISA extensions  : XOP
// Supported forms : (5 forms)
//
//    * VPROTW imm8, xmm, xmm
//    * VPROTW xmm, xmm, xmm
//    * VPROTW m128, xmm, xmm
//    * VPROTW imm8, m128, xmm
//    * VPROTW xmm, m128, xmm
//
func (self *Program) VPROTW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPROTW imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe8 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78)
            m.emit(0xc1)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPROTW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[0]) << 3))
            m.emit(0x91)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf8 ^ (hlcode(v[1]) << 3))
            m.emit(0x91)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPROTW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x80, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x91)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPROTW imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1000, 0x00, hcode(v[2]), addr(v[1]), 0)
            m.emit(0xc1)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPROTW xmm, m128, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0x91)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPROTW")
    }
    return p
}

// VPSADBW performs "Compute Sum of Absolute Differences".
//
// Mnemonic        : VPSADBW
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPSADBW xmm, xmm, xmm
//    * VPSADBW xmm, xmm, xmm
//    * VPSADBW m128, xmm, xmm
//    * VPSADBW m128, xmm, xmm
//    * VPSADBW ymm, ymm, ymm
//    * VPSADBW ymm, ymm, ymm
//    * VPSADBW m256, ymm, ymm
//    * VPSADBW m256, ymm, ymm
//    * VPSADBW zmm, zmm, zmm
//    * VPSADBW m512, zmm, zmm
//
func (self *Program) VPSADBW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSADBW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xf6)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSADBW xmm, xmm, xmm
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | 0x00)
            m.emit(0xf6)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSADBW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xf6)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSADBW m128, xmm, xmm
    if isM128(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), 0, 0, 0)
            m.emit(0xf6)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSADBW ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xf6)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSADBW ymm, ymm, ymm
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isEVEXYMM(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | 0x20)
            m.emit(0xf6)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSADBW m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xf6)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSADBW m256, ymm, ymm
    if isM256(v0) && isEVEXYMM(v1) && isEVEXYMM(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), 0, 0, 0)
            m.emit(0xf6)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPSADBW zmm, zmm, zmm
    if isZMM(v0) && isZMM(v1) && isZMM(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | 0x40)
            m.emit(0xf6)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSADBW m512, zmm, zmm
    if isM512(v0) && isZMM(v1) && isZMM(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), 0, 0, 0)
            m.emit(0xf6)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSADBW")
    }
    return p
}

// VPSCATTERDD performs "Scatter Packed Doubleword Values with Signed Doubleword Indices".
//
// Mnemonic        : VPSCATTERDD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (3 forms)
//
//    * VPSCATTERDD xmm, vm32x{k}
//    * VPSCATTERDD ymm, vm32y{k}
//    * VPSCATTERDD zmm, vm32z{k}
//
func (self *Program) VPSCATTERDD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPSCATTERDD xmm, vm32x{k}
    if isEVEXXMM(v0) && isVMXk(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), 0, 0)
            m.emit(0xa0)
            m.mrsd(lcode(v[0]), addr(v[1]), 4)
        })
    }
    // VPSCATTERDD ymm, vm32y{k}
    if isEVEXYMM(v0) && isVMYk(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), 0, 0)
            m.emit(0xa0)
            m.mrsd(lcode(v[0]), addr(v[1]), 4)
        })
    }
    // VPSCATTERDD zmm, vm32z{k}
    if isZMM(v0) && isVMZk(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), 0, 0)
            m.emit(0xa0)
            m.mrsd(lcode(v[0]), addr(v[1]), 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSCATTERDD")
    }
    return p
}

// VPSCATTERDQ performs "Scatter Packed Quadword Values with Signed Doubleword Indices".
//
// Mnemonic        : VPSCATTERDQ
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (3 forms)
//
//    * VPSCATTERDQ xmm, vm32x{k}
//    * VPSCATTERDQ ymm, vm32x{k}
//    * VPSCATTERDQ zmm, vm32y{k}
//
func (self *Program) VPSCATTERDQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPSCATTERDQ xmm, vm32x{k}
    if isEVEXXMM(v0) && isVMXk(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), 0, 0)
            m.emit(0xa0)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    // VPSCATTERDQ ymm, vm32x{k}
    if isEVEXYMM(v0) && isVMXk(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), 0, 0)
            m.emit(0xa0)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    // VPSCATTERDQ zmm, vm32y{k}
    if isZMM(v0) && isVMYk(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), 0, 0)
            m.emit(0xa0)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSCATTERDQ")
    }
    return p
}

// VPSCATTERQD performs "Scatter Packed Doubleword Values with Signed Quadword Indices".
//
// Mnemonic        : VPSCATTERQD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (3 forms)
//
//    * VPSCATTERQD xmm, vm64x{k}
//    * VPSCATTERQD xmm, vm64y{k}
//    * VPSCATTERQD ymm, vm64z{k}
//
func (self *Program) VPSCATTERQD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPSCATTERQD xmm, vm64x{k}
    if isEVEXXMM(v0) && isVMXk(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), 0, 0)
            m.emit(0xa1)
            m.mrsd(lcode(v[0]), addr(v[1]), 4)
        })
    }
    // VPSCATTERQD xmm, vm64y{k}
    if isEVEXXMM(v0) && isVMYk(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), 0, 0)
            m.emit(0xa1)
            m.mrsd(lcode(v[0]), addr(v[1]), 4)
        })
    }
    // VPSCATTERQD ymm, vm64z{k}
    if isEVEXYMM(v0) && isVMZk(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), 0, 0)
            m.emit(0xa1)
            m.mrsd(lcode(v[0]), addr(v[1]), 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSCATTERQD")
    }
    return p
}

// VPSCATTERQQ performs "Scatter Packed Quadword Values with Signed Quadword Indices".
//
// Mnemonic        : VPSCATTERQQ
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (3 forms)
//
//    * VPSCATTERQQ xmm, vm64x{k}
//    * VPSCATTERQQ ymm, vm64y{k}
//    * VPSCATTERQQ zmm, vm64z{k}
//
func (self *Program) VPSCATTERQQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPSCATTERQQ xmm, vm64x{k}
    if isEVEXXMM(v0) && isVMXk(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), 0, 0)
            m.emit(0xa1)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    // VPSCATTERQQ ymm, vm64y{k}
    if isEVEXYMM(v0) && isVMYk(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), 0, 0)
            m.emit(0xa1)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    // VPSCATTERQQ zmm, vm64z{k}
    if isZMM(v0) && isVMZk(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), 0, 0)
            m.emit(0xa1)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSCATTERQQ")
    }
    return p
}

// VPSHAB performs "Packed Shift Arithmetic Bytes".
//
// Mnemonic        : VPSHAB
// ISA extensions  : XOP
// Supported forms : (3 forms)
//
//    * VPSHAB xmm, xmm, xmm
//    * VPSHAB m128, xmm, xmm
//    * VPSHAB xmm, m128, xmm
//
func (self *Program) VPSHAB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSHAB xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[0]) << 3))
            m.emit(0x98)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf8 ^ (hlcode(v[1]) << 3))
            m.emit(0x98)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSHAB m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x80, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x98)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSHAB xmm, m128, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0x98)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSHAB")
    }
    return p
}

// VPSHAD performs "Packed Shift Arithmetic Doublewords".
//
// Mnemonic        : VPSHAD
// ISA extensions  : XOP
// Supported forms : (3 forms)
//
//    * VPSHAD xmm, xmm, xmm
//    * VPSHAD m128, xmm, xmm
//    * VPSHAD xmm, m128, xmm
//
func (self *Program) VPSHAD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSHAD xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[0]) << 3))
            m.emit(0x9a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf8 ^ (hlcode(v[1]) << 3))
            m.emit(0x9a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSHAD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x80, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x9a)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSHAD xmm, m128, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0x9a)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSHAD")
    }
    return p
}

// VPSHAQ performs "Packed Shift Arithmetic Quadwords".
//
// Mnemonic        : VPSHAQ
// ISA extensions  : XOP
// Supported forms : (3 forms)
//
//    * VPSHAQ xmm, xmm, xmm
//    * VPSHAQ m128, xmm, xmm
//    * VPSHAQ xmm, m128, xmm
//
func (self *Program) VPSHAQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSHAQ xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[0]) << 3))
            m.emit(0x9b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf8 ^ (hlcode(v[1]) << 3))
            m.emit(0x9b)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSHAQ m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x80, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x9b)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSHAQ xmm, m128, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0x9b)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSHAQ")
    }
    return p
}

// VPSHAW performs "Packed Shift Arithmetic Words".
//
// Mnemonic        : VPSHAW
// ISA extensions  : XOP
// Supported forms : (3 forms)
//
//    * VPSHAW xmm, xmm, xmm
//    * VPSHAW m128, xmm, xmm
//    * VPSHAW xmm, m128, xmm
//
func (self *Program) VPSHAW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSHAW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[0]) << 3))
            m.emit(0x99)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf8 ^ (hlcode(v[1]) << 3))
            m.emit(0x99)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSHAW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x80, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x99)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSHAW xmm, m128, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0x99)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSHAW")
    }
    return p
}

// VPSHLB performs "Packed Shift Logical Bytes".
//
// Mnemonic        : VPSHLB
// ISA extensions  : XOP
// Supported forms : (3 forms)
//
//    * VPSHLB xmm, xmm, xmm
//    * VPSHLB m128, xmm, xmm
//    * VPSHLB xmm, m128, xmm
//
func (self *Program) VPSHLB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSHLB xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[0]) << 3))
            m.emit(0x94)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf8 ^ (hlcode(v[1]) << 3))
            m.emit(0x94)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSHLB m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x80, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x94)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSHLB xmm, m128, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0x94)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSHLB")
    }
    return p
}

// VPSHLD performs "Packed Shift Logical Doublewords".
//
// Mnemonic        : VPSHLD
// ISA extensions  : XOP
// Supported forms : (3 forms)
//
//    * VPSHLD xmm, xmm, xmm
//    * VPSHLD m128, xmm, xmm
//    * VPSHLD xmm, m128, xmm
//
func (self *Program) VPSHLD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSHLD xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[0]) << 3))
            m.emit(0x96)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf8 ^ (hlcode(v[1]) << 3))
            m.emit(0x96)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSHLD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x80, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x96)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSHLD xmm, m128, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0x96)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSHLD")
    }
    return p
}

// VPSHLQ performs "Packed Shift Logical Quadwords".
//
// Mnemonic        : VPSHLQ
// ISA extensions  : XOP
// Supported forms : (3 forms)
//
//    * VPSHLQ xmm, xmm, xmm
//    * VPSHLQ m128, xmm, xmm
//    * VPSHLQ xmm, m128, xmm
//
func (self *Program) VPSHLQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSHLQ xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[0]) << 3))
            m.emit(0x97)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf8 ^ (hlcode(v[1]) << 3))
            m.emit(0x97)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSHLQ m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x80, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x97)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSHLQ xmm, m128, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0x97)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSHLQ")
    }
    return p
}

// VPSHLW performs "Packed Shift Logical Words".
//
// Mnemonic        : VPSHLW
// ISA extensions  : XOP
// Supported forms : (3 forms)
//
//    * VPSHLW xmm, xmm, xmm
//    * VPSHLW m128, xmm, xmm
//    * VPSHLW xmm, m128, xmm
//
func (self *Program) VPSHLW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSHLW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x78 ^ (hlcode(v[0]) << 3))
            m.emit(0x95)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x8f)
            m.emit(0xe9 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf8 ^ (hlcode(v[1]) << 3))
            m.emit(0x95)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSHLW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x80, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x95)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSHLW xmm, m128, xmm
    if isXMM(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_XOP)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0x8f, 0b1001, 0x00, hcode(v[2]), addr(v[1]), hlcode(v[0]))
            m.emit(0x95)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSHLW")
    }
    return p
}

// VPSHUFB performs "Packed Shuffle Bytes".
//
// Mnemonic        : VPSHUFB
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPSHUFB xmm, xmm, xmm{k}{z}
//    * VPSHUFB m128, xmm, xmm{k}{z}
//    * VPSHUFB ymm, ymm, ymm{k}{z}
//    * VPSHUFB m256, ymm, ymm{k}{z}
//    * VPSHUFB zmm, zmm, zmm{k}{z}
//    * VPSHUFB m512, zmm, zmm{k}{z}
//    * VPSHUFB xmm, xmm, xmm
//    * VPSHUFB m128, xmm, xmm
//    * VPSHUFB ymm, ymm, ymm
//    * VPSHUFB m256, ymm, ymm
//
func (self *Program) VPSHUFB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSHUFB xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x00)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSHUFB m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x00)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSHUFB ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x00)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSHUFB m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x00)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPSHUFB zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x00)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSHUFB m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x00)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPSHUFB xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x00)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSHUFB m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x00)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSHUFB ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x00)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSHUFB m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x00)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSHUFB")
    }
    return p
}

// VPSHUFD performs "Shuffle Packed Doublewords".
//
// Mnemonic        : VPSHUFD
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPSHUFD imm8, m128/m32bcst, xmm{k}{z}
//    * VPSHUFD imm8, m256/m32bcst, ymm{k}{z}
//    * VPSHUFD imm8, m512/m32bcst, zmm{k}{z}
//    * VPSHUFD imm8, xmm, xmm{k}{z}
//    * VPSHUFD imm8, ymm, ymm{k}{z}
//    * VPSHUFD imm8, zmm, zmm{k}{z}
//    * VPSHUFD imm8, xmm, xmm
//    * VPSHUFD imm8, m128, xmm
//    * VPSHUFD imm8, ymm, ymm
//    * VPSHUFD imm8, m256, ymm
//
func (self *Program) VPSHUFD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSHUFD imm8, m128/m32bcst, xmm{k}{z}
    if isImm8(v0) && isM128M32bcst(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x70)
            m.mrsd(lcode(v[2]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSHUFD imm8, m256/m32bcst, ymm{k}{z}
    if isImm8(v0) && isM256M32bcst(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x70)
            m.mrsd(lcode(v[2]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSHUFD imm8, m512/m32bcst, zmm{k}{z}
    if isImm8(v0) && isM512M32bcst(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x70)
            m.mrsd(lcode(v[2]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSHUFD imm8, xmm, xmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x08)
            m.emit(0x70)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSHUFD imm8, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x28)
            m.emit(0x70)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSHUFD imm8, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x48)
            m.emit(0x70)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSHUFD imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[1], 0)
            m.emit(0x70)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSHUFD imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[1]), 0)
            m.emit(0x70)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSHUFD imm8, ymm, ymm
    if isImm8(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[1], 0)
            m.emit(0x70)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSHUFD imm8, m256, ymm
    if isImm8(v0) && isM256(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[1]), 0)
            m.emit(0x70)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSHUFD")
    }
    return p
}

// VPSHUFHW performs "Shuffle Packed High Words".
//
// Mnemonic        : VPSHUFHW
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPSHUFHW imm8, xmm, xmm{k}{z}
//    * VPSHUFHW imm8, ymm, ymm{k}{z}
//    * VPSHUFHW imm8, zmm, zmm{k}{z}
//    * VPSHUFHW imm8, m128, xmm{k}{z}
//    * VPSHUFHW imm8, m256, ymm{k}{z}
//    * VPSHUFHW imm8, m512, zmm{k}{z}
//    * VPSHUFHW imm8, xmm, xmm
//    * VPSHUFHW imm8, m128, xmm
//    * VPSHUFHW imm8, ymm, ymm
//    * VPSHUFHW imm8, m256, ymm
//
func (self *Program) VPSHUFHW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSHUFHW imm8, xmm, xmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x08)
            m.emit(0x70)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSHUFHW imm8, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x28)
            m.emit(0x70)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSHUFHW imm8, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7e)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x48)
            m.emit(0x70)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSHUFHW imm8, m128, xmm{k}{z}
    if isImm8(v0) && isM128(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b00, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x70)
            m.mrsd(lcode(v[2]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSHUFHW imm8, m256, ymm{k}{z}
    if isImm8(v0) && isM256(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b01, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x70)
            m.mrsd(lcode(v[2]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSHUFHW imm8, m512, zmm{k}{z}
    if isImm8(v0) && isM512(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b10, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x70)
            m.mrsd(lcode(v[2]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSHUFHW imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[2]), v[1], 0)
            m.emit(0x70)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSHUFHW imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[2]), addr(v[1]), 0)
            m.emit(0x70)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSHUFHW imm8, ymm, ymm
    if isImm8(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(6, hcode(v[2]), v[1], 0)
            m.emit(0x70)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSHUFHW imm8, m256, ymm
    if isImm8(v0) && isM256(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(6, hcode(v[2]), addr(v[1]), 0)
            m.emit(0x70)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSHUFHW")
    }
    return p
}

// VPSHUFLW performs "Shuffle Packed Low Words".
//
// Mnemonic        : VPSHUFLW
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPSHUFLW imm8, xmm, xmm{k}{z}
//    * VPSHUFLW imm8, ymm, ymm{k}{z}
//    * VPSHUFLW imm8, zmm, zmm{k}{z}
//    * VPSHUFLW imm8, m128, xmm{k}{z}
//    * VPSHUFLW imm8, m256, ymm{k}{z}
//    * VPSHUFLW imm8, m512, zmm{k}{z}
//    * VPSHUFLW imm8, xmm, xmm
//    * VPSHUFLW imm8, m128, xmm
//    * VPSHUFLW imm8, ymm, ymm
//    * VPSHUFLW imm8, m256, ymm
//
func (self *Program) VPSHUFLW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSHUFLW imm8, xmm, xmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7f)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x08)
            m.emit(0x70)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSHUFLW imm8, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7f)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x28)
            m.emit(0x70)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSHUFLW imm8, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7f)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x48)
            m.emit(0x70)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSHUFLW imm8, m128, xmm{k}{z}
    if isImm8(v0) && isM128(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x07, 0b00, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x70)
            m.mrsd(lcode(v[2]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSHUFLW imm8, m256, ymm{k}{z}
    if isImm8(v0) && isM256(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x07, 0b01, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x70)
            m.mrsd(lcode(v[2]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSHUFLW imm8, m512, zmm{k}{z}
    if isImm8(v0) && isM512(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x07, 0b10, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x70)
            m.mrsd(lcode(v[2]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSHUFLW imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[2]), v[1], 0)
            m.emit(0x70)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSHUFLW imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[2]), addr(v[1]), 0)
            m.emit(0x70)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSHUFLW imm8, ymm, ymm
    if isImm8(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(7, hcode(v[2]), v[1], 0)
            m.emit(0x70)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSHUFLW imm8, m256, ymm
    if isImm8(v0) && isM256(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(7, hcode(v[2]), addr(v[1]), 0)
            m.emit(0x70)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSHUFLW")
    }
    return p
}

// VPSIGNB performs "Packed Sign of Byte Integers".
//
// Mnemonic        : VPSIGNB
// ISA extensions  : AVX, AVX2
// Supported forms : (4 forms)
//
//    * VPSIGNB xmm, xmm, xmm
//    * VPSIGNB m128, xmm, xmm
//    * VPSIGNB ymm, ymm, ymm
//    * VPSIGNB m256, ymm, ymm
//
func (self *Program) VPSIGNB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSIGNB xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x08)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSIGNB m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x08)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSIGNB ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x08)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSIGNB m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x08)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSIGNB")
    }
    return p
}

// VPSIGND performs "Packed Sign of Doubleword Integers".
//
// Mnemonic        : VPSIGND
// ISA extensions  : AVX, AVX2
// Supported forms : (4 forms)
//
//    * VPSIGND xmm, xmm, xmm
//    * VPSIGND m128, xmm, xmm
//    * VPSIGND ymm, ymm, ymm
//    * VPSIGND m256, ymm, ymm
//
func (self *Program) VPSIGND(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSIGND xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x0a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSIGND m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x0a)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSIGND ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x0a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSIGND m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x0a)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSIGND")
    }
    return p
}

// VPSIGNW performs "Packed Sign of Word Integers".
//
// Mnemonic        : VPSIGNW
// ISA extensions  : AVX, AVX2
// Supported forms : (4 forms)
//
//    * VPSIGNW xmm, xmm, xmm
//    * VPSIGNW m128, xmm, xmm
//    * VPSIGNW ymm, ymm, ymm
//    * VPSIGNW m256, ymm, ymm
//
func (self *Program) VPSIGNW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSIGNW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x09)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSIGNW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x09)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSIGNW ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x09)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSIGNW m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x09)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSIGNW")
    }
    return p
}

// VPSLLD performs "Shift Packed Doubleword Data Left Logical".
//
// Mnemonic        : VPSLLD
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (18 forms)
//
//    * VPSLLD imm8, m128/m32bcst, xmm{k}{z}
//    * VPSLLD imm8, m256/m32bcst, ymm{k}{z}
//    * VPSLLD imm8, m512/m32bcst, zmm{k}{z}
//    * VPSLLD imm8, xmm, xmm{k}{z}
//    * VPSLLD xmm, xmm, xmm{k}{z}
//    * VPSLLD m128, xmm, xmm{k}{z}
//    * VPSLLD imm8, ymm, ymm{k}{z}
//    * VPSLLD xmm, ymm, ymm{k}{z}
//    * VPSLLD m128, ymm, ymm{k}{z}
//    * VPSLLD imm8, zmm, zmm{k}{z}
//    * VPSLLD xmm, zmm, zmm{k}{z}
//    * VPSLLD m128, zmm, zmm{k}{z}
//    * VPSLLD imm8, xmm, xmm
//    * VPSLLD xmm, xmm, xmm
//    * VPSLLD m128, xmm, xmm
//    * VPSLLD imm8, ymm, ymm
//    * VPSLLD xmm, ymm, ymm
//    * VPSLLD m128, ymm, ymm
//
func (self *Program) VPSLLD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSLLD imm8, m128/m32bcst, xmm{k}{z}
    if isImm8(v0) && isM128M32bcst(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x72)
            m.mrsd(6, addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSLLD imm8, m256/m32bcst, ymm{k}{z}
    if isImm8(v0) && isM256M32bcst(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x72)
            m.mrsd(6, addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSLLD imm8, m512/m32bcst, zmm{k}{z}
    if isImm8(v0) && isM512M32bcst(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x72)
            m.mrsd(6, addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSLLD imm8, xmm, xmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x72)
            m.emit(0xf0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSLLD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xf2)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSLLD m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xf2)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSLLD imm8, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x72)
            m.emit(0xf0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSLLD xmm, ymm, ymm{k}{z}
    if isEVEXXMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xf2)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSLLD m128, ymm, ymm{k}{z}
    if isM128(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xf2)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSLLD imm8, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x72)
            m.emit(0xf0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSLLD xmm, zmm, zmm{k}{z}
    if isEVEXXMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xf2)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSLLD m128, zmm, zmm{k}{z}
    if isM128(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xf2)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSLLD imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, 0, v[1], hlcode(v[2]))
            m.emit(0x72)
            m.emit(0xf0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSLLD xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xf2)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSLLD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xf2)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSLLD imm8, ymm, ymm
    if isImm8(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, 0, v[1], hlcode(v[2]))
            m.emit(0x72)
            m.emit(0xf0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSLLD xmm, ymm, ymm
    if isXMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xf2)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSLLD m128, ymm, ymm
    if isM128(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xf2)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSLLD")
    }
    return p
}

// VPSLLDQ performs "Shift Packed Double Quadword Left Logical".
//
// Mnemonic        : VPSLLDQ
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (8 forms)
//
//    * VPSLLDQ imm8, xmm, xmm
//    * VPSLLDQ imm8, xmm, xmm
//    * VPSLLDQ imm8, m128, xmm
//    * VPSLLDQ imm8, ymm, ymm
//    * VPSLLDQ imm8, ymm, ymm
//    * VPSLLDQ imm8, m256, ymm
//    * VPSLLDQ imm8, zmm, zmm
//    * VPSLLDQ imm8, m512, zmm
//
func (self *Program) VPSLLDQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSLLDQ imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, 0, v[1], hlcode(v[2]))
            m.emit(0x73)
            m.emit(0xf8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSLLDQ imm8, xmm, xmm
    if isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | 0x00)
            m.emit(0x73)
            m.emit(0xf8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSLLDQ imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isEVEXXMM(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, 0, addr(v[1]), vcode(v[2]), 0, 0, 0)
            m.emit(0x73)
            m.mrsd(7, addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSLLDQ imm8, ymm, ymm
    if isImm8(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, 0, v[1], hlcode(v[2]))
            m.emit(0x73)
            m.emit(0xf8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSLLDQ imm8, ymm, ymm
    if isImm8(v0) && isEVEXYMM(v1) && isEVEXYMM(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | 0x20)
            m.emit(0x73)
            m.emit(0xf8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSLLDQ imm8, m256, ymm
    if isImm8(v0) && isM256(v1) && isEVEXYMM(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, 0, addr(v[1]), vcode(v[2]), 0, 0, 0)
            m.emit(0x73)
            m.mrsd(7, addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSLLDQ imm8, zmm, zmm
    if isImm8(v0) && isZMM(v1) && isZMM(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | 0x40)
            m.emit(0x73)
            m.emit(0xf8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSLLDQ imm8, m512, zmm
    if isImm8(v0) && isM512(v1) && isZMM(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, 0, addr(v[1]), vcode(v[2]), 0, 0, 0)
            m.emit(0x73)
            m.mrsd(7, addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSLLDQ")
    }
    return p
}

// VPSLLQ performs "Shift Packed Quadword Data Left Logical".
//
// Mnemonic        : VPSLLQ
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (18 forms)
//
//    * VPSLLQ imm8, m128/m64bcst, xmm{k}{z}
//    * VPSLLQ imm8, m256/m64bcst, ymm{k}{z}
//    * VPSLLQ imm8, m512/m64bcst, zmm{k}{z}
//    * VPSLLQ imm8, xmm, xmm{k}{z}
//    * VPSLLQ xmm, xmm, xmm{k}{z}
//    * VPSLLQ m128, xmm, xmm{k}{z}
//    * VPSLLQ imm8, ymm, ymm{k}{z}
//    * VPSLLQ xmm, ymm, ymm{k}{z}
//    * VPSLLQ m128, ymm, ymm{k}{z}
//    * VPSLLQ imm8, zmm, zmm{k}{z}
//    * VPSLLQ xmm, zmm, zmm{k}{z}
//    * VPSLLQ m128, zmm, zmm{k}{z}
//    * VPSLLQ imm8, xmm, xmm
//    * VPSLLQ xmm, xmm, xmm
//    * VPSLLQ m128, xmm, xmm
//    * VPSLLQ imm8, ymm, ymm
//    * VPSLLQ xmm, ymm, ymm
//    * VPSLLQ m128, ymm, ymm
//
func (self *Program) VPSLLQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSLLQ imm8, m128/m64bcst, xmm{k}{z}
    if isImm8(v0) && isM128M64bcst(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x73)
            m.mrsd(6, addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSLLQ imm8, m256/m64bcst, ymm{k}{z}
    if isImm8(v0) && isM256M64bcst(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x73)
            m.mrsd(6, addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSLLQ imm8, m512/m64bcst, zmm{k}{z}
    if isImm8(v0) && isM512M64bcst(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x73)
            m.mrsd(6, addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSLLQ imm8, xmm, xmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x73)
            m.emit(0xf0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSLLQ xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xf3)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSLLQ m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xf3)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSLLQ imm8, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x73)
            m.emit(0xf0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSLLQ xmm, ymm, ymm{k}{z}
    if isEVEXXMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xf3)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSLLQ m128, ymm, ymm{k}{z}
    if isM128(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xf3)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSLLQ imm8, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x73)
            m.emit(0xf0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSLLQ xmm, zmm, zmm{k}{z}
    if isEVEXXMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xf3)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSLLQ m128, zmm, zmm{k}{z}
    if isM128(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xf3)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSLLQ imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, 0, v[1], hlcode(v[2]))
            m.emit(0x73)
            m.emit(0xf0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSLLQ xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xf3)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSLLQ m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xf3)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSLLQ imm8, ymm, ymm
    if isImm8(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, 0, v[1], hlcode(v[2]))
            m.emit(0x73)
            m.emit(0xf0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSLLQ xmm, ymm, ymm
    if isXMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xf3)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSLLQ m128, ymm, ymm
    if isM128(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xf3)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSLLQ")
    }
    return p
}

// VPSLLVD performs "Variable Shift Packed Doubleword Data Left Logical".
//
// Mnemonic        : VPSLLVD
// ISA extensions  : AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPSLLVD m128/m32bcst, xmm, xmm{k}{z}
//    * VPSLLVD xmm, xmm, xmm{k}{z}
//    * VPSLLVD m256/m32bcst, ymm, ymm{k}{z}
//    * VPSLLVD ymm, ymm, ymm{k}{z}
//    * VPSLLVD m512/m32bcst, zmm, zmm{k}{z}
//    * VPSLLVD zmm, zmm, zmm{k}{z}
//    * VPSLLVD xmm, xmm, xmm
//    * VPSLLVD m128, xmm, xmm
//    * VPSLLVD ymm, ymm, ymm
//    * VPSLLVD m256, ymm, ymm
//
func (self *Program) VPSLLVD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSLLVD m128/m32bcst, xmm, xmm{k}{z}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x47)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSLLVD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x47)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSLLVD m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x47)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPSLLVD ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x47)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSLLVD m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x47)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPSLLVD zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x47)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSLLVD xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x47)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSLLVD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x47)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSLLVD ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x47)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSLLVD m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x47)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSLLVD")
    }
    return p
}

// VPSLLVQ performs "Variable Shift Packed Quadword Data Left Logical".
//
// Mnemonic        : VPSLLVQ
// ISA extensions  : AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPSLLVQ m128/m64bcst, xmm, xmm{k}{z}
//    * VPSLLVQ xmm, xmm, xmm{k}{z}
//    * VPSLLVQ m256/m64bcst, ymm, ymm{k}{z}
//    * VPSLLVQ ymm, ymm, ymm{k}{z}
//    * VPSLLVQ m512/m64bcst, zmm, zmm{k}{z}
//    * VPSLLVQ zmm, zmm, zmm{k}{z}
//    * VPSLLVQ xmm, xmm, xmm
//    * VPSLLVQ m128, xmm, xmm
//    * VPSLLVQ ymm, ymm, ymm
//    * VPSLLVQ m256, ymm, ymm
//
func (self *Program) VPSLLVQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSLLVQ m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x47)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSLLVQ xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x47)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSLLVQ m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x47)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPSLLVQ ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x47)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSLLVQ m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x47)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPSLLVQ zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x47)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSLLVQ xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[1]) << 3))
            m.emit(0x47)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSLLVQ m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x47)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSLLVQ ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit(0x47)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSLLVQ m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x85, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x47)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSLLVQ")
    }
    return p
}

// VPSLLVW performs "Variable Shift Packed Word Data Left Logical".
//
// Mnemonic        : VPSLLVW
// ISA extensions  : AVX512BW, AVX512VL
// Supported forms : (6 forms)
//
//    * VPSLLVW xmm, xmm, xmm{k}{z}
//    * VPSLLVW m128, xmm, xmm{k}{z}
//    * VPSLLVW ymm, ymm, ymm{k}{z}
//    * VPSLLVW m256, ymm, ymm{k}{z}
//    * VPSLLVW zmm, zmm, zmm{k}{z}
//    * VPSLLVW m512, zmm, zmm{k}{z}
//
func (self *Program) VPSLLVW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSLLVW xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x12)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSLLVW m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x12)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSLLVW ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x12)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSLLVW m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x12)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPSLLVW zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x12)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSLLVW m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x12)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSLLVW")
    }
    return p
}

// VPSLLW performs "Shift Packed Word Data Left Logical".
//
// Mnemonic        : VPSLLW
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (18 forms)
//
//    * VPSLLW imm8, xmm, xmm{k}{z}
//    * VPSLLW xmm, xmm, xmm{k}{z}
//    * VPSLLW m128, xmm, xmm{k}{z}
//    * VPSLLW imm8, ymm, ymm{k}{z}
//    * VPSLLW xmm, ymm, ymm{k}{z}
//    * VPSLLW m128, ymm, ymm{k}{z}
//    * VPSLLW imm8, zmm, zmm{k}{z}
//    * VPSLLW xmm, zmm, zmm{k}{z}
//    * VPSLLW m128, zmm, zmm{k}{z}
//    * VPSLLW imm8, m128, xmm{k}{z}
//    * VPSLLW imm8, m256, ymm{k}{z}
//    * VPSLLW imm8, m512, zmm{k}{z}
//    * VPSLLW imm8, xmm, xmm
//    * VPSLLW xmm, xmm, xmm
//    * VPSLLW m128, xmm, xmm
//    * VPSLLW imm8, ymm, ymm
//    * VPSLLW xmm, ymm, ymm
//    * VPSLLW m128, ymm, ymm
//
func (self *Program) VPSLLW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSLLW imm8, xmm, xmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x71)
            m.emit(0xf0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSLLW xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xf1)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSLLW m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xf1)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSLLW imm8, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x71)
            m.emit(0xf0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSLLW xmm, ymm, ymm{k}{z}
    if isEVEXXMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xf1)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSLLW m128, ymm, ymm{k}{z}
    if isM128(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xf1)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSLLW imm8, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x71)
            m.emit(0xf0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSLLW xmm, zmm, zmm{k}{z}
    if isEVEXXMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xf1)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSLLW m128, zmm, zmm{k}{z}
    if isM128(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xf1)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSLLW imm8, m128, xmm{k}{z}
    if isImm8(v0) && isM128(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x71)
            m.mrsd(6, addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSLLW imm8, m256, ymm{k}{z}
    if isImm8(v0) && isM256(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x71)
            m.mrsd(6, addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSLLW imm8, m512, zmm{k}{z}
    if isImm8(v0) && isM512(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x71)
            m.mrsd(6, addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSLLW imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, 0, v[1], hlcode(v[2]))
            m.emit(0x71)
            m.emit(0xf0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSLLW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xf1)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSLLW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xf1)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSLLW imm8, ymm, ymm
    if isImm8(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, 0, v[1], hlcode(v[2]))
            m.emit(0x71)
            m.emit(0xf0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSLLW xmm, ymm, ymm
    if isXMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xf1)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSLLW m128, ymm, ymm
    if isM128(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xf1)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSLLW")
    }
    return p
}

// VPSRAD performs "Shift Packed Doubleword Data Right Arithmetic".
//
// Mnemonic        : VPSRAD
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (18 forms)
//
//    * VPSRAD imm8, m128/m32bcst, xmm{k}{z}
//    * VPSRAD imm8, m256/m32bcst, ymm{k}{z}
//    * VPSRAD imm8, m512/m32bcst, zmm{k}{z}
//    * VPSRAD imm8, xmm, xmm{k}{z}
//    * VPSRAD xmm, xmm, xmm{k}{z}
//    * VPSRAD m128, xmm, xmm{k}{z}
//    * VPSRAD imm8, ymm, ymm{k}{z}
//    * VPSRAD xmm, ymm, ymm{k}{z}
//    * VPSRAD m128, ymm, ymm{k}{z}
//    * VPSRAD imm8, zmm, zmm{k}{z}
//    * VPSRAD xmm, zmm, zmm{k}{z}
//    * VPSRAD m128, zmm, zmm{k}{z}
//    * VPSRAD imm8, xmm, xmm
//    * VPSRAD xmm, xmm, xmm
//    * VPSRAD m128, xmm, xmm
//    * VPSRAD imm8, ymm, ymm
//    * VPSRAD xmm, ymm, ymm
//    * VPSRAD m128, ymm, ymm
//
func (self *Program) VPSRAD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSRAD imm8, m128/m32bcst, xmm{k}{z}
    if isImm8(v0) && isM128M32bcst(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x72)
            m.mrsd(4, addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRAD imm8, m256/m32bcst, ymm{k}{z}
    if isImm8(v0) && isM256M32bcst(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x72)
            m.mrsd(4, addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRAD imm8, m512/m32bcst, zmm{k}{z}
    if isImm8(v0) && isM512M32bcst(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x72)
            m.mrsd(4, addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRAD imm8, xmm, xmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x72)
            m.emit(0xe0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRAD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xe2)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRAD m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xe2)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSRAD imm8, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x72)
            m.emit(0xe0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRAD xmm, ymm, ymm{k}{z}
    if isEVEXXMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xe2)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRAD m128, ymm, ymm{k}{z}
    if isM128(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xe2)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSRAD imm8, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x72)
            m.emit(0xe0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRAD xmm, zmm, zmm{k}{z}
    if isEVEXXMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xe2)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRAD m128, zmm, zmm{k}{z}
    if isM128(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xe2)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSRAD imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, 0, v[1], hlcode(v[2]))
            m.emit(0x72)
            m.emit(0xe0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRAD xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xe2)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRAD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xe2)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSRAD imm8, ymm, ymm
    if isImm8(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, 0, v[1], hlcode(v[2]))
            m.emit(0x72)
            m.emit(0xe0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRAD xmm, ymm, ymm
    if isXMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xe2)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRAD m128, ymm, ymm
    if isM128(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xe2)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSRAD")
    }
    return p
}

// VPSRAQ performs "Shift Packed Quadword Data Right Arithmetic".
//
// Mnemonic        : VPSRAQ
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (12 forms)
//
//    * VPSRAQ imm8, m128/m64bcst, xmm{k}{z}
//    * VPSRAQ imm8, m256/m64bcst, ymm{k}{z}
//    * VPSRAQ imm8, m512/m64bcst, zmm{k}{z}
//    * VPSRAQ imm8, xmm, xmm{k}{z}
//    * VPSRAQ xmm, xmm, xmm{k}{z}
//    * VPSRAQ m128, xmm, xmm{k}{z}
//    * VPSRAQ imm8, ymm, ymm{k}{z}
//    * VPSRAQ xmm, ymm, ymm{k}{z}
//    * VPSRAQ m128, ymm, ymm{k}{z}
//    * VPSRAQ imm8, zmm, zmm{k}{z}
//    * VPSRAQ xmm, zmm, zmm{k}{z}
//    * VPSRAQ m128, zmm, zmm{k}{z}
//
func (self *Program) VPSRAQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSRAQ imm8, m128/m64bcst, xmm{k}{z}
    if isImm8(v0) && isM128M64bcst(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x72)
            m.mrsd(4, addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRAQ imm8, m256/m64bcst, ymm{k}{z}
    if isImm8(v0) && isM256M64bcst(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x72)
            m.mrsd(4, addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRAQ imm8, m512/m64bcst, zmm{k}{z}
    if isImm8(v0) && isM512M64bcst(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x72)
            m.mrsd(4, addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRAQ imm8, xmm, xmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x72)
            m.emit(0xe0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRAQ xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xe2)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRAQ m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xe2)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSRAQ imm8, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x72)
            m.emit(0xe0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRAQ xmm, ymm, ymm{k}{z}
    if isEVEXXMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xe2)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRAQ m128, ymm, ymm{k}{z}
    if isM128(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xe2)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSRAQ imm8, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x72)
            m.emit(0xe0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRAQ xmm, zmm, zmm{k}{z}
    if isEVEXXMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xe2)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRAQ m128, zmm, zmm{k}{z}
    if isM128(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xe2)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSRAQ")
    }
    return p
}

// VPSRAVD performs "Variable Shift Packed Doubleword Data Right Arithmetic".
//
// Mnemonic        : VPSRAVD
// ISA extensions  : AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPSRAVD m128/m32bcst, xmm, xmm{k}{z}
//    * VPSRAVD xmm, xmm, xmm{k}{z}
//    * VPSRAVD m256/m32bcst, ymm, ymm{k}{z}
//    * VPSRAVD ymm, ymm, ymm{k}{z}
//    * VPSRAVD m512/m32bcst, zmm, zmm{k}{z}
//    * VPSRAVD zmm, zmm, zmm{k}{z}
//    * VPSRAVD xmm, xmm, xmm
//    * VPSRAVD m128, xmm, xmm
//    * VPSRAVD ymm, ymm, ymm
//    * VPSRAVD m256, ymm, ymm
//
func (self *Program) VPSRAVD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSRAVD m128/m32bcst, xmm, xmm{k}{z}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x46)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSRAVD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x46)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRAVD m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x46)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPSRAVD ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x46)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRAVD m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x46)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPSRAVD zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x46)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRAVD xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x46)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRAVD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x46)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSRAVD ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x46)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRAVD m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x46)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSRAVD")
    }
    return p
}

// VPSRAVQ performs "Variable Shift Packed Quadword Data Right Arithmetic".
//
// Mnemonic        : VPSRAVQ
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPSRAVQ m128/m64bcst, xmm, xmm{k}{z}
//    * VPSRAVQ xmm, xmm, xmm{k}{z}
//    * VPSRAVQ m256/m64bcst, ymm, ymm{k}{z}
//    * VPSRAVQ ymm, ymm, ymm{k}{z}
//    * VPSRAVQ m512/m64bcst, zmm, zmm{k}{z}
//    * VPSRAVQ zmm, zmm, zmm{k}{z}
//
func (self *Program) VPSRAVQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSRAVQ m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x46)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSRAVQ xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x46)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRAVQ m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x46)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPSRAVQ ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x46)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRAVQ m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x46)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPSRAVQ zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x46)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSRAVQ")
    }
    return p
}

// VPSRAVW performs "Variable Shift Packed Word Data Right Arithmetic".
//
// Mnemonic        : VPSRAVW
// ISA extensions  : AVX512BW, AVX512VL
// Supported forms : (6 forms)
//
//    * VPSRAVW xmm, xmm, xmm{k}{z}
//    * VPSRAVW m128, xmm, xmm{k}{z}
//    * VPSRAVW ymm, ymm, ymm{k}{z}
//    * VPSRAVW m256, ymm, ymm{k}{z}
//    * VPSRAVW zmm, zmm, zmm{k}{z}
//    * VPSRAVW m512, zmm, zmm{k}{z}
//
func (self *Program) VPSRAVW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSRAVW xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x11)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRAVW m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x11)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSRAVW ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x11)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRAVW m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x11)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPSRAVW zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x11)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRAVW m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x11)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSRAVW")
    }
    return p
}

// VPSRAW performs "Shift Packed Word Data Right Arithmetic".
//
// Mnemonic        : VPSRAW
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (18 forms)
//
//    * VPSRAW imm8, xmm, xmm{k}{z}
//    * VPSRAW xmm, xmm, xmm{k}{z}
//    * VPSRAW m128, xmm, xmm{k}{z}
//    * VPSRAW imm8, ymm, ymm{k}{z}
//    * VPSRAW xmm, ymm, ymm{k}{z}
//    * VPSRAW m128, ymm, ymm{k}{z}
//    * VPSRAW imm8, zmm, zmm{k}{z}
//    * VPSRAW xmm, zmm, zmm{k}{z}
//    * VPSRAW m128, zmm, zmm{k}{z}
//    * VPSRAW imm8, m128, xmm{k}{z}
//    * VPSRAW imm8, m256, ymm{k}{z}
//    * VPSRAW imm8, m512, zmm{k}{z}
//    * VPSRAW imm8, xmm, xmm
//    * VPSRAW xmm, xmm, xmm
//    * VPSRAW m128, xmm, xmm
//    * VPSRAW imm8, ymm, ymm
//    * VPSRAW xmm, ymm, ymm
//    * VPSRAW m128, ymm, ymm
//
func (self *Program) VPSRAW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSRAW imm8, xmm, xmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x71)
            m.emit(0xe0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRAW xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xe1)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRAW m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xe1)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSRAW imm8, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x71)
            m.emit(0xe0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRAW xmm, ymm, ymm{k}{z}
    if isEVEXXMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xe1)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRAW m128, ymm, ymm{k}{z}
    if isM128(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xe1)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSRAW imm8, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x71)
            m.emit(0xe0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRAW xmm, zmm, zmm{k}{z}
    if isEVEXXMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xe1)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRAW m128, zmm, zmm{k}{z}
    if isM128(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xe1)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSRAW imm8, m128, xmm{k}{z}
    if isImm8(v0) && isM128(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x71)
            m.mrsd(4, addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRAW imm8, m256, ymm{k}{z}
    if isImm8(v0) && isM256(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x71)
            m.mrsd(4, addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRAW imm8, m512, zmm{k}{z}
    if isImm8(v0) && isM512(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x71)
            m.mrsd(4, addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRAW imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, 0, v[1], hlcode(v[2]))
            m.emit(0x71)
            m.emit(0xe0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRAW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xe1)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRAW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xe1)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSRAW imm8, ymm, ymm
    if isImm8(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, 0, v[1], hlcode(v[2]))
            m.emit(0x71)
            m.emit(0xe0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRAW xmm, ymm, ymm
    if isXMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xe1)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRAW m128, ymm, ymm
    if isM128(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xe1)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSRAW")
    }
    return p
}

// VPSRLD performs "Shift Packed Doubleword Data Right Logical".
//
// Mnemonic        : VPSRLD
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (18 forms)
//
//    * VPSRLD imm8, m128/m32bcst, xmm{k}{z}
//    * VPSRLD imm8, m256/m32bcst, ymm{k}{z}
//    * VPSRLD imm8, m512/m32bcst, zmm{k}{z}
//    * VPSRLD imm8, xmm, xmm{k}{z}
//    * VPSRLD xmm, xmm, xmm{k}{z}
//    * VPSRLD m128, xmm, xmm{k}{z}
//    * VPSRLD imm8, ymm, ymm{k}{z}
//    * VPSRLD xmm, ymm, ymm{k}{z}
//    * VPSRLD m128, ymm, ymm{k}{z}
//    * VPSRLD imm8, zmm, zmm{k}{z}
//    * VPSRLD xmm, zmm, zmm{k}{z}
//    * VPSRLD m128, zmm, zmm{k}{z}
//    * VPSRLD imm8, xmm, xmm
//    * VPSRLD xmm, xmm, xmm
//    * VPSRLD m128, xmm, xmm
//    * VPSRLD imm8, ymm, ymm
//    * VPSRLD xmm, ymm, ymm
//    * VPSRLD m128, ymm, ymm
//
func (self *Program) VPSRLD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSRLD imm8, m128/m32bcst, xmm{k}{z}
    if isImm8(v0) && isM128M32bcst(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x72)
            m.mrsd(2, addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRLD imm8, m256/m32bcst, ymm{k}{z}
    if isImm8(v0) && isM256M32bcst(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x72)
            m.mrsd(2, addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRLD imm8, m512/m32bcst, zmm{k}{z}
    if isImm8(v0) && isM512M32bcst(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x72)
            m.mrsd(2, addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRLD imm8, xmm, xmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x72)
            m.emit(0xd0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRLD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xd2)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRLD m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xd2)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSRLD imm8, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x72)
            m.emit(0xd0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRLD xmm, ymm, ymm{k}{z}
    if isEVEXXMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xd2)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRLD m128, ymm, ymm{k}{z}
    if isM128(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xd2)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSRLD imm8, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x72)
            m.emit(0xd0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRLD xmm, zmm, zmm{k}{z}
    if isEVEXXMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xd2)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRLD m128, zmm, zmm{k}{z}
    if isM128(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xd2)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSRLD imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, 0, v[1], hlcode(v[2]))
            m.emit(0x72)
            m.emit(0xd0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRLD xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xd2)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRLD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xd2)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSRLD imm8, ymm, ymm
    if isImm8(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, 0, v[1], hlcode(v[2]))
            m.emit(0x72)
            m.emit(0xd0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRLD xmm, ymm, ymm
    if isXMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xd2)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRLD m128, ymm, ymm
    if isM128(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xd2)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSRLD")
    }
    return p
}

// VPSRLDQ performs "Shift Packed Double Quadword Right Logical".
//
// Mnemonic        : VPSRLDQ
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (8 forms)
//
//    * VPSRLDQ imm8, xmm, xmm
//    * VPSRLDQ imm8, xmm, xmm
//    * VPSRLDQ imm8, m128, xmm
//    * VPSRLDQ imm8, ymm, ymm
//    * VPSRLDQ imm8, ymm, ymm
//    * VPSRLDQ imm8, m256, ymm
//    * VPSRLDQ imm8, zmm, zmm
//    * VPSRLDQ imm8, m512, zmm
//
func (self *Program) VPSRLDQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSRLDQ imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, 0, v[1], hlcode(v[2]))
            m.emit(0x73)
            m.emit(0xd8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRLDQ imm8, xmm, xmm
    if isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | 0x00)
            m.emit(0x73)
            m.emit(0xd8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRLDQ imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isEVEXXMM(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, 0, addr(v[1]), vcode(v[2]), 0, 0, 0)
            m.emit(0x73)
            m.mrsd(3, addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRLDQ imm8, ymm, ymm
    if isImm8(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, 0, v[1], hlcode(v[2]))
            m.emit(0x73)
            m.emit(0xd8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRLDQ imm8, ymm, ymm
    if isImm8(v0) && isEVEXYMM(v1) && isEVEXYMM(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | 0x20)
            m.emit(0x73)
            m.emit(0xd8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRLDQ imm8, m256, ymm
    if isImm8(v0) && isM256(v1) && isEVEXYMM(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, 0, addr(v[1]), vcode(v[2]), 0, 0, 0)
            m.emit(0x73)
            m.mrsd(3, addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRLDQ imm8, zmm, zmm
    if isImm8(v0) && isZMM(v1) && isZMM(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((0x08 ^ (ecode(v[2]) << 3)) | 0x40)
            m.emit(0x73)
            m.emit(0xd8 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRLDQ imm8, m512, zmm
    if isImm8(v0) && isM512(v1) && isZMM(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, 0, addr(v[1]), vcode(v[2]), 0, 0, 0)
            m.emit(0x73)
            m.mrsd(3, addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSRLDQ")
    }
    return p
}

// VPSRLQ performs "Shift Packed Quadword Data Right Logical".
//
// Mnemonic        : VPSRLQ
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (18 forms)
//
//    * VPSRLQ imm8, m128/m64bcst, xmm{k}{z}
//    * VPSRLQ imm8, m256/m64bcst, ymm{k}{z}
//    * VPSRLQ imm8, m512/m64bcst, zmm{k}{z}
//    * VPSRLQ imm8, xmm, xmm{k}{z}
//    * VPSRLQ xmm, xmm, xmm{k}{z}
//    * VPSRLQ m128, xmm, xmm{k}{z}
//    * VPSRLQ imm8, ymm, ymm{k}{z}
//    * VPSRLQ xmm, ymm, ymm{k}{z}
//    * VPSRLQ m128, ymm, ymm{k}{z}
//    * VPSRLQ imm8, zmm, zmm{k}{z}
//    * VPSRLQ xmm, zmm, zmm{k}{z}
//    * VPSRLQ m128, zmm, zmm{k}{z}
//    * VPSRLQ imm8, xmm, xmm
//    * VPSRLQ xmm, xmm, xmm
//    * VPSRLQ m128, xmm, xmm
//    * VPSRLQ imm8, ymm, ymm
//    * VPSRLQ xmm, ymm, ymm
//    * VPSRLQ m128, ymm, ymm
//
func (self *Program) VPSRLQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSRLQ imm8, m128/m64bcst, xmm{k}{z}
    if isImm8(v0) && isM128M64bcst(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x73)
            m.mrsd(2, addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRLQ imm8, m256/m64bcst, ymm{k}{z}
    if isImm8(v0) && isM256M64bcst(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x73)
            m.mrsd(2, addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRLQ imm8, m512/m64bcst, zmm{k}{z}
    if isImm8(v0) && isM512M64bcst(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x73)
            m.mrsd(2, addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRLQ imm8, xmm, xmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x73)
            m.emit(0xd0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRLQ xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xd3)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRLQ m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xd3)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSRLQ imm8, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x73)
            m.emit(0xd0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRLQ xmm, ymm, ymm{k}{z}
    if isEVEXXMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xd3)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRLQ m128, ymm, ymm{k}{z}
    if isM128(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xd3)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSRLQ imm8, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x73)
            m.emit(0xd0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRLQ xmm, zmm, zmm{k}{z}
    if isEVEXXMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xd3)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRLQ m128, zmm, zmm{k}{z}
    if isM128(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xd3)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSRLQ imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, 0, v[1], hlcode(v[2]))
            m.emit(0x73)
            m.emit(0xd0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRLQ xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xd3)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRLQ m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xd3)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSRLQ imm8, ymm, ymm
    if isImm8(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, 0, v[1], hlcode(v[2]))
            m.emit(0x73)
            m.emit(0xd0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRLQ xmm, ymm, ymm
    if isXMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xd3)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRLQ m128, ymm, ymm
    if isM128(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xd3)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSRLQ")
    }
    return p
}

// VPSRLVD performs "Variable Shift Packed Doubleword Data Right Logical".
//
// Mnemonic        : VPSRLVD
// ISA extensions  : AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPSRLVD m128/m32bcst, xmm, xmm{k}{z}
//    * VPSRLVD xmm, xmm, xmm{k}{z}
//    * VPSRLVD m256/m32bcst, ymm, ymm{k}{z}
//    * VPSRLVD ymm, ymm, ymm{k}{z}
//    * VPSRLVD m512/m32bcst, zmm, zmm{k}{z}
//    * VPSRLVD zmm, zmm, zmm{k}{z}
//    * VPSRLVD xmm, xmm, xmm
//    * VPSRLVD m128, xmm, xmm
//    * VPSRLVD ymm, ymm, ymm
//    * VPSRLVD m256, ymm, ymm
//
func (self *Program) VPSRLVD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSRLVD m128/m32bcst, xmm, xmm{k}{z}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x45)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSRLVD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x45)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRLVD m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x45)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPSRLVD ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x45)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRLVD m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x45)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPSRLVD zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x45)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRLVD xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79 ^ (hlcode(v[1]) << 3))
            m.emit(0x45)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRLVD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x45)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSRLVD ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit(0x45)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRLVD m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x45)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSRLVD")
    }
    return p
}

// VPSRLVQ performs "Variable Shift Packed Quadword Data Right Logical".
//
// Mnemonic        : VPSRLVQ
// ISA extensions  : AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPSRLVQ m128/m64bcst, xmm, xmm{k}{z}
//    * VPSRLVQ xmm, xmm, xmm{k}{z}
//    * VPSRLVQ m256/m64bcst, ymm, ymm{k}{z}
//    * VPSRLVQ ymm, ymm, ymm{k}{z}
//    * VPSRLVQ m512/m64bcst, zmm, zmm{k}{z}
//    * VPSRLVQ zmm, zmm, zmm{k}{z}
//    * VPSRLVQ xmm, xmm, xmm
//    * VPSRLVQ m128, xmm, xmm
//    * VPSRLVQ ymm, ymm, ymm
//    * VPSRLVQ m256, ymm, ymm
//
func (self *Program) VPSRLVQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSRLVQ m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x45)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSRLVQ xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x45)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRLVQ m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x45)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPSRLVQ ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x45)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRLVQ m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x45)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPSRLVQ zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x45)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRLVQ xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xf9 ^ (hlcode(v[1]) << 3))
            m.emit(0x45)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRLVQ m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x81, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x45)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSRLVQ ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[2]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit(0x45)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRLVQ m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x85, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x45)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSRLVQ")
    }
    return p
}

// VPSRLVW performs "Variable Shift Packed Word Data Right Logical".
//
// Mnemonic        : VPSRLVW
// ISA extensions  : AVX512BW, AVX512VL
// Supported forms : (6 forms)
//
//    * VPSRLVW xmm, xmm, xmm{k}{z}
//    * VPSRLVW m128, xmm, xmm{k}{z}
//    * VPSRLVW ymm, ymm, ymm{k}{z}
//    * VPSRLVW m256, ymm, ymm{k}{z}
//    * VPSRLVW zmm, zmm, zmm{k}{z}
//    * VPSRLVW m512, zmm, zmm{k}{z}
//
func (self *Program) VPSRLVW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSRLVW xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x10)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRLVW m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x10)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSRLVW ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x10)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRLVW m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x10)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPSRLVW zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x10)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRLVW m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x10)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSRLVW")
    }
    return p
}

// VPSRLW performs "Shift Packed Word Data Right Logical".
//
// Mnemonic        : VPSRLW
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (18 forms)
//
//    * VPSRLW imm8, xmm, xmm{k}{z}
//    * VPSRLW xmm, xmm, xmm{k}{z}
//    * VPSRLW m128, xmm, xmm{k}{z}
//    * VPSRLW imm8, ymm, ymm{k}{z}
//    * VPSRLW xmm, ymm, ymm{k}{z}
//    * VPSRLW m128, ymm, ymm{k}{z}
//    * VPSRLW imm8, zmm, zmm{k}{z}
//    * VPSRLW xmm, zmm, zmm{k}{z}
//    * VPSRLW m128, zmm, zmm{k}{z}
//    * VPSRLW imm8, m128, xmm{k}{z}
//    * VPSRLW imm8, m256, ymm{k}{z}
//    * VPSRLW imm8, m512, zmm{k}{z}
//    * VPSRLW imm8, xmm, xmm
//    * VPSRLW xmm, xmm, xmm
//    * VPSRLW m128, xmm, xmm
//    * VPSRLW imm8, ymm, ymm
//    * VPSRLW xmm, ymm, ymm
//    * VPSRLW m128, ymm, ymm
//
func (self *Program) VPSRLW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSRLW imm8, xmm, xmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x71)
            m.emit(0xd0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRLW xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xd1)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRLW m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xd1)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSRLW imm8, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x71)
            m.emit(0xd0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRLW xmm, ymm, ymm{k}{z}
    if isEVEXXMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xd1)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRLW m128, ymm, ymm{k}{z}
    if isM128(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xd1)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSRLW imm8, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ (ehcode(v[1]) << 5))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x71)
            m.emit(0xd0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRLW xmm, zmm, zmm{k}{z}
    if isEVEXXMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xd1)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRLW m128, zmm, zmm{k}{z}
    if isM128(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xd1)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSRLW imm8, m128, xmm{k}{z}
    if isImm8(v0) && isM128(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x71)
            m.mrsd(2, addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRLW imm8, m256, ymm{k}{z}
    if isImm8(v0) && isM256(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x71)
            m.mrsd(2, addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRLW imm8, m512, zmm{k}{z}
    if isImm8(v0) && isM512(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, 0, addr(v[1]), vcode(v[2]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x71)
            m.mrsd(2, addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRLW imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, 0, v[1], hlcode(v[2]))
            m.emit(0x71)
            m.emit(0xd0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRLW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xd1)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRLW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xd1)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSRLW imm8, ymm, ymm
    if isImm8(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, 0, v[1], hlcode(v[2]))
            m.emit(0x71)
            m.emit(0xd0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPSRLW xmm, ymm, ymm
    if isXMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xd1)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSRLW m128, ymm, ymm
    if isM128(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xd1)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSRLW")
    }
    return p
}

// VPSUBB performs "Subtract Packed Byte Integers".
//
// Mnemonic        : VPSUBB
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPSUBB xmm, xmm, xmm{k}{z}
//    * VPSUBB m128, xmm, xmm{k}{z}
//    * VPSUBB ymm, ymm, ymm{k}{z}
//    * VPSUBB m256, ymm, ymm{k}{z}
//    * VPSUBB zmm, zmm, zmm{k}{z}
//    * VPSUBB m512, zmm, zmm{k}{z}
//    * VPSUBB xmm, xmm, xmm
//    * VPSUBB m128, xmm, xmm
//    * VPSUBB ymm, ymm, ymm
//    * VPSUBB m256, ymm, ymm
//
func (self *Program) VPSUBB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSUBB xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xf8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBB m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xf8)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSUBB ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xf8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBB m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xf8)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPSUBB zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xf8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBB m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xf8)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPSUBB xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xf8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBB m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xf8)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSUBB ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xf8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBB m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xf8)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSUBB")
    }
    return p
}

// VPSUBD performs "Subtract Packed Doubleword Integers".
//
// Mnemonic        : VPSUBD
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPSUBD m128/m32bcst, xmm, xmm{k}{z}
//    * VPSUBD xmm, xmm, xmm{k}{z}
//    * VPSUBD m256/m32bcst, ymm, ymm{k}{z}
//    * VPSUBD ymm, ymm, ymm{k}{z}
//    * VPSUBD m512/m32bcst, zmm, zmm{k}{z}
//    * VPSUBD zmm, zmm, zmm{k}{z}
//    * VPSUBD xmm, xmm, xmm
//    * VPSUBD m128, xmm, xmm
//    * VPSUBD ymm, ymm, ymm
//    * VPSUBD m256, ymm, ymm
//
func (self *Program) VPSUBD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSUBD m128/m32bcst, xmm, xmm{k}{z}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xfa)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSUBD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xfa)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBD m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xfa)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPSUBD ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xfa)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBD m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xfa)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPSUBD zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xfa)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBD xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xfa)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xfa)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSUBD ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xfa)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBD m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xfa)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSUBD")
    }
    return p
}

// VPSUBQ performs "Subtract Packed Quadword Integers".
//
// Mnemonic        : VPSUBQ
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPSUBQ m128/m64bcst, xmm, xmm{k}{z}
//    * VPSUBQ xmm, xmm, xmm{k}{z}
//    * VPSUBQ m256/m64bcst, ymm, ymm{k}{z}
//    * VPSUBQ ymm, ymm, ymm{k}{z}
//    * VPSUBQ m512/m64bcst, zmm, zmm{k}{z}
//    * VPSUBQ zmm, zmm, zmm{k}{z}
//    * VPSUBQ xmm, xmm, xmm
//    * VPSUBQ m128, xmm, xmm
//    * VPSUBQ ymm, ymm, ymm
//    * VPSUBQ m256, ymm, ymm
//
func (self *Program) VPSUBQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSUBQ m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xfb)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSUBQ xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xfb)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBQ m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xfb)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPSUBQ ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xfb)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBQ m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xfb)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPSUBQ zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xfb)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBQ xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xfb)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBQ m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xfb)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSUBQ ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xfb)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBQ m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xfb)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSUBQ")
    }
    return p
}

// VPSUBSB performs "Subtract Packed Signed Byte Integers with Signed Saturation".
//
// Mnemonic        : VPSUBSB
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPSUBSB xmm, xmm, xmm{k}{z}
//    * VPSUBSB m128, xmm, xmm{k}{z}
//    * VPSUBSB ymm, ymm, ymm{k}{z}
//    * VPSUBSB m256, ymm, ymm{k}{z}
//    * VPSUBSB zmm, zmm, zmm{k}{z}
//    * VPSUBSB m512, zmm, zmm{k}{z}
//    * VPSUBSB xmm, xmm, xmm
//    * VPSUBSB m128, xmm, xmm
//    * VPSUBSB ymm, ymm, ymm
//    * VPSUBSB m256, ymm, ymm
//
func (self *Program) VPSUBSB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSUBSB xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xe8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBSB m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xe8)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSUBSB ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xe8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBSB m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xe8)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPSUBSB zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xe8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBSB m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xe8)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPSUBSB xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xe8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBSB m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xe8)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSUBSB ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xe8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBSB m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xe8)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSUBSB")
    }
    return p
}

// VPSUBSW performs "Subtract Packed Signed Word Integers with Signed Saturation".
//
// Mnemonic        : VPSUBSW
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPSUBSW xmm, xmm, xmm{k}{z}
//    * VPSUBSW m128, xmm, xmm{k}{z}
//    * VPSUBSW ymm, ymm, ymm{k}{z}
//    * VPSUBSW m256, ymm, ymm{k}{z}
//    * VPSUBSW zmm, zmm, zmm{k}{z}
//    * VPSUBSW m512, zmm, zmm{k}{z}
//    * VPSUBSW xmm, xmm, xmm
//    * VPSUBSW m128, xmm, xmm
//    * VPSUBSW ymm, ymm, ymm
//    * VPSUBSW m256, ymm, ymm
//
func (self *Program) VPSUBSW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSUBSW xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xe9)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBSW m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xe9)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSUBSW ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xe9)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBSW m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xe9)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPSUBSW zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xe9)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBSW m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xe9)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPSUBSW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xe9)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBSW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xe9)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSUBSW ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xe9)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBSW m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xe9)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSUBSW")
    }
    return p
}

// VPSUBUSB performs "Subtract Packed Unsigned Byte Integers with Unsigned Saturation".
//
// Mnemonic        : VPSUBUSB
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPSUBUSB xmm, xmm, xmm{k}{z}
//    * VPSUBUSB m128, xmm, xmm{k}{z}
//    * VPSUBUSB ymm, ymm, ymm{k}{z}
//    * VPSUBUSB m256, ymm, ymm{k}{z}
//    * VPSUBUSB zmm, zmm, zmm{k}{z}
//    * VPSUBUSB m512, zmm, zmm{k}{z}
//    * VPSUBUSB xmm, xmm, xmm
//    * VPSUBUSB m128, xmm, xmm
//    * VPSUBUSB ymm, ymm, ymm
//    * VPSUBUSB m256, ymm, ymm
//
func (self *Program) VPSUBUSB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSUBUSB xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xd8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBUSB m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xd8)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSUBUSB ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xd8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBUSB m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xd8)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPSUBUSB zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xd8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBUSB m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xd8)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPSUBUSB xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xd8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBUSB m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xd8)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSUBUSB ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xd8)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBUSB m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xd8)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSUBUSB")
    }
    return p
}

// VPSUBUSW performs "Subtract Packed Unsigned Word Integers with Unsigned Saturation".
//
// Mnemonic        : VPSUBUSW
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPSUBUSW xmm, xmm, xmm{k}{z}
//    * VPSUBUSW m128, xmm, xmm{k}{z}
//    * VPSUBUSW ymm, ymm, ymm{k}{z}
//    * VPSUBUSW m256, ymm, ymm{k}{z}
//    * VPSUBUSW zmm, zmm, zmm{k}{z}
//    * VPSUBUSW m512, zmm, zmm{k}{z}
//    * VPSUBUSW xmm, xmm, xmm
//    * VPSUBUSW m128, xmm, xmm
//    * VPSUBUSW ymm, ymm, ymm
//    * VPSUBUSW m256, ymm, ymm
//
func (self *Program) VPSUBUSW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSUBUSW xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xd9)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBUSW m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xd9)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSUBUSW ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xd9)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBUSW m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xd9)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPSUBUSW zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xd9)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBUSW m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xd9)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPSUBUSW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xd9)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBUSW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xd9)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSUBUSW ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xd9)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBUSW m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xd9)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSUBUSW")
    }
    return p
}

// VPSUBW performs "Subtract Packed Word Integers".
//
// Mnemonic        : VPSUBW
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPSUBW xmm, xmm, xmm{k}{z}
//    * VPSUBW m128, xmm, xmm{k}{z}
//    * VPSUBW ymm, ymm, ymm{k}{z}
//    * VPSUBW m256, ymm, ymm{k}{z}
//    * VPSUBW zmm, zmm, zmm{k}{z}
//    * VPSUBW m512, zmm, zmm{k}{z}
//    * VPSUBW xmm, xmm, xmm
//    * VPSUBW m128, xmm, xmm
//    * VPSUBW ymm, ymm, ymm
//    * VPSUBW m256, ymm, ymm
//
func (self *Program) VPSUBW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPSUBW xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xf9)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBW m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xf9)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPSUBW ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xf9)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBW m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xf9)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPSUBW zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xf9)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBW m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xf9)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPSUBW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xf9)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xf9)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPSUBW ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xf9)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPSUBW m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xf9)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPSUBW")
    }
    return p
}

// VPTERNLOGD performs "Bitwise Ternary Logical Operation on Doubleword Values".
//
// Mnemonic        : VPTERNLOGD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPTERNLOGD imm8, m128/m32bcst, xmm, xmm{k}{z}
//    * VPTERNLOGD imm8, xmm, xmm, xmm{k}{z}
//    * VPTERNLOGD imm8, m256/m32bcst, ymm, ymm{k}{z}
//    * VPTERNLOGD imm8, ymm, ymm, ymm{k}{z}
//    * VPTERNLOGD imm8, m512/m32bcst, zmm, zmm{k}{z}
//    * VPTERNLOGD imm8, zmm, zmm, zmm{k}{z}
//
func (self *Program) VPTERNLOGD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPTERNLOGD imm8, m128/m32bcst, xmm, xmm{k}{z}
    if isImm8(v0) && isM128M32bcst(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0x25)
            m.mrsd(lcode(v[3]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPTERNLOGD imm8, xmm, xmm, xmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x00)
            m.emit(0x25)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPTERNLOGD imm8, m256/m32bcst, ymm, ymm{k}{z}
    if isImm8(v0) && isM256M32bcst(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b01, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0x25)
            m.mrsd(lcode(v[3]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPTERNLOGD imm8, ymm, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x20)
            m.emit(0x25)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPTERNLOGD imm8, m512/m32bcst, zmm, zmm{k}{z}
    if isImm8(v0) && isM512M32bcst(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0x25)
            m.mrsd(lcode(v[3]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPTERNLOGD imm8, zmm, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x25)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPTERNLOGD")
    }
    return p
}

// VPTERNLOGQ performs "Bitwise Ternary Logical Operation on Quadword Values".
//
// Mnemonic        : VPTERNLOGQ
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPTERNLOGQ imm8, m128/m64bcst, xmm, xmm{k}{z}
//    * VPTERNLOGQ imm8, xmm, xmm, xmm{k}{z}
//    * VPTERNLOGQ imm8, m256/m64bcst, ymm, ymm{k}{z}
//    * VPTERNLOGQ imm8, ymm, ymm, ymm{k}{z}
//    * VPTERNLOGQ imm8, m512/m64bcst, zmm, zmm{k}{z}
//    * VPTERNLOGQ imm8, zmm, zmm, zmm{k}{z}
//
func (self *Program) VPTERNLOGQ(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VPTERNLOGQ imm8, m128/m64bcst, xmm, xmm{k}{z}
    if isImm8(v0) && isM128M64bcst(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0x25)
            m.mrsd(lcode(v[3]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPTERNLOGQ imm8, xmm, xmm, xmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x00)
            m.emit(0x25)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPTERNLOGQ imm8, m256/m64bcst, ymm, ymm{k}{z}
    if isImm8(v0) && isM256M64bcst(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b01, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0x25)
            m.mrsd(lcode(v[3]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPTERNLOGQ imm8, ymm, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x20)
            m.emit(0x25)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPTERNLOGQ imm8, m512/m64bcst, zmm, zmm{k}{z}
    if isImm8(v0) && isM512M64bcst(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0x25)
            m.mrsd(lcode(v[3]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VPTERNLOGQ imm8, zmm, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x25)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPTERNLOGQ")
    }
    return p
}

// VPTEST performs "Packed Logical Compare".
//
// Mnemonic        : VPTEST
// ISA extensions  : AVX
// Supported forms : (4 forms)
//
//    * VPTEST xmm, xmm
//    * VPTEST m128, xmm
//    * VPTEST ymm, ymm
//    * VPTEST m256, ymm
//
func (self *Program) VPTEST(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VPTEST xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79)
            m.emit(0x17)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPTEST m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x17)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VPTEST ymm, ymm
    if isYMM(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d)
            m.emit(0x17)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VPTEST m256, ymm
    if isM256(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x17)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPTEST")
    }
    return p
}

// VPTESTMB performs "Logical AND of Packed Byte Integer Values and Set Mask".
//
// Mnemonic        : VPTESTMB
// ISA extensions  : AVX512BW, AVX512VL
// Supported forms : (6 forms)
//
//    * VPTESTMB xmm, xmm, k{k}
//    * VPTESTMB m128, xmm, k{k}
//    * VPTESTMB ymm, ymm, k{k}
//    * VPTESTMB m256, ymm, k{k}
//    * VPTESTMB zmm, zmm, k{k}
//    * VPTESTMB m512, zmm, k{k}
//
func (self *Program) VPTESTMB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPTESTMB xmm, xmm, k{k}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x26)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPTESTMB m128, xmm, k{k}
    if isM128(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, 0)
            m.emit(0x26)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPTESTMB ymm, ymm, k{k}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x26)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPTESTMB m256, ymm, k{k}
    if isM256(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, 0)
            m.emit(0x26)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPTESTMB zmm, zmm, k{k}
    if isZMM(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x26)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPTESTMB m512, zmm, k{k}
    if isM512(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, 0)
            m.emit(0x26)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPTESTMB")
    }
    return p
}

// VPTESTMD performs "Logical AND of Packed Doubleword Integer Values and Set Mask".
//
// Mnemonic        : VPTESTMD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPTESTMD m128/m32bcst, xmm, k{k}
//    * VPTESTMD xmm, xmm, k{k}
//    * VPTESTMD m256/m32bcst, ymm, k{k}
//    * VPTESTMD ymm, ymm, k{k}
//    * VPTESTMD m512/m32bcst, zmm, k{k}
//    * VPTESTMD zmm, zmm, k{k}
//
func (self *Program) VPTESTMD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPTESTMD m128/m32bcst, xmm, k{k}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, bcode(v[0]))
            m.emit(0x27)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPTESTMD xmm, xmm, k{k}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x27)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPTESTMD m256/m32bcst, ymm, k{k}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, bcode(v[0]))
            m.emit(0x27)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPTESTMD ymm, ymm, k{k}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x27)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPTESTMD m512/m32bcst, zmm, k{k}
    if isM512M32bcst(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, bcode(v[0]))
            m.emit(0x27)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPTESTMD zmm, zmm, k{k}
    if isZMM(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x27)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPTESTMD")
    }
    return p
}

// VPTESTMQ performs "Logical AND of Packed Quadword Integer Values and Set Mask".
//
// Mnemonic        : VPTESTMQ
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPTESTMQ m128/m64bcst, xmm, k{k}
//    * VPTESTMQ xmm, xmm, k{k}
//    * VPTESTMQ m256/m64bcst, ymm, k{k}
//    * VPTESTMQ ymm, ymm, k{k}
//    * VPTESTMQ m512/m64bcst, zmm, k{k}
//    * VPTESTMQ zmm, zmm, k{k}
//
func (self *Program) VPTESTMQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPTESTMQ m128/m64bcst, xmm, k{k}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, bcode(v[0]))
            m.emit(0x27)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPTESTMQ xmm, xmm, k{k}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x27)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPTESTMQ m256/m64bcst, ymm, k{k}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, bcode(v[0]))
            m.emit(0x27)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPTESTMQ ymm, ymm, k{k}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x27)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPTESTMQ m512/m64bcst, zmm, k{k}
    if isM512M64bcst(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, bcode(v[0]))
            m.emit(0x27)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPTESTMQ zmm, zmm, k{k}
    if isZMM(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x27)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPTESTMQ")
    }
    return p
}

// VPTESTMW performs "Logical AND of Packed Word Integer Values and Set Mask".
//
// Mnemonic        : VPTESTMW
// ISA extensions  : AVX512BW, AVX512VL
// Supported forms : (6 forms)
//
//    * VPTESTMW xmm, xmm, k{k}
//    * VPTESTMW m128, xmm, k{k}
//    * VPTESTMW ymm, ymm, k{k}
//    * VPTESTMW m256, ymm, k{k}
//    * VPTESTMW zmm, zmm, k{k}
//    * VPTESTMW m512, zmm, k{k}
//
func (self *Program) VPTESTMW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPTESTMW xmm, xmm, k{k}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x26)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPTESTMW m128, xmm, k{k}
    if isM128(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, 0)
            m.emit(0x26)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPTESTMW ymm, ymm, k{k}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x26)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPTESTMW m256, ymm, k{k}
    if isM256(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, 0)
            m.emit(0x26)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPTESTMW zmm, zmm, k{k}
    if isZMM(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x26)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPTESTMW m512, zmm, k{k}
    if isM512(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, 0)
            m.emit(0x26)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPTESTMW")
    }
    return p
}

// VPTESTNMB performs "Logical NAND of Packed Byte Integer Values and Set Mask".
//
// Mnemonic        : VPTESTNMB
// ISA extensions  : AVX512BW, AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPTESTNMB xmm, xmm, k{k}
//    * VPTESTNMB m128, xmm, k{k}
//    * VPTESTNMB ymm, ymm, k{k}
//    * VPTESTNMB m256, ymm, k{k}
//    * VPTESTNMB zmm, zmm, k{k}
//    * VPTESTNMB m512, zmm, k{k}
//
func (self *Program) VPTESTNMB(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPTESTNMB xmm, xmm, k{k}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7e ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x26)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPTESTNMB m128, xmm, k{k}
    if isM128(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, 0)
            m.emit(0x26)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPTESTNMB ymm, ymm, k{k}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7e ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x26)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPTESTNMB m256, ymm, k{k}
    if isM256(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, 0)
            m.emit(0x26)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPTESTNMB zmm, zmm, k{k}
    if isZMM(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512F | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7e ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x26)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPTESTNMB m512, zmm, k{k}
    if isM512(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512F | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, 0)
            m.emit(0x26)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPTESTNMB")
    }
    return p
}

// VPTESTNMD performs "Logical NAND of Packed Doubleword Integer Values and Set Mask".
//
// Mnemonic        : VPTESTNMD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPTESTNMD m128/m32bcst, xmm, k{k}
//    * VPTESTNMD xmm, xmm, k{k}
//    * VPTESTNMD m256/m32bcst, ymm, k{k}
//    * VPTESTNMD ymm, ymm, k{k}
//    * VPTESTNMD m512/m32bcst, zmm, k{k}
//    * VPTESTNMD zmm, zmm, k{k}
//
func (self *Program) VPTESTNMD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPTESTNMD m128/m32bcst, xmm, k{k}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, bcode(v[0]))
            m.emit(0x27)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPTESTNMD xmm, xmm, k{k}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7e ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x27)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPTESTNMD m256/m32bcst, ymm, k{k}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, bcode(v[0]))
            m.emit(0x27)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPTESTNMD ymm, ymm, k{k}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7e ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x27)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPTESTNMD m512/m32bcst, zmm, k{k}
    if isM512M32bcst(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x06, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, bcode(v[0]))
            m.emit(0x27)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPTESTNMD zmm, zmm, k{k}
    if isZMM(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7e ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x27)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPTESTNMD")
    }
    return p
}

// VPTESTNMQ performs "Logical NAND of Packed Quadword Integer Values and Set Mask".
//
// Mnemonic        : VPTESTNMQ
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPTESTNMQ m128/m64bcst, xmm, k{k}
//    * VPTESTNMQ xmm, xmm, k{k}
//    * VPTESTNMQ m256/m64bcst, ymm, k{k}
//    * VPTESTNMQ ymm, ymm, k{k}
//    * VPTESTNMQ m512/m64bcst, zmm, k{k}
//    * VPTESTNMQ zmm, zmm, k{k}
//
func (self *Program) VPTESTNMQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPTESTNMQ m128/m64bcst, xmm, k{k}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x86, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, bcode(v[0]))
            m.emit(0x27)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPTESTNMQ xmm, xmm, k{k}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfe ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x27)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPTESTNMQ m256/m64bcst, ymm, k{k}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x86, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, bcode(v[0]))
            m.emit(0x27)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPTESTNMQ ymm, ymm, k{k}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfe ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x27)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPTESTNMQ m512/m64bcst, zmm, k{k}
    if isM512M64bcst(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x86, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, bcode(v[0]))
            m.emit(0x27)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPTESTNMQ zmm, zmm, k{k}
    if isZMM(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfe ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x27)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPTESTNMQ")
    }
    return p
}

// VPTESTNMW performs "Logical NAND of Packed Word Integer Values and Set Mask".
//
// Mnemonic        : VPTESTNMW
// ISA extensions  : AVX512BW, AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPTESTNMW xmm, xmm, k{k}
//    * VPTESTNMW m128, xmm, k{k}
//    * VPTESTNMW ymm, ymm, k{k}
//    * VPTESTNMW m256, ymm, k{k}
//    * VPTESTNMW zmm, zmm, k{k}
//    * VPTESTNMW m512, zmm, k{k}
//
func (self *Program) VPTESTNMW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPTESTNMW xmm, xmm, k{k}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfe ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x26)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPTESTNMW m128, xmm, k{k}
    if isM128(v0) && isEVEXXMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x86, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, 0)
            m.emit(0x26)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPTESTNMW ymm, ymm, k{k}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfe ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x26)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPTESTNMW m256, ymm, k{k}
    if isM256(v0) && isEVEXYMM(v1) && isKk(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x86, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, 0)
            m.emit(0x26)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPTESTNMW zmm, zmm, k{k}
    if isZMM(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512F | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfe ^ (hlcode(v[1]) << 3))
            m.emit((0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x26)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPTESTNMW m512, zmm, k{k}
    if isM512(v0) && isZMM(v1) && isKk(v2) {
        self.require(ISA_AVX512F | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x86, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), 0, 0)
            m.emit(0x26)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPTESTNMW")
    }
    return p
}

// VPUNPCKHBW performs "Unpack and Interleave High-Order Bytes into Words".
//
// Mnemonic        : VPUNPCKHBW
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPUNPCKHBW xmm, xmm, xmm{k}{z}
//    * VPUNPCKHBW m128, xmm, xmm{k}{z}
//    * VPUNPCKHBW ymm, ymm, ymm{k}{z}
//    * VPUNPCKHBW m256, ymm, ymm{k}{z}
//    * VPUNPCKHBW zmm, zmm, zmm{k}{z}
//    * VPUNPCKHBW m512, zmm, zmm{k}{z}
//    * VPUNPCKHBW xmm, xmm, xmm
//    * VPUNPCKHBW m128, xmm, xmm
//    * VPUNPCKHBW ymm, ymm, ymm
//    * VPUNPCKHBW m256, ymm, ymm
//
func (self *Program) VPUNPCKHBW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPUNPCKHBW xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x68)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKHBW m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x68)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPUNPCKHBW ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x68)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKHBW m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x68)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPUNPCKHBW zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x68)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKHBW m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x68)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPUNPCKHBW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x68)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKHBW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x68)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPUNPCKHBW ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x68)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKHBW m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x68)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPUNPCKHBW")
    }
    return p
}

// VPUNPCKHDQ performs "Unpack and Interleave High-Order Doublewords into Quadwords".
//
// Mnemonic        : VPUNPCKHDQ
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPUNPCKHDQ m128/m32bcst, xmm, xmm{k}{z}
//    * VPUNPCKHDQ xmm, xmm, xmm{k}{z}
//    * VPUNPCKHDQ m256/m32bcst, ymm, ymm{k}{z}
//    * VPUNPCKHDQ ymm, ymm, ymm{k}{z}
//    * VPUNPCKHDQ m512/m32bcst, zmm, zmm{k}{z}
//    * VPUNPCKHDQ zmm, zmm, zmm{k}{z}
//    * VPUNPCKHDQ xmm, xmm, xmm
//    * VPUNPCKHDQ m128, xmm, xmm
//    * VPUNPCKHDQ ymm, ymm, ymm
//    * VPUNPCKHDQ m256, ymm, ymm
//
func (self *Program) VPUNPCKHDQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPUNPCKHDQ m128/m32bcst, xmm, xmm{k}{z}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x6a)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPUNPCKHDQ xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x6a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKHDQ m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x6a)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPUNPCKHDQ ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x6a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKHDQ m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x6a)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPUNPCKHDQ zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x6a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKHDQ xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x6a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKHDQ m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x6a)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPUNPCKHDQ ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x6a)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKHDQ m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x6a)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPUNPCKHDQ")
    }
    return p
}

// VPUNPCKHQDQ performs "Unpack and Interleave High-Order Quadwords into Double Quadwords".
//
// Mnemonic        : VPUNPCKHQDQ
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPUNPCKHQDQ m128/m64bcst, xmm, xmm{k}{z}
//    * VPUNPCKHQDQ xmm, xmm, xmm{k}{z}
//    * VPUNPCKHQDQ m256/m64bcst, ymm, ymm{k}{z}
//    * VPUNPCKHQDQ ymm, ymm, ymm{k}{z}
//    * VPUNPCKHQDQ m512/m64bcst, zmm, zmm{k}{z}
//    * VPUNPCKHQDQ zmm, zmm, zmm{k}{z}
//    * VPUNPCKHQDQ xmm, xmm, xmm
//    * VPUNPCKHQDQ m128, xmm, xmm
//    * VPUNPCKHQDQ ymm, ymm, ymm
//    * VPUNPCKHQDQ m256, ymm, ymm
//
func (self *Program) VPUNPCKHQDQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPUNPCKHQDQ m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x6d)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPUNPCKHQDQ xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x6d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKHQDQ m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x6d)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPUNPCKHQDQ ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x6d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKHQDQ m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x6d)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPUNPCKHQDQ zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x6d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKHQDQ xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x6d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKHQDQ m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x6d)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPUNPCKHQDQ ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x6d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKHQDQ m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x6d)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPUNPCKHQDQ")
    }
    return p
}

// VPUNPCKHWD performs "Unpack and Interleave High-Order Words into Doublewords".
//
// Mnemonic        : VPUNPCKHWD
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPUNPCKHWD xmm, xmm, xmm{k}{z}
//    * VPUNPCKHWD m128, xmm, xmm{k}{z}
//    * VPUNPCKHWD ymm, ymm, ymm{k}{z}
//    * VPUNPCKHWD m256, ymm, ymm{k}{z}
//    * VPUNPCKHWD zmm, zmm, zmm{k}{z}
//    * VPUNPCKHWD m512, zmm, zmm{k}{z}
//    * VPUNPCKHWD xmm, xmm, xmm
//    * VPUNPCKHWD m128, xmm, xmm
//    * VPUNPCKHWD ymm, ymm, ymm
//    * VPUNPCKHWD m256, ymm, ymm
//
func (self *Program) VPUNPCKHWD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPUNPCKHWD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x69)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKHWD m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x69)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPUNPCKHWD ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x69)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKHWD m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x69)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPUNPCKHWD zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x69)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKHWD m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x69)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPUNPCKHWD xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x69)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKHWD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x69)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPUNPCKHWD ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x69)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKHWD m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x69)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPUNPCKHWD")
    }
    return p
}

// VPUNPCKLBW performs "Unpack and Interleave Low-Order Bytes into Words".
//
// Mnemonic        : VPUNPCKLBW
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPUNPCKLBW xmm, xmm, xmm{k}{z}
//    * VPUNPCKLBW m128, xmm, xmm{k}{z}
//    * VPUNPCKLBW ymm, ymm, ymm{k}{z}
//    * VPUNPCKLBW m256, ymm, ymm{k}{z}
//    * VPUNPCKLBW zmm, zmm, zmm{k}{z}
//    * VPUNPCKLBW m512, zmm, zmm{k}{z}
//    * VPUNPCKLBW xmm, xmm, xmm
//    * VPUNPCKLBW m128, xmm, xmm
//    * VPUNPCKLBW ymm, ymm, ymm
//    * VPUNPCKLBW m256, ymm, ymm
//
func (self *Program) VPUNPCKLBW(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPUNPCKLBW xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x60)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKLBW m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x60)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPUNPCKLBW ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x60)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKLBW m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x60)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPUNPCKLBW zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x60)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKLBW m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x60)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPUNPCKLBW xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x60)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKLBW m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x60)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPUNPCKLBW ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x60)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKLBW m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x60)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPUNPCKLBW")
    }
    return p
}

// VPUNPCKLDQ performs "Unpack and Interleave Low-Order Doublewords into Quadwords".
//
// Mnemonic        : VPUNPCKLDQ
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPUNPCKLDQ m128/m32bcst, xmm, xmm{k}{z}
//    * VPUNPCKLDQ xmm, xmm, xmm{k}{z}
//    * VPUNPCKLDQ m256/m32bcst, ymm, ymm{k}{z}
//    * VPUNPCKLDQ ymm, ymm, ymm{k}{z}
//    * VPUNPCKLDQ m512/m32bcst, zmm, zmm{k}{z}
//    * VPUNPCKLDQ zmm, zmm, zmm{k}{z}
//    * VPUNPCKLDQ xmm, xmm, xmm
//    * VPUNPCKLDQ m128, xmm, xmm
//    * VPUNPCKLDQ ymm, ymm, ymm
//    * VPUNPCKLDQ m256, ymm, ymm
//
func (self *Program) VPUNPCKLDQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPUNPCKLDQ m128/m32bcst, xmm, xmm{k}{z}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x62)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPUNPCKLDQ xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x62)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKLDQ m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x62)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPUNPCKLDQ ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x62)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKLDQ m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x62)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPUNPCKLDQ zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x62)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKLDQ xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x62)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKLDQ m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x62)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPUNPCKLDQ ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x62)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKLDQ m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x62)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPUNPCKLDQ")
    }
    return p
}

// VPUNPCKLQDQ performs "Unpack and Interleave Low-Order Quadwords into Double Quadwords".
//
// Mnemonic        : VPUNPCKLQDQ
// ISA extensions  : AVX, AVX2, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VPUNPCKLQDQ m128/m64bcst, xmm, xmm{k}{z}
//    * VPUNPCKLQDQ xmm, xmm, xmm{k}{z}
//    * VPUNPCKLQDQ m256/m64bcst, ymm, ymm{k}{z}
//    * VPUNPCKLQDQ ymm, ymm, ymm{k}{z}
//    * VPUNPCKLQDQ m512/m64bcst, zmm, zmm{k}{z}
//    * VPUNPCKLQDQ zmm, zmm, zmm{k}{z}
//    * VPUNPCKLQDQ xmm, xmm, xmm
//    * VPUNPCKLQDQ m128, xmm, xmm
//    * VPUNPCKLQDQ ymm, ymm, ymm
//    * VPUNPCKLQDQ m256, ymm, ymm
//
func (self *Program) VPUNPCKLQDQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPUNPCKLQDQ m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x6c)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPUNPCKLQDQ xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x6c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKLQDQ m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x6c)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPUNPCKLQDQ ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x6c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKLQDQ m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x6c)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPUNPCKLQDQ zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x6c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKLQDQ xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x6c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKLQDQ m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x6c)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPUNPCKLQDQ ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x6c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKLQDQ m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x6c)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPUNPCKLQDQ")
    }
    return p
}

// VPUNPCKLWD performs "Unpack and Interleave Low-Order Words into Doublewords".
//
// Mnemonic        : VPUNPCKLWD
// ISA extensions  : AVX, AVX2, AVX512BW, AVX512VL
// Supported forms : (10 forms)
//
//    * VPUNPCKLWD xmm, xmm, xmm{k}{z}
//    * VPUNPCKLWD m128, xmm, xmm{k}{z}
//    * VPUNPCKLWD ymm, ymm, ymm{k}{z}
//    * VPUNPCKLWD m256, ymm, ymm{k}{z}
//    * VPUNPCKLWD zmm, zmm, zmm{k}{z}
//    * VPUNPCKLWD m512, zmm, zmm{k}{z}
//    * VPUNPCKLWD xmm, xmm, xmm
//    * VPUNPCKLWD m128, xmm, xmm
//    * VPUNPCKLWD ymm, ymm, ymm
//    * VPUNPCKLWD m256, ymm, ymm
//
func (self *Program) VPUNPCKLWD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPUNPCKLWD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x61)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKLWD m128, xmm, xmm{k}{z}
    if isM128(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x61)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPUNPCKLWD ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x61)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKLWD m256, ymm, ymm{k}{z}
    if isM256(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x61)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPUNPCKLWD zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x61)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKLWD m512, zmm, zmm{k}{z}
    if isM512(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512BW)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x61)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPUNPCKLWD xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x61)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKLWD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x61)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPUNPCKLWD ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x61)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPUNPCKLWD m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x61)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPUNPCKLWD")
    }
    return p
}

// VPXOR performs "Packed Bitwise Logical Exclusive OR".
//
// Mnemonic        : VPXOR
// ISA extensions  : AVX, AVX2
// Supported forms : (4 forms)
//
//    * VPXOR xmm, xmm, xmm
//    * VPXOR m128, xmm, xmm
//    * VPXOR ymm, ymm, ymm
//    * VPXOR m256, ymm, ymm
//
func (self *Program) VPXOR(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPXOR xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xef)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPXOR m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xef)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VPXOR ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0xef)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPXOR m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0xef)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPXOR")
    }
    return p
}

// VPXORD performs "Bitwise Logical Exclusive OR of Packed Doubleword Integers".
//
// Mnemonic        : VPXORD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPXORD m128/m32bcst, xmm, xmm{k}{z}
//    * VPXORD xmm, xmm, xmm{k}{z}
//    * VPXORD m256/m32bcst, ymm, ymm{k}{z}
//    * VPXORD ymm, ymm, ymm{k}{z}
//    * VPXORD m512/m32bcst, zmm, zmm{k}{z}
//    * VPXORD zmm, zmm, zmm{k}{z}
//
func (self *Program) VPXORD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPXORD m128/m32bcst, xmm, xmm{k}{z}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xef)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPXORD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xef)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPXORD m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xef)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPXORD ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xef)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPXORD m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xef)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPXORD zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xef)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPXORD")
    }
    return p
}

// VPXORQ performs "Bitwise Logical Exclusive OR of Packed Quadword Integers".
//
// Mnemonic        : VPXORQ
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VPXORQ m128/m64bcst, xmm, xmm{k}{z}
//    * VPXORQ xmm, xmm, xmm{k}{z}
//    * VPXORQ m256/m64bcst, ymm, ymm{k}{z}
//    * VPXORQ ymm, ymm, ymm{k}{z}
//    * VPXORQ m512/m64bcst, zmm, zmm{k}{z}
//    * VPXORQ zmm, zmm, zmm{k}{z}
//
func (self *Program) VPXORQ(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VPXORQ m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xef)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VPXORQ xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0xef)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPXORQ m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xef)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VPXORQ ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0xef)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VPXORQ m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0xef)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VPXORQ zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xef)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VPXORQ")
    }
    return p
}

// VRANGEPD performs "Range Restriction Calculation For Packed Pairs of Double-Precision Floating-Point Values".
//
// Mnemonic        : VRANGEPD
// ISA extensions  : AVX512DQ, AVX512VL
// Supported forms : (7 forms)
//
//    * VRANGEPD imm8, m128/m64bcst, xmm, xmm{k}{z}
//    * VRANGEPD imm8, xmm, xmm, xmm{k}{z}
//    * VRANGEPD imm8, m256/m64bcst, ymm, ymm{k}{z}
//    * VRANGEPD imm8, ymm, ymm, ymm{k}{z}
//    * VRANGEPD imm8, m512/m64bcst, zmm, zmm{k}{z}
//    * VRANGEPD imm8, {sae}, zmm, zmm, zmm{k}{z}
//    * VRANGEPD imm8, zmm, zmm, zmm{k}{z}
//
func (self *Program) VRANGEPD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(4, Operands{v0, v1, v2, v3})
        case 1  : p = self.alloc(5, Operands{v0, v1, v2, v3, vv[0]})
        default : panic("instruction VRANGEPD takes 4 or 5 operands")
    }
    // VRANGEPD imm8, m128/m64bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isM128M64bcst(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0x50)
            m.mrsd(lcode(v[3]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VRANGEPD imm8, xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x00)
            m.emit(0x50)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VRANGEPD imm8, m256/m64bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isM256M64bcst(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b01, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0x50)
            m.mrsd(lcode(v[3]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VRANGEPD imm8, ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isEVEXYMM(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x20)
            m.emit(0x50)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VRANGEPD imm8, m512/m64bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isM512M64bcst(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0x50)
            m.mrsd(lcode(v[3]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VRANGEPD imm8, {sae}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isImm8(v0) && isSAE(v1) && isZMM(v2) && isZMM(v3) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[4]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[4]) << 4)))
            m.emit(0xfd ^ (hlcode(v[3]) << 3))
            m.emit((zcode(v[4]) << 7) | (0x08 ^ (ecode(v[3]) << 3)) | kcode(v[4]) | 0x10)
            m.emit(0x50)
            m.emit(0xc0 | lcode(v[4]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VRANGEPD imm8, zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x50)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VRANGEPD")
    }
    return p
}

// VRANGEPS performs "Range Restriction Calculation For Packed Pairs of Single-Precision Floating-Point Values".
//
// Mnemonic        : VRANGEPS
// ISA extensions  : AVX512DQ, AVX512VL
// Supported forms : (7 forms)
//
//    * VRANGEPS imm8, m128/m32bcst, xmm, xmm{k}{z}
//    * VRANGEPS imm8, xmm, xmm, xmm{k}{z}
//    * VRANGEPS imm8, m256/m32bcst, ymm, ymm{k}{z}
//    * VRANGEPS imm8, ymm, ymm, ymm{k}{z}
//    * VRANGEPS imm8, m512/m32bcst, zmm, zmm{k}{z}
//    * VRANGEPS imm8, {sae}, zmm, zmm, zmm{k}{z}
//    * VRANGEPS imm8, zmm, zmm, zmm{k}{z}
//
func (self *Program) VRANGEPS(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(4, Operands{v0, v1, v2, v3})
        case 1  : p = self.alloc(5, Operands{v0, v1, v2, v3, vv[0]})
        default : panic("instruction VRANGEPS takes 4 or 5 operands")
    }
    // VRANGEPS imm8, m128/m32bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isM128M32bcst(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0x50)
            m.mrsd(lcode(v[3]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VRANGEPS imm8, xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x00)
            m.emit(0x50)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VRANGEPS imm8, m256/m32bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isM256M32bcst(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b01, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0x50)
            m.mrsd(lcode(v[3]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VRANGEPS imm8, ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isEVEXYMM(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x20)
            m.emit(0x50)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VRANGEPS imm8, m512/m32bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isM512M32bcst(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0x50)
            m.mrsd(lcode(v[3]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VRANGEPS imm8, {sae}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isImm8(v0) && isSAE(v1) && isZMM(v2) && isZMM(v3) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[4]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[4]) << 4)))
            m.emit(0x7d ^ (hlcode(v[3]) << 3))
            m.emit((zcode(v[4]) << 7) | (0x08 ^ (ecode(v[3]) << 3)) | kcode(v[4]) | 0x10)
            m.emit(0x50)
            m.emit(0xc0 | lcode(v[4]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VRANGEPS imm8, zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x50)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VRANGEPS")
    }
    return p
}

// VRANGESD performs "Range Restriction Calculation For a pair of Scalar Double-Precision Floating-Point Values".
//
// Mnemonic        : VRANGESD
// ISA extensions  : AVX512DQ
// Supported forms : (3 forms)
//
//    * VRANGESD imm8, m64, xmm, xmm{k}{z}
//    * VRANGESD imm8, {sae}, xmm, xmm, xmm{k}{z}
//    * VRANGESD imm8, xmm, xmm, xmm{k}{z}
//
func (self *Program) VRANGESD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(4, Operands{v0, v1, v2, v3})
        case 1  : p = self.alloc(5, Operands{v0, v1, v2, v3, vv[0]})
        default : panic("instruction VRANGESD takes 4 or 5 operands")
    }
    // VRANGESD imm8, m64, xmm, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isM64(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), 0)
            m.emit(0x51)
            m.mrsd(lcode(v[3]), addr(v[1]), 8)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VRANGESD imm8, {sae}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isImm8(v0) && isSAE(v1) && isEVEXXMM(v2) && isEVEXXMM(v3) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[4]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[4]) << 4)))
            m.emit(0xfd ^ (hlcode(v[3]) << 3))
            m.emit((zcode(v[4]) << 7) | (0x08 ^ (ecode(v[3]) << 3)) | kcode(v[4]) | 0x10)
            m.emit(0x51)
            m.emit(0xc0 | lcode(v[4]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VRANGESD imm8, xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x51)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VRANGESD")
    }
    return p
}

// VRANGESS performs "Range Restriction Calculation For a pair of Scalar Single-Precision Floating-Point Values".
//
// Mnemonic        : VRANGESS
// ISA extensions  : AVX512DQ
// Supported forms : (3 forms)
//
//    * VRANGESS imm8, m32, xmm, xmm{k}{z}
//    * VRANGESS imm8, {sae}, xmm, xmm, xmm{k}{z}
//    * VRANGESS imm8, xmm, xmm, xmm{k}{z}
//
func (self *Program) VRANGESS(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(4, Operands{v0, v1, v2, v3})
        case 1  : p = self.alloc(5, Operands{v0, v1, v2, v3, vv[0]})
        default : panic("instruction VRANGESS takes 4 or 5 operands")
    }
    // VRANGESS imm8, m32, xmm, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isM32(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), 0)
            m.emit(0x51)
            m.mrsd(lcode(v[3]), addr(v[1]), 4)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VRANGESS imm8, {sae}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isImm8(v0) && isSAE(v1) && isEVEXXMM(v2) && isEVEXXMM(v3) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[4]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[4]) << 4)))
            m.emit(0x7d ^ (hlcode(v[3]) << 3))
            m.emit((zcode(v[4]) << 7) | (0x08 ^ (ecode(v[3]) << 3)) | kcode(v[4]) | 0x10)
            m.emit(0x51)
            m.emit(0xc0 | lcode(v[4]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VRANGESS imm8, xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x51)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VRANGESS")
    }
    return p
}

// VRCP14PD performs "Compute Approximate Reciprocals of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VRCP14PD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VRCP14PD m128/m64bcst, xmm{k}{z}
//    * VRCP14PD m256/m64bcst, ymm{k}{z}
//    * VRCP14PD m512/m64bcst, zmm{k}{z}
//    * VRCP14PD xmm, xmm{k}{z}
//    * VRCP14PD ymm, ymm{k}{z}
//    * VRCP14PD zmm, zmm{k}{z}
//
func (self *Program) VRCP14PD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VRCP14PD m128/m64bcst, xmm{k}{z}
    if isM128M64bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x4c)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VRCP14PD m256/m64bcst, ymm{k}{z}
    if isM256M64bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x4c)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VRCP14PD m512/m64bcst, zmm{k}{z}
    if isM512M64bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x4c)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VRCP14PD xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x4c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VRCP14PD ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x4c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VRCP14PD zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x4c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VRCP14PD")
    }
    return p
}

// VRCP14PS performs "Compute Approximate Reciprocals of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VRCP14PS
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VRCP14PS m128/m32bcst, xmm{k}{z}
//    * VRCP14PS m256/m32bcst, ymm{k}{z}
//    * VRCP14PS m512/m32bcst, zmm{k}{z}
//    * VRCP14PS xmm, xmm{k}{z}
//    * VRCP14PS ymm, ymm{k}{z}
//    * VRCP14PS zmm, zmm{k}{z}
//
func (self *Program) VRCP14PS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VRCP14PS m128/m32bcst, xmm{k}{z}
    if isM128M32bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x4c)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VRCP14PS m256/m32bcst, ymm{k}{z}
    if isM256M32bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x4c)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VRCP14PS m512/m32bcst, zmm{k}{z}
    if isM512M32bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x4c)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VRCP14PS xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x4c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VRCP14PS ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x4c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VRCP14PS zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x4c)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VRCP14PS")
    }
    return p
}

// VRCP14SD performs "Compute Approximate Reciprocal of a Scalar Double-Precision Floating-Point Value".
//
// Mnemonic        : VRCP14SD
// ISA extensions  : AVX512F
// Supported forms : (2 forms)
//
//    * VRCP14SD xmm, xmm, xmm{k}{z}
//    * VRCP14SD m64, xmm, xmm{k}{z}
//
func (self *Program) VRCP14SD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VRCP14SD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x4d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VRCP14SD m64, xmm, xmm{k}{z}
    if isM64(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x4d)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VRCP14SD")
    }
    return p
}

// VRCP14SS performs "Compute Approximate Reciprocal of a Scalar Single-Precision Floating-Point Value".
//
// Mnemonic        : VRCP14SS
// ISA extensions  : AVX512F
// Supported forms : (2 forms)
//
//    * VRCP14SS xmm, xmm, xmm{k}{z}
//    * VRCP14SS m32, xmm, xmm{k}{z}
//
func (self *Program) VRCP14SS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VRCP14SS xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x4d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VRCP14SS m32, xmm, xmm{k}{z}
    if isM32(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x4d)
            m.mrsd(lcode(v[2]), addr(v[0]), 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VRCP14SS")
    }
    return p
}

// VRCP28PD performs "Approximation to the Reciprocal of Packed Double-Precision Floating-Point Values with Less Than 2^-28 Relative Error".
//
// Mnemonic        : VRCP28PD
// ISA extensions  : AVX512ER
// Supported forms : (3 forms)
//
//    * VRCP28PD m512/m64bcst, zmm{k}{z}
//    * VRCP28PD {sae}, zmm, zmm{k}{z}
//    * VRCP28PD zmm, zmm{k}{z}
//
func (self *Program) VRCP28PD(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VRCP28PD takes 2 or 3 operands")
    }
    // VRCP28PD m512/m64bcst, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512ER)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0xca)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VRCP28PD {sae}, zmm, zmm{k}{z}
    if len(vv) == 1 && isSAE(v0) && isZMM(v1) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512ER)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x18)
            m.emit(0xca)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VRCP28PD zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512ER)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0xca)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VRCP28PD")
    }
    return p
}

// VRCP28PS performs "Approximation to the Reciprocal of Packed Single-Precision Floating-Point Values with Less Than 2^-28 Relative Error".
//
// Mnemonic        : VRCP28PS
// ISA extensions  : AVX512ER
// Supported forms : (3 forms)
//
//    * VRCP28PS m512/m32bcst, zmm{k}{z}
//    * VRCP28PS {sae}, zmm, zmm{k}{z}
//    * VRCP28PS zmm, zmm{k}{z}
//
func (self *Program) VRCP28PS(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VRCP28PS takes 2 or 3 operands")
    }
    // VRCP28PS m512/m32bcst, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512ER)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0xca)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VRCP28PS {sae}, zmm, zmm{k}{z}
    if len(vv) == 1 && isSAE(v0) && isZMM(v1) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512ER)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x18)
            m.emit(0xca)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VRCP28PS zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512ER)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0xca)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VRCP28PS")
    }
    return p
}

// VRCP28SD performs "Approximation to the Reciprocal of a Scalar Double-Precision Floating-Point Value with Less Than 2^-28 Relative Error".
//
// Mnemonic        : VRCP28SD
// ISA extensions  : AVX512ER
// Supported forms : (3 forms)
//
//    * VRCP28SD m64, xmm, xmm{k}{z}
//    * VRCP28SD {sae}, xmm, xmm, xmm{k}{z}
//    * VRCP28SD xmm, xmm, xmm{k}{z}
//
func (self *Program) VRCP28SD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VRCP28SD takes 3 or 4 operands")
    }
    // VRCP28SD m64, xmm, xmm{k}{z}
    if len(vv) == 0 && isM64(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512ER)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xcb)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    // VRCP28SD {sae}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isSAE(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512ER)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xcb)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VRCP28SD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512ER)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xcb)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VRCP28SD")
    }
    return p
}

// VRCP28SS performs "Approximation to the Reciprocal of a Scalar Single-Precision Floating-Point Value with Less Than 2^-28 Relative Error".
//
// Mnemonic        : VRCP28SS
// ISA extensions  : AVX512ER
// Supported forms : (3 forms)
//
//    * VRCP28SS m32, xmm, xmm{k}{z}
//    * VRCP28SS {sae}, xmm, xmm, xmm{k}{z}
//    * VRCP28SS xmm, xmm, xmm{k}{z}
//
func (self *Program) VRCP28SS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VRCP28SS takes 3 or 4 operands")
    }
    // VRCP28SS m32, xmm, xmm{k}{z}
    if len(vv) == 0 && isM32(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512ER)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xcb)
            m.mrsd(lcode(v[2]), addr(v[0]), 4)
        })
    }
    // VRCP28SS {sae}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isSAE(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512ER)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xcb)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VRCP28SS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512ER)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xcb)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VRCP28SS")
    }
    return p
}

// VRCPPS performs "Compute Approximate Reciprocals of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VRCPPS
// ISA extensions  : AVX
// Supported forms : (4 forms)
//
//    * VRCPPS xmm, xmm
//    * VRCPPS m128, xmm
//    * VRCPPS ymm, ymm
//    * VRCPPS m256, ymm
//
func (self *Program) VRCPPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VRCPPS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[1]), v[0], 0)
            m.emit(0x53)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VRCPPS m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x53)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VRCPPS ymm, ymm
    if isYMM(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[1]), v[0], 0)
            m.emit(0x53)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VRCPPS m256, ymm
    if isM256(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x53)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VRCPPS")
    }
    return p
}

// VRCPSS performs "Compute Approximate Reciprocal of Scalar Single-Precision Floating-Point Values".
//
// Mnemonic        : VRCPSS
// ISA extensions  : AVX
// Supported forms : (2 forms)
//
//    * VRCPSS xmm, xmm, xmm
//    * VRCPSS m32, xmm, xmm
//
func (self *Program) VRCPSS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VRCPSS xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x53)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VRCPSS m32, xmm, xmm
    if isM32(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x53)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VRCPSS")
    }
    return p
}

// VREDUCEPD performs "Perform Reduction Transformation on Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VREDUCEPD
// ISA extensions  : AVX512DQ, AVX512VL
// Supported forms : (6 forms)
//
//    * VREDUCEPD imm8, m128/m64bcst, xmm{k}{z}
//    * VREDUCEPD imm8, m256/m64bcst, ymm{k}{z}
//    * VREDUCEPD imm8, m512/m64bcst, zmm{k}{z}
//    * VREDUCEPD imm8, xmm, xmm{k}{z}
//    * VREDUCEPD imm8, ymm, ymm{k}{z}
//    * VREDUCEPD imm8, zmm, zmm{k}{z}
//
func (self *Program) VREDUCEPD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VREDUCEPD imm8, m128/m64bcst, xmm{k}{z}
    if isImm8(v0) && isM128M64bcst(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b00, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x56)
            m.mrsd(lcode(v[2]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VREDUCEPD imm8, m256/m64bcst, ymm{k}{z}
    if isImm8(v0) && isM256M64bcst(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b01, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x56)
            m.mrsd(lcode(v[2]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VREDUCEPD imm8, m512/m64bcst, zmm{k}{z}
    if isImm8(v0) && isM512M64bcst(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b10, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x56)
            m.mrsd(lcode(v[2]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VREDUCEPD imm8, xmm, xmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x08)
            m.emit(0x56)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VREDUCEPD imm8, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x28)
            m.emit(0x56)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VREDUCEPD imm8, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x48)
            m.emit(0x56)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VREDUCEPD")
    }
    return p
}

// VREDUCEPS performs "Perform Reduction Transformation on Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VREDUCEPS
// ISA extensions  : AVX512DQ, AVX512VL
// Supported forms : (6 forms)
//
//    * VREDUCEPS imm8, m128/m32bcst, xmm{k}{z}
//    * VREDUCEPS imm8, m256/m32bcst, ymm{k}{z}
//    * VREDUCEPS imm8, m512/m32bcst, zmm{k}{z}
//    * VREDUCEPS imm8, xmm, xmm{k}{z}
//    * VREDUCEPS imm8, ymm, ymm{k}{z}
//    * VREDUCEPS imm8, zmm, zmm{k}{z}
//
func (self *Program) VREDUCEPS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VREDUCEPS imm8, m128/m32bcst, xmm{k}{z}
    if isImm8(v0) && isM128M32bcst(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b00, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x56)
            m.mrsd(lcode(v[2]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VREDUCEPS imm8, m256/m32bcst, ymm{k}{z}
    if isImm8(v0) && isM256M32bcst(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b01, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x56)
            m.mrsd(lcode(v[2]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VREDUCEPS imm8, m512/m32bcst, zmm{k}{z}
    if isImm8(v0) && isM512M32bcst(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b10, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x56)
            m.mrsd(lcode(v[2]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VREDUCEPS imm8, xmm, xmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x08)
            m.emit(0x56)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VREDUCEPS imm8, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x28)
            m.emit(0x56)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VREDUCEPS imm8, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x48)
            m.emit(0x56)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VREDUCEPS")
    }
    return p
}

// VREDUCESD performs "Perform Reduction Transformation on a Scalar Double-Precision Floating-Point Value".
//
// Mnemonic        : VREDUCESD
// ISA extensions  : AVX512DQ
// Supported forms : (2 forms)
//
//    * VREDUCESD imm8, xmm, xmm, xmm{k}{z}
//    * VREDUCESD imm8, m64, xmm, xmm{k}{z}
//
func (self *Program) VREDUCESD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VREDUCESD imm8, xmm, xmm, xmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x00)
            m.emit(0x57)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VREDUCESD imm8, m64, xmm, xmm{k}{z}
    if isImm8(v0) && isM64(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), 0)
            m.emit(0x57)
            m.mrsd(lcode(v[3]), addr(v[1]), 8)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VREDUCESD")
    }
    return p
}

// VREDUCESS performs "Perform Reduction Transformation on a Scalar Single-Precision Floating-Point Value".
//
// Mnemonic        : VREDUCESS
// ISA extensions  : AVX512DQ
// Supported forms : (2 forms)
//
//    * VREDUCESS imm8, xmm, xmm, xmm{k}{z}
//    * VREDUCESS imm8, m32, xmm, xmm{k}{z}
//
func (self *Program) VREDUCESS(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VREDUCESS imm8, xmm, xmm, xmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x00)
            m.emit(0x57)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VREDUCESS imm8, m32, xmm, xmm{k}{z}
    if isImm8(v0) && isM32(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), 0)
            m.emit(0x57)
            m.mrsd(lcode(v[3]), addr(v[1]), 4)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VREDUCESS")
    }
    return p
}

// VRNDSCALEPD performs "Round Packed Double-Precision Floating-Point Values To Include A Given Number Of Fraction Bits".
//
// Mnemonic        : VRNDSCALEPD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (7 forms)
//
//    * VRNDSCALEPD imm8, m128/m64bcst, xmm{k}{z}
//    * VRNDSCALEPD imm8, m256/m64bcst, ymm{k}{z}
//    * VRNDSCALEPD imm8, m512/m64bcst, zmm{k}{z}
//    * VRNDSCALEPD imm8, xmm, xmm{k}{z}
//    * VRNDSCALEPD imm8, ymm, ymm{k}{z}
//    * VRNDSCALEPD imm8, {sae}, zmm, zmm{k}{z}
//    * VRNDSCALEPD imm8, zmm, zmm{k}{z}
//
func (self *Program) VRNDSCALEPD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VRNDSCALEPD takes 3 or 4 operands")
    }
    // VRNDSCALEPD imm8, m128/m64bcst, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isM128M64bcst(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b00, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x09)
            m.mrsd(lcode(v[2]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VRNDSCALEPD imm8, m256/m64bcst, ymm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isM256M64bcst(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b01, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x09)
            m.mrsd(lcode(v[2]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VRNDSCALEPD imm8, m512/m64bcst, zmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isM512M64bcst(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b10, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x09)
            m.mrsd(lcode(v[2]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VRNDSCALEPD imm8, xmm, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x08)
            m.emit(0x09)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VRNDSCALEPD imm8, ymm, ymm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x28)
            m.emit(0x09)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VRNDSCALEPD imm8, {sae}, zmm, zmm{k}{z}
    if len(vv) == 1 && isImm8(v0) && isSAE(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[3]) << 7) | kcode(v[3]) | 0x18)
            m.emit(0x09)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VRNDSCALEPD imm8, zmm, zmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x48)
            m.emit(0x09)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VRNDSCALEPD")
    }
    return p
}

// VRNDSCALEPS performs "Round Packed Single-Precision Floating-Point Values To Include A Given Number Of Fraction Bits".
//
// Mnemonic        : VRNDSCALEPS
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (7 forms)
//
//    * VRNDSCALEPS imm8, m128/m32bcst, xmm{k}{z}
//    * VRNDSCALEPS imm8, m256/m32bcst, ymm{k}{z}
//    * VRNDSCALEPS imm8, m512/m32bcst, zmm{k}{z}
//    * VRNDSCALEPS imm8, xmm, xmm{k}{z}
//    * VRNDSCALEPS imm8, ymm, ymm{k}{z}
//    * VRNDSCALEPS imm8, {sae}, zmm, zmm{k}{z}
//    * VRNDSCALEPS imm8, zmm, zmm{k}{z}
//
func (self *Program) VRNDSCALEPS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VRNDSCALEPS takes 3 or 4 operands")
    }
    // VRNDSCALEPS imm8, m128/m32bcst, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isM128M32bcst(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b00, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x08)
            m.mrsd(lcode(v[2]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VRNDSCALEPS imm8, m256/m32bcst, ymm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isM256M32bcst(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b01, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x08)
            m.mrsd(lcode(v[2]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VRNDSCALEPS imm8, m512/m32bcst, zmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isM512M32bcst(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b10, ehcode(v[2]), addr(v[1]), 0, kcode(v[2]), zcode(v[2]), bcode(v[1]))
            m.emit(0x08)
            m.mrsd(lcode(v[2]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VRNDSCALEPS imm8, xmm, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x08)
            m.emit(0x08)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VRNDSCALEPS imm8, ymm, ymm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x28)
            m.emit(0x08)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VRNDSCALEPS imm8, {sae}, zmm, zmm{k}{z}
    if len(vv) == 1 && isImm8(v0) && isSAE(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[3]) << 7) | kcode(v[3]) | 0x18)
            m.emit(0x08)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VRNDSCALEPS imm8, zmm, zmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x48)
            m.emit(0x08)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VRNDSCALEPS")
    }
    return p
}

// VRNDSCALESD performs "Round Scalar Double-Precision Floating-Point Value To Include A Given Number Of Fraction Bits".
//
// Mnemonic        : VRNDSCALESD
// ISA extensions  : AVX512F
// Supported forms : (3 forms)
//
//    * VRNDSCALESD imm8, m64, xmm, xmm{k}{z}
//    * VRNDSCALESD imm8, {sae}, xmm, xmm, xmm{k}{z}
//    * VRNDSCALESD imm8, xmm, xmm, xmm{k}{z}
//
func (self *Program) VRNDSCALESD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(4, Operands{v0, v1, v2, v3})
        case 1  : p = self.alloc(5, Operands{v0, v1, v2, v3, vv[0]})
        default : panic("instruction VRNDSCALESD takes 4 or 5 operands")
    }
    // VRNDSCALESD imm8, m64, xmm, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isM64(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), 0)
            m.emit(0x0b)
            m.mrsd(lcode(v[3]), addr(v[1]), 8)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VRNDSCALESD imm8, {sae}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isImm8(v0) && isSAE(v1) && isEVEXXMM(v2) && isEVEXXMM(v3) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[4]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[4]) << 4)))
            m.emit(0xfd ^ (hlcode(v[3]) << 3))
            m.emit((zcode(v[4]) << 7) | (0x08 ^ (ecode(v[3]) << 3)) | kcode(v[4]) | 0x10)
            m.emit(0x0b)
            m.emit(0xc0 | lcode(v[4]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VRNDSCALESD imm8, xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x0b)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VRNDSCALESD")
    }
    return p
}

// VRNDSCALESS performs "Round Scalar Single-Precision Floating-Point Value To Include A Given Number Of Fraction Bits".
//
// Mnemonic        : VRNDSCALESS
// ISA extensions  : AVX512F
// Supported forms : (3 forms)
//
//    * VRNDSCALESS imm8, m32, xmm, xmm{k}{z}
//    * VRNDSCALESS imm8, {sae}, xmm, xmm, xmm{k}{z}
//    * VRNDSCALESS imm8, xmm, xmm, xmm{k}{z}
//
func (self *Program) VRNDSCALESS(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(4, Operands{v0, v1, v2, v3})
        case 1  : p = self.alloc(5, Operands{v0, v1, v2, v3, vv[0]})
        default : panic("instruction VRNDSCALESS takes 4 or 5 operands")
    }
    // VRNDSCALESS imm8, m32, xmm, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isM32(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), 0)
            m.emit(0x0a)
            m.mrsd(lcode(v[3]), addr(v[1]), 4)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VRNDSCALESS imm8, {sae}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isImm8(v0) && isSAE(v1) && isEVEXXMM(v2) && isEVEXXMM(v3) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[4]) << 7) | (ehcode(v[2]) << 5) | (ecode(v[4]) << 4)))
            m.emit(0x7d ^ (hlcode(v[3]) << 3))
            m.emit((zcode(v[4]) << 7) | (0x08 ^ (ecode(v[3]) << 3)) | kcode(v[4]) | 0x10)
            m.emit(0x0a)
            m.emit(0xc0 | lcode(v[4]) << 3 | lcode(v[2]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VRNDSCALESS imm8, xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x0a)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VRNDSCALESS")
    }
    return p
}

// VROUNDPD performs "Round Packed Double Precision Floating-Point Values".
//
// Mnemonic        : VROUNDPD
// ISA extensions  : AVX
// Supported forms : (4 forms)
//
//    * VROUNDPD imm8, xmm, xmm
//    * VROUNDPD imm8, m128, xmm
//    * VROUNDPD imm8, ymm, ymm
//    * VROUNDPD imm8, m256, ymm
//
func (self *Program) VROUNDPD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VROUNDPD imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79)
            m.emit(0x09)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VROUNDPD imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[2]), addr(v[1]), 0)
            m.emit(0x09)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VROUNDPD imm8, ymm, ymm
    if isImm8(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7d)
            m.emit(0x09)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VROUNDPD imm8, m256, ymm
    if isImm8(v0) && isM256(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[2]), addr(v[1]), 0)
            m.emit(0x09)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VROUNDPD")
    }
    return p
}

// VROUNDPS performs "Round Packed Single Precision Floating-Point Values".
//
// Mnemonic        : VROUNDPS
// ISA extensions  : AVX
// Supported forms : (4 forms)
//
//    * VROUNDPS imm8, xmm, xmm
//    * VROUNDPS imm8, m128, xmm
//    * VROUNDPS imm8, ymm, ymm
//    * VROUNDPS imm8, m256, ymm
//
func (self *Program) VROUNDPS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VROUNDPS imm8, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79)
            m.emit(0x08)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VROUNDPS imm8, m128, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[2]), addr(v[1]), 0)
            m.emit(0x08)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VROUNDPS imm8, ymm, ymm
    if isImm8(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[2]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x7d)
            m.emit(0x08)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VROUNDPS imm8, m256, ymm
    if isImm8(v0) && isM256(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x05, hcode(v[2]), addr(v[1]), 0)
            m.emit(0x08)
            m.mrsd(lcode(v[2]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VROUNDPS")
    }
    return p
}

// VROUNDSD performs "Round Scalar Double Precision Floating-Point Values".
//
// Mnemonic        : VROUNDSD
// ISA extensions  : AVX
// Supported forms : (2 forms)
//
//    * VROUNDSD imm8, xmm, xmm, xmm
//    * VROUNDSD imm8, m64, xmm, xmm
//
func (self *Program) VROUNDSD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VROUNDSD imm8, xmm, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x0b)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VROUNDSD imm8, m64, xmm, xmm
    if isImm8(v0) && isM64(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x0b)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VROUNDSD")
    }
    return p
}

// VROUNDSS performs "Round Scalar Single Precision Floating-Point Values".
//
// Mnemonic        : VROUNDSS
// ISA extensions  : AVX
// Supported forms : (2 forms)
//
//    * VROUNDSS imm8, xmm, xmm, xmm
//    * VROUNDSS imm8, m32, xmm, xmm
//
func (self *Program) VROUNDSS(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VROUNDSS imm8, xmm, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe3 ^ (hcode(v[3]) << 7) ^ (hcode(v[1]) << 5))
            m.emit(0x79 ^ (hlcode(v[2]) << 3))
            m.emit(0x0a)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VROUNDSS imm8, m32, xmm, xmm
    if isImm8(v0) && isM32(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b11, 0x01, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0x0a)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VROUNDSS")
    }
    return p
}

// VRSQRT14PD performs "Compute Approximate Reciprocals of Square Roots of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VRSQRT14PD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VRSQRT14PD m128/m64bcst, xmm{k}{z}
//    * VRSQRT14PD m256/m64bcst, ymm{k}{z}
//    * VRSQRT14PD m512/m64bcst, zmm{k}{z}
//    * VRSQRT14PD xmm, xmm{k}{z}
//    * VRSQRT14PD ymm, ymm{k}{z}
//    * VRSQRT14PD zmm, zmm{k}{z}
//
func (self *Program) VRSQRT14PD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VRSQRT14PD m128/m64bcst, xmm{k}{z}
    if isM128M64bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x4e)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VRSQRT14PD m256/m64bcst, ymm{k}{z}
    if isM256M64bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x4e)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VRSQRT14PD m512/m64bcst, zmm{k}{z}
    if isM512M64bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x4e)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VRSQRT14PD xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x4e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VRSQRT14PD ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x4e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VRSQRT14PD zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x4e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VRSQRT14PD")
    }
    return p
}

// VRSQRT14PS performs "Compute Approximate Reciprocals of Square Roots of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VRSQRT14PS
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (6 forms)
//
//    * VRSQRT14PS m128/m32bcst, xmm{k}{z}
//    * VRSQRT14PS m256/m32bcst, ymm{k}{z}
//    * VRSQRT14PS m512/m32bcst, zmm{k}{z}
//    * VRSQRT14PS xmm, xmm{k}{z}
//    * VRSQRT14PS ymm, ymm{k}{z}
//    * VRSQRT14PS zmm, zmm{k}{z}
//
func (self *Program) VRSQRT14PS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VRSQRT14PS m128/m32bcst, xmm{k}{z}
    if isM128M32bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x4e)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VRSQRT14PS m256/m32bcst, ymm{k}{z}
    if isM256M32bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x4e)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VRSQRT14PS m512/m32bcst, zmm{k}{z}
    if isM512M32bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x4e)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VRSQRT14PS xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x4e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VRSQRT14PS ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x4e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VRSQRT14PS zmm, zmm{k}{z}
    if isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x4e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VRSQRT14PS")
    }
    return p
}

// VRSQRT14SD performs "Compute Approximate Reciprocal of a Square Root of a Scalar Double-Precision Floating-Point Value".
//
// Mnemonic        : VRSQRT14SD
// ISA extensions  : AVX512F
// Supported forms : (2 forms)
//
//    * VRSQRT14SD xmm, xmm, xmm{k}{z}
//    * VRSQRT14SD m64, xmm, xmm{k}{z}
//
func (self *Program) VRSQRT14SD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VRSQRT14SD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x4f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VRSQRT14SD m64, xmm, xmm{k}{z}
    if isM64(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x4f)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VRSQRT14SD")
    }
    return p
}

// VRSQRT14SS performs "Compute Approximate Reciprocal of a Square Root of a Scalar Single-Precision Floating-Point Value".
//
// Mnemonic        : VRSQRT14SS
// ISA extensions  : AVX512F
// Supported forms : (2 forms)
//
//    * VRSQRT14SS xmm, xmm, xmm{k}{z}
//    * VRSQRT14SS m32, xmm, xmm{k}{z}
//
func (self *Program) VRSQRT14SS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VRSQRT14SS xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x4f)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VRSQRT14SS m32, xmm, xmm{k}{z}
    if isM32(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x4f)
            m.mrsd(lcode(v[2]), addr(v[0]), 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VRSQRT14SS")
    }
    return p
}

// VRSQRT28PD performs "Approximation to the Reciprocal Square Root of Packed Double-Precision Floating-Point Values with Less Than 2^-28 Relative Error".
//
// Mnemonic        : VRSQRT28PD
// ISA extensions  : AVX512ER
// Supported forms : (3 forms)
//
//    * VRSQRT28PD m512/m64bcst, zmm{k}{z}
//    * VRSQRT28PD {sae}, zmm, zmm{k}{z}
//    * VRSQRT28PD zmm, zmm{k}{z}
//
func (self *Program) VRSQRT28PD(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VRSQRT28PD takes 2 or 3 operands")
    }
    // VRSQRT28PD m512/m64bcst, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512ER)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0xcc)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VRSQRT28PD {sae}, zmm, zmm{k}{z}
    if len(vv) == 1 && isSAE(v0) && isZMM(v1) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512ER)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x18)
            m.emit(0xcc)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VRSQRT28PD zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512ER)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0xcc)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VRSQRT28PD")
    }
    return p
}

// VRSQRT28PS performs "Approximation to the Reciprocal Square Root of Packed Single-Precision Floating-Point Values with Less Than 2^-28 Relative Error".
//
// Mnemonic        : VRSQRT28PS
// ISA extensions  : AVX512ER
// Supported forms : (3 forms)
//
//    * VRSQRT28PS m512/m32bcst, zmm{k}{z}
//    * VRSQRT28PS {sae}, zmm, zmm{k}{z}
//    * VRSQRT28PS zmm, zmm{k}{z}
//
func (self *Program) VRSQRT28PS(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VRSQRT28PS takes 2 or 3 operands")
    }
    // VRSQRT28PS m512/m32bcst, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512ER)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0xcc)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VRSQRT28PS {sae}, zmm, zmm{k}{z}
    if len(vv) == 1 && isSAE(v0) && isZMM(v1) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512ER)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[2]) << 7) | kcode(v[2]) | 0x18)
            m.emit(0xcc)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VRSQRT28PS zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512ER)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7d)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0xcc)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VRSQRT28PS")
    }
    return p
}

// VRSQRT28SD performs "Approximation to the Reciprocal Square Root of a Scalar Double-Precision Floating-Point Value with Less Than 2^-28 Relative Error".
//
// Mnemonic        : VRSQRT28SD
// ISA extensions  : AVX512ER
// Supported forms : (3 forms)
//
//    * VRSQRT28SD m64, xmm, xmm{k}{z}
//    * VRSQRT28SD {sae}, xmm, xmm, xmm{k}{z}
//    * VRSQRT28SD xmm, xmm, xmm{k}{z}
//
func (self *Program) VRSQRT28SD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VRSQRT28SD takes 3 or 4 operands")
    }
    // VRSQRT28SD m64, xmm, xmm{k}{z}
    if len(vv) == 0 && isM64(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512ER)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xcd)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    // VRSQRT28SD {sae}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isSAE(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512ER)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xcd)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VRSQRT28SD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512ER)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xcd)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VRSQRT28SD")
    }
    return p
}

// VRSQRT28SS performs "Approximation to the Reciprocal Square Root of a Scalar Single-Precision Floating-Point Value with Less Than 2^-28 Relative Error".
//
// Mnemonic        : VRSQRT28SS
// ISA extensions  : AVX512ER
// Supported forms : (3 forms)
//
//    * VRSQRT28SS m32, xmm, xmm{k}{z}
//    * VRSQRT28SS {sae}, xmm, xmm, xmm{k}{z}
//    * VRSQRT28SS xmm, xmm, xmm{k}{z}
//
func (self *Program) VRSQRT28SS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VRSQRT28SS takes 3 or 4 operands")
    }
    // VRSQRT28SS m32, xmm, xmm{k}{z}
    if len(vv) == 0 && isM32(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512ER)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0xcd)
            m.mrsd(lcode(v[2]), addr(v[0]), 4)
        })
    }
    // VRSQRT28SS {sae}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isSAE(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512ER)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0xcd)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VRSQRT28SS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512ER)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0xcd)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VRSQRT28SS")
    }
    return p
}

// VRSQRTPS performs "Compute Reciprocals of Square Roots of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VRSQRTPS
// ISA extensions  : AVX
// Supported forms : (4 forms)
//
//    * VRSQRTPS xmm, xmm
//    * VRSQRTPS m128, xmm
//    * VRSQRTPS ymm, ymm
//    * VRSQRTPS m256, ymm
//
func (self *Program) VRSQRTPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VRSQRTPS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[1]), v[0], 0)
            m.emit(0x52)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VRSQRTPS m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x52)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VRSQRTPS ymm, ymm
    if isYMM(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[1]), v[0], 0)
            m.emit(0x52)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VRSQRTPS m256, ymm
    if isM256(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x52)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VRSQRTPS")
    }
    return p
}

// VRSQRTSS performs "Compute Reciprocal of Square Root of Scalar Single-Precision Floating-Point Value".
//
// Mnemonic        : VRSQRTSS
// ISA extensions  : AVX
// Supported forms : (2 forms)
//
//    * VRSQRTSS xmm, xmm, xmm
//    * VRSQRTSS m32, xmm, xmm
//
func (self *Program) VRSQRTSS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VRSQRTSS xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x52)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VRSQRTSS m32, xmm, xmm
    if isM32(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x52)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VRSQRTSS")
    }
    return p
}

// VSCALEFPD performs "Scale Packed Double-Precision Floating-Point Values With Double-Precision Floating-Point Values".
//
// Mnemonic        : VSCALEFPD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (7 forms)
//
//    * VSCALEFPD m128/m64bcst, xmm, xmm{k}{z}
//    * VSCALEFPD xmm, xmm, xmm{k}{z}
//    * VSCALEFPD m256/m64bcst, ymm, ymm{k}{z}
//    * VSCALEFPD ymm, ymm, ymm{k}{z}
//    * VSCALEFPD m512/m64bcst, zmm, zmm{k}{z}
//    * VSCALEFPD {er}, zmm, zmm, zmm{k}{z}
//    * VSCALEFPD zmm, zmm, zmm{k}{z}
//
func (self *Program) VSCALEFPD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VSCALEFPD takes 3 or 4 operands")
    }
    // VSCALEFPD m128/m64bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x2c)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VSCALEFPD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x2c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VSCALEFPD m256/m64bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x2c)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VSCALEFPD ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x2c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VSCALEFPD m512/m64bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x2c)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VSCALEFPD {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x2c)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VSCALEFPD zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x2c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VSCALEFPD")
    }
    return p
}

// VSCALEFPS performs "Scale Packed Single-Precision Floating-Point Values With Single-Precision Floating-Point Values".
//
// Mnemonic        : VSCALEFPS
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (7 forms)
//
//    * VSCALEFPS m128/m32bcst, xmm, xmm{k}{z}
//    * VSCALEFPS xmm, xmm, xmm{k}{z}
//    * VSCALEFPS m256/m32bcst, ymm, ymm{k}{z}
//    * VSCALEFPS ymm, ymm, ymm{k}{z}
//    * VSCALEFPS m512/m32bcst, zmm, zmm{k}{z}
//    * VSCALEFPS {er}, zmm, zmm, zmm{k}{z}
//    * VSCALEFPS zmm, zmm, zmm{k}{z}
//
func (self *Program) VSCALEFPS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VSCALEFPS takes 3 or 4 operands")
    }
    // VSCALEFPS m128/m32bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x2c)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VSCALEFPS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x2c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VSCALEFPS m256/m32bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x2c)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VSCALEFPS ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x2c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VSCALEFPS m512/m32bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x2c)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VSCALEFPS {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x2c)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VSCALEFPS zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x2c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VSCALEFPS")
    }
    return p
}

// VSCALEFSD performs "Scale Scalar Double-Precision Floating-Point Value With a Double-Precision Floating-Point Value".
//
// Mnemonic        : VSCALEFSD
// ISA extensions  : AVX512F
// Supported forms : (3 forms)
//
//    * VSCALEFSD m64, xmm, xmm{k}{z}
//    * VSCALEFSD {er}, xmm, xmm, xmm{k}{z}
//    * VSCALEFSD xmm, xmm, xmm{k}{z}
//
func (self *Program) VSCALEFSD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VSCALEFSD takes 3 or 4 operands")
    }
    // VSCALEFSD m64, xmm, xmm{k}{z}
    if len(vv) == 0 && isM64(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x2d)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    // VSCALEFSD {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x2d)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VSCALEFSD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x2d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VSCALEFSD")
    }
    return p
}

// VSCALEFSS performs "Scale Scalar Single-Precision Floating-Point Value With a Single-Precision Floating-Point Value".
//
// Mnemonic        : VSCALEFSS
// ISA extensions  : AVX512F
// Supported forms : (3 forms)
//
//    * VSCALEFSS m32, xmm, xmm{k}{z}
//    * VSCALEFSS {er}, xmm, xmm, xmm{k}{z}
//    * VSCALEFSS xmm, xmm, xmm{k}{z}
//
func (self *Program) VSCALEFSS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VSCALEFSS takes 3 or 4 operands")
    }
    // VSCALEFSS m32, xmm, xmm{k}{z}
    if len(vv) == 0 && isM32(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x2d)
            m.mrsd(lcode(v[2]), addr(v[0]), 4)
        })
    }
    // VSCALEFSS {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x2d)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VSCALEFSS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf2 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7d ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x2d)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VSCALEFSS")
    }
    return p
}

// VSCATTERDPD performs "Scatter Packed Double-Precision Floating-Point Values with Signed Doubleword Indices".
//
// Mnemonic        : VSCATTERDPD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (3 forms)
//
//    * VSCATTERDPD xmm, vm32x{k}
//    * VSCATTERDPD ymm, vm32x{k}
//    * VSCATTERDPD zmm, vm32y{k}
//
func (self *Program) VSCATTERDPD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VSCATTERDPD xmm, vm32x{k}
    if isEVEXXMM(v0) && isVMXk(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), 0, 0)
            m.emit(0xa2)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    // VSCATTERDPD ymm, vm32x{k}
    if isEVEXYMM(v0) && isVMXk(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), 0, 0)
            m.emit(0xa2)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    // VSCATTERDPD zmm, vm32y{k}
    if isZMM(v0) && isVMYk(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), 0, 0)
            m.emit(0xa2)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VSCATTERDPD")
    }
    return p
}

// VSCATTERDPS performs "Scatter Packed Single-Precision Floating-Point Values with Signed Doubleword Indices".
//
// Mnemonic        : VSCATTERDPS
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (3 forms)
//
//    * VSCATTERDPS xmm, vm32x{k}
//    * VSCATTERDPS ymm, vm32y{k}
//    * VSCATTERDPS zmm, vm32z{k}
//
func (self *Program) VSCATTERDPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VSCATTERDPS xmm, vm32x{k}
    if isEVEXXMM(v0) && isVMXk(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), 0, 0)
            m.emit(0xa2)
            m.mrsd(lcode(v[0]), addr(v[1]), 4)
        })
    }
    // VSCATTERDPS ymm, vm32y{k}
    if isEVEXYMM(v0) && isVMYk(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), 0, 0)
            m.emit(0xa2)
            m.mrsd(lcode(v[0]), addr(v[1]), 4)
        })
    }
    // VSCATTERDPS zmm, vm32z{k}
    if isZMM(v0) && isVMZk(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), 0, 0)
            m.emit(0xa2)
            m.mrsd(lcode(v[0]), addr(v[1]), 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VSCATTERDPS")
    }
    return p
}

// VSCATTERPF0DPD performs "Sparse Prefetch Packed Double-Precision Floating-Point Data Values with Signed Doubleword Indices Using T0 Hint with Intent to Write".
//
// Mnemonic        : VSCATTERPF0DPD
// ISA extensions  : AVX512PF
// Supported forms : (1 form)
//
//    * VSCATTERPF0DPD vm32y{k}
//
func (self *Program) VSCATTERPF0DPD(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // VSCATTERPF0DPD vm32y{k}
    if isVMYk(v0) {
        self.require(ISA_AVX512PF)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, 0, addr(v[0]), 0, kcode(v[0]), 0, 0)
            m.emit(0xc6)
            m.mrsd(5, addr(v[0]), 8)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VSCATTERPF0DPD")
    }
    return p
}

// VSCATTERPF0DPS performs "Sparse Prefetch Packed Single-Precision Floating-Point Data Values with Signed Doubleword Indices Using T0 Hint with Intent to Write".
//
// Mnemonic        : VSCATTERPF0DPS
// ISA extensions  : AVX512PF
// Supported forms : (1 form)
//
//    * VSCATTERPF0DPS vm32z{k}
//
func (self *Program) VSCATTERPF0DPS(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // VSCATTERPF0DPS vm32z{k}
    if isVMZk(v0) {
        self.require(ISA_AVX512PF)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, 0, addr(v[0]), 0, kcode(v[0]), 0, 0)
            m.emit(0xc6)
            m.mrsd(5, addr(v[0]), 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VSCATTERPF0DPS")
    }
    return p
}

// VSCATTERPF0QPD performs "Sparse Prefetch Packed Double-Precision Floating-Point Data Values with Signed Quadword Indices Using T0 Hint with Intent to Write".
//
// Mnemonic        : VSCATTERPF0QPD
// ISA extensions  : AVX512PF
// Supported forms : (1 form)
//
//    * VSCATTERPF0QPD vm64z{k}
//
func (self *Program) VSCATTERPF0QPD(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // VSCATTERPF0QPD vm64z{k}
    if isVMZk(v0) {
        self.require(ISA_AVX512PF)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, 0, addr(v[0]), 0, kcode(v[0]), 0, 0)
            m.emit(0xc7)
            m.mrsd(5, addr(v[0]), 8)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VSCATTERPF0QPD")
    }
    return p
}

// VSCATTERPF0QPS performs "Sparse Prefetch Packed Single-Precision Floating-Point Data Values with Signed Quadword Indices Using T0 Hint with Intent to Write".
//
// Mnemonic        : VSCATTERPF0QPS
// ISA extensions  : AVX512PF
// Supported forms : (1 form)
//
//    * VSCATTERPF0QPS vm64z{k}
//
func (self *Program) VSCATTERPF0QPS(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // VSCATTERPF0QPS vm64z{k}
    if isVMZk(v0) {
        self.require(ISA_AVX512PF)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, 0, addr(v[0]), 0, kcode(v[0]), 0, 0)
            m.emit(0xc7)
            m.mrsd(5, addr(v[0]), 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VSCATTERPF0QPS")
    }
    return p
}

// VSCATTERPF1DPD performs "Sparse Prefetch Packed Double-Precision Floating-Point Data Values with Signed Doubleword Indices Using T1 Hint with Intent to Write".
//
// Mnemonic        : VSCATTERPF1DPD
// ISA extensions  : AVX512PF
// Supported forms : (1 form)
//
//    * VSCATTERPF1DPD vm32y{k}
//
func (self *Program) VSCATTERPF1DPD(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // VSCATTERPF1DPD vm32y{k}
    if isVMYk(v0) {
        self.require(ISA_AVX512PF)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, 0, addr(v[0]), 0, kcode(v[0]), 0, 0)
            m.emit(0xc6)
            m.mrsd(6, addr(v[0]), 8)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VSCATTERPF1DPD")
    }
    return p
}

// VSCATTERPF1DPS performs "Sparse Prefetch Packed Single-Precision Floating-Point Data Values with Signed Doubleword Indices Using T1 Hint with Intent to Write".
//
// Mnemonic        : VSCATTERPF1DPS
// ISA extensions  : AVX512PF
// Supported forms : (1 form)
//
//    * VSCATTERPF1DPS vm32z{k}
//
func (self *Program) VSCATTERPF1DPS(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // VSCATTERPF1DPS vm32z{k}
    if isVMZk(v0) {
        self.require(ISA_AVX512PF)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, 0, addr(v[0]), 0, kcode(v[0]), 0, 0)
            m.emit(0xc6)
            m.mrsd(6, addr(v[0]), 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VSCATTERPF1DPS")
    }
    return p
}

// VSCATTERPF1QPD performs "Sparse Prefetch Packed Double-Precision Floating-Point Data Values with Signed Quadword Indices Using T1 Hint with Intent to Write".
//
// Mnemonic        : VSCATTERPF1QPD
// ISA extensions  : AVX512PF
// Supported forms : (1 form)
//
//    * VSCATTERPF1QPD vm64z{k}
//
func (self *Program) VSCATTERPF1QPD(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // VSCATTERPF1QPD vm64z{k}
    if isVMZk(v0) {
        self.require(ISA_AVX512PF)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, 0, addr(v[0]), 0, kcode(v[0]), 0, 0)
            m.emit(0xc7)
            m.mrsd(6, addr(v[0]), 8)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VSCATTERPF1QPD")
    }
    return p
}

// VSCATTERPF1QPS performs "Sparse Prefetch Packed Single-Precision Floating-Point Data Values with Signed Quadword Indices Using T1 Hint with Intent to Write".
//
// Mnemonic        : VSCATTERPF1QPS
// ISA extensions  : AVX512PF
// Supported forms : (1 form)
//
//    * VSCATTERPF1QPS vm64z{k}
//
func (self *Program) VSCATTERPF1QPS(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // VSCATTERPF1QPS vm64z{k}
    if isVMZk(v0) {
        self.require(ISA_AVX512PF)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, 0, addr(v[0]), 0, kcode(v[0]), 0, 0)
            m.emit(0xc7)
            m.mrsd(6, addr(v[0]), 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VSCATTERPF1QPS")
    }
    return p
}

// VSCATTERQPD performs "Scatter Packed Double-Precision Floating-Point Values with Signed Quadword Indices".
//
// Mnemonic        : VSCATTERQPD
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (3 forms)
//
//    * VSCATTERQPD xmm, vm64x{k}
//    * VSCATTERQPD ymm, vm64y{k}
//    * VSCATTERQPD zmm, vm64z{k}
//
func (self *Program) VSCATTERQPD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VSCATTERQPD xmm, vm64x{k}
    if isEVEXXMM(v0) && isVMXk(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), 0, 0)
            m.emit(0xa3)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    // VSCATTERQPD ymm, vm64y{k}
    if isEVEXYMM(v0) && isVMYk(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), 0, 0)
            m.emit(0xa3)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    // VSCATTERQPD zmm, vm64z{k}
    if isZMM(v0) && isVMZk(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x85, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), 0, 0)
            m.emit(0xa3)
            m.mrsd(lcode(v[0]), addr(v[1]), 8)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VSCATTERQPD")
    }
    return p
}

// VSCATTERQPS performs "Scatter Packed Single-Precision Floating-Point Values with Signed Quadword Indices".
//
// Mnemonic        : VSCATTERQPS
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (3 forms)
//
//    * VSCATTERQPS xmm, vm64x{k}
//    * VSCATTERQPS xmm, vm64y{k}
//    * VSCATTERQPS ymm, vm64z{k}
//
func (self *Program) VSCATTERQPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VSCATTERQPS xmm, vm64x{k}
    if isEVEXXMM(v0) && isVMXk(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b00, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), 0, 0)
            m.emit(0xa3)
            m.mrsd(lcode(v[0]), addr(v[1]), 4)
        })
    }
    // VSCATTERQPS xmm, vm64y{k}
    if isEVEXXMM(v0) && isVMYk(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b01, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), 0, 0)
            m.emit(0xa3)
            m.mrsd(lcode(v[0]), addr(v[1]), 4)
        })
    }
    // VSCATTERQPS ymm, vm64z{k}
    if isEVEXYMM(v0) && isVMZk(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b10, 0x05, 0b10, ehcode(v[0]), addr(v[1]), 0, kcode(v[1]), 0, 0)
            m.emit(0xa3)
            m.mrsd(lcode(v[0]), addr(v[1]), 4)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VSCATTERQPS")
    }
    return p
}

// VSHUFF32X4 performs "Shuffle 128-Bit Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VSHUFF32X4
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (4 forms)
//
//    * VSHUFF32X4 imm8, m256/m32bcst, ymm, ymm{k}{z}
//    * VSHUFF32X4 imm8, ymm, ymm, ymm{k}{z}
//    * VSHUFF32X4 imm8, m512/m32bcst, zmm, zmm{k}{z}
//    * VSHUFF32X4 imm8, zmm, zmm, zmm{k}{z}
//
func (self *Program) VSHUFF32X4(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VSHUFF32X4 imm8, m256/m32bcst, ymm, ymm{k}{z}
    if isImm8(v0) && isM256M32bcst(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b01, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0x23)
            m.mrsd(lcode(v[3]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VSHUFF32X4 imm8, ymm, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x20)
            m.emit(0x23)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VSHUFF32X4 imm8, m512/m32bcst, zmm, zmm{k}{z}
    if isImm8(v0) && isM512M32bcst(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0x23)
            m.mrsd(lcode(v[3]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VSHUFF32X4 imm8, zmm, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x23)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VSHUFF32X4")
    }
    return p
}

// VSHUFF64X2 performs "Shuffle 128-Bit Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VSHUFF64X2
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (4 forms)
//
//    * VSHUFF64X2 imm8, m256/m64bcst, ymm, ymm{k}{z}
//    * VSHUFF64X2 imm8, ymm, ymm, ymm{k}{z}
//    * VSHUFF64X2 imm8, m512/m64bcst, zmm, zmm{k}{z}
//    * VSHUFF64X2 imm8, zmm, zmm, zmm{k}{z}
//
func (self *Program) VSHUFF64X2(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VSHUFF64X2 imm8, m256/m64bcst, ymm, ymm{k}{z}
    if isImm8(v0) && isM256M64bcst(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b01, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0x23)
            m.mrsd(lcode(v[3]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VSHUFF64X2 imm8, ymm, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x20)
            m.emit(0x23)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VSHUFF64X2 imm8, m512/m64bcst, zmm, zmm{k}{z}
    if isImm8(v0) && isM512M64bcst(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0x23)
            m.mrsd(lcode(v[3]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VSHUFF64X2 imm8, zmm, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x23)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VSHUFF64X2")
    }
    return p
}

// VSHUFI32X4 performs "Shuffle 128-Bit Packed Doubleword Integer Values".
//
// Mnemonic        : VSHUFI32X4
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (4 forms)
//
//    * VSHUFI32X4 imm8, m256/m32bcst, ymm, ymm{k}{z}
//    * VSHUFI32X4 imm8, ymm, ymm, ymm{k}{z}
//    * VSHUFI32X4 imm8, m512/m32bcst, zmm, zmm{k}{z}
//    * VSHUFI32X4 imm8, zmm, zmm, zmm{k}{z}
//
func (self *Program) VSHUFI32X4(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VSHUFI32X4 imm8, m256/m32bcst, ymm, ymm{k}{z}
    if isImm8(v0) && isM256M32bcst(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b01, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0x43)
            m.mrsd(lcode(v[3]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VSHUFI32X4 imm8, ymm, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x20)
            m.emit(0x43)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VSHUFI32X4 imm8, m512/m32bcst, zmm, zmm{k}{z}
    if isImm8(v0) && isM512M32bcst(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x05, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0x43)
            m.mrsd(lcode(v[3]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VSHUFI32X4 imm8, zmm, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7d ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x43)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VSHUFI32X4")
    }
    return p
}

// VSHUFI64X2 performs "Shuffle 128-Bit Packed Quadword Integer Values".
//
// Mnemonic        : VSHUFI64X2
// ISA extensions  : AVX512F, AVX512VL
// Supported forms : (4 forms)
//
//    * VSHUFI64X2 imm8, m256/m64bcst, ymm, ymm{k}{z}
//    * VSHUFI64X2 imm8, ymm, ymm, ymm{k}{z}
//    * VSHUFI64X2 imm8, m512/m64bcst, zmm, zmm{k}{z}
//    * VSHUFI64X2 imm8, zmm, zmm, zmm{k}{z}
//
func (self *Program) VSHUFI64X2(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VSHUFI64X2 imm8, m256/m64bcst, ymm, ymm{k}{z}
    if isImm8(v0) && isM256M64bcst(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b01, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0x43)
            m.mrsd(lcode(v[3]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VSHUFI64X2 imm8, ymm, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x20)
            m.emit(0x43)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VSHUFI64X2 imm8, m512/m64bcst, zmm, zmm{k}{z}
    if isImm8(v0) && isM512M64bcst(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b11, 0x85, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0x43)
            m.mrsd(lcode(v[3]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VSHUFI64X2 imm8, zmm, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf3 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0x43)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VSHUFI64X2")
    }
    return p
}

// VSHUFPD performs "Shuffle Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VSHUFPD
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VSHUFPD imm8, m128/m64bcst, xmm, xmm{k}{z}
//    * VSHUFPD imm8, xmm, xmm, xmm{k}{z}
//    * VSHUFPD imm8, m256/m64bcst, ymm, ymm{k}{z}
//    * VSHUFPD imm8, ymm, ymm, ymm{k}{z}
//    * VSHUFPD imm8, m512/m64bcst, zmm, zmm{k}{z}
//    * VSHUFPD imm8, zmm, zmm, zmm{k}{z}
//    * VSHUFPD imm8, xmm, xmm, xmm
//    * VSHUFPD imm8, m128, xmm, xmm
//    * VSHUFPD imm8, ymm, ymm, ymm
//    * VSHUFPD imm8, m256, ymm, ymm
//
func (self *Program) VSHUFPD(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VSHUFPD imm8, m128/m64bcst, xmm, xmm{k}{z}
    if isImm8(v0) && isM128M64bcst(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0xc6)
            m.mrsd(lcode(v[3]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VSHUFPD imm8, xmm, xmm, xmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x00)
            m.emit(0xc6)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VSHUFPD imm8, m256/m64bcst, ymm, ymm{k}{z}
    if isImm8(v0) && isM256M64bcst(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0xc6)
            m.mrsd(lcode(v[3]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VSHUFPD imm8, ymm, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x20)
            m.emit(0xc6)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VSHUFPD imm8, m512/m64bcst, zmm, zmm{k}{z}
    if isImm8(v0) && isM512M64bcst(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0xc6)
            m.mrsd(lcode(v[3]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VSHUFPD imm8, zmm, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0xc6)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VSHUFPD imm8, xmm, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[3]), v[1], hlcode(v[2]))
            m.emit(0xc6)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VSHUFPD imm8, m128, xmm, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0xc6)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VSHUFPD imm8, ymm, ymm, ymm
    if isImm8(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[3]), v[1], hlcode(v[2]))
            m.emit(0xc6)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VSHUFPD imm8, m256, ymm, ymm
    if isImm8(v0) && isM256(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0xc6)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VSHUFPD")
    }
    return p
}

// VSHUFPS performs "Shuffle Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VSHUFPS
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VSHUFPS imm8, m128/m32bcst, xmm, xmm{k}{z}
//    * VSHUFPS imm8, xmm, xmm, xmm{k}{z}
//    * VSHUFPS imm8, m256/m32bcst, ymm, ymm{k}{z}
//    * VSHUFPS imm8, ymm, ymm, ymm{k}{z}
//    * VSHUFPS imm8, m512/m32bcst, zmm, zmm{k}{z}
//    * VSHUFPS imm8, zmm, zmm, zmm{k}{z}
//    * VSHUFPS imm8, xmm, xmm, xmm
//    * VSHUFPS imm8, m128, xmm, xmm
//    * VSHUFPS imm8, ymm, ymm, ymm
//    * VSHUFPS imm8, m256, ymm, ymm
//
func (self *Program) VSHUFPS(v0 interface{}, v1 interface{}, v2 interface{}, v3 interface{}) *Instruction {
    p := self.alloc(4, Operands{v0, v1, v2, v3})
    // VSHUFPS imm8, m128/m32bcst, xmm, xmm{k}{z}
    if isImm8(v0) && isM128M32bcst(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b00, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0xc6)
            m.mrsd(lcode(v[3]), addr(v[1]), 16)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VSHUFPS imm8, xmm, xmm, xmm{k}{z}
    if isImm8(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7c ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x00)
            m.emit(0xc6)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VSHUFPS imm8, m256/m32bcst, ymm, ymm{k}{z}
    if isImm8(v0) && isM256M32bcst(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b01, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0xc6)
            m.mrsd(lcode(v[3]), addr(v[1]), 32)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VSHUFPS imm8, ymm, ymm, ymm{k}{z}
    if isImm8(v0) && isEVEXYMM(v1) && isEVEXYMM(v2) && isYMMkz(v3) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7c ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x20)
            m.emit(0xc6)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VSHUFPS imm8, m512/m32bcst, zmm, zmm{k}{z}
    if isImm8(v0) && isM512M32bcst(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b10, ehcode(v[3]), addr(v[1]), vcode(v[2]), kcode(v[3]), zcode(v[3]), bcode(v[1]))
            m.emit(0xc6)
            m.mrsd(lcode(v[3]), addr(v[1]), 64)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VSHUFPS imm8, zmm, zmm, zmm{k}{z}
    if isImm8(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(v3) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7c ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x40)
            m.emit(0xc6)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VSHUFPS imm8, xmm, xmm, xmm
    if isImm8(v0) && isXMM(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[3]), v[1], hlcode(v[2]))
            m.emit(0xc6)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VSHUFPS imm8, m128, xmm, xmm
    if isImm8(v0) && isM128(v1) && isXMM(v2) && isXMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0xc6)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // VSHUFPS imm8, ymm, ymm, ymm
    if isImm8(v0) && isYMM(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[3]), v[1], hlcode(v[2]))
            m.emit(0xc6)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // VSHUFPS imm8, m256, ymm, ymm
    if isImm8(v0) && isM256(v1) && isYMM(v2) && isYMM(v3) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[3]), addr(v[1]), hlcode(v[2]))
            m.emit(0xc6)
            m.mrsd(lcode(v[3]), addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VSHUFPS")
    }
    return p
}

// VSQRTPD performs "Compute Square Roots of Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VSQRTPD
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (11 forms)
//
//    * VSQRTPD m128/m32bcst, xmm{k}{z}
//    * VSQRTPD m256/m32bcst, ymm{k}{z}
//    * VSQRTPD m512/m64bcst, zmm{k}{z}
//    * VSQRTPD xmm, xmm{k}{z}
//    * VSQRTPD ymm, ymm{k}{z}
//    * VSQRTPD xmm, xmm
//    * VSQRTPD m128, xmm
//    * VSQRTPD ymm, ymm
//    * VSQRTPD m256, ymm
//    * VSQRTPD {er}, zmm, zmm{k}{z}
//    * VSQRTPD zmm, zmm{k}{z}
//
func (self *Program) VSQRTPD(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VSQRTPD takes 2 or 3 operands")
    }
    // VSQRTPD m128/m32bcst, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x51)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VSQRTPD m256/m32bcst, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x51)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VSQRTPD m512/m64bcst, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x51)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VSQRTPD xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x51)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VSQRTPD ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x51)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VSQRTPD xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[1]), v[0], 0)
            m.emit(0x51)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VSQRTPD m128, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x51)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VSQRTPD ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[1]), v[0], 0)
            m.emit(0x51)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VSQRTPD m256, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x51)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VSQRTPD {er}, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[2]) << 7) | (vcode(v[0]) << 5) | kcode(v[2]) | 0x18)
            m.emit(0x51)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VSQRTPD zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x51)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VSQRTPD")
    }
    return p
}

// VSQRTPS performs "Compute Square Roots of Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VSQRTPS
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (11 forms)
//
//    * VSQRTPS m128/m32bcst, xmm{k}{z}
//    * VSQRTPS m256/m32bcst, ymm{k}{z}
//    * VSQRTPS m512/m32bcst, zmm{k}{z}
//    * VSQRTPS xmm, xmm{k}{z}
//    * VSQRTPS ymm, ymm{k}{z}
//    * VSQRTPS xmm, xmm
//    * VSQRTPS m128, xmm
//    * VSQRTPS ymm, ymm
//    * VSQRTPS m256, ymm
//    * VSQRTPS {er}, zmm, zmm{k}{z}
//    * VSQRTPS zmm, zmm{k}{z}
//
func (self *Program) VSQRTPS(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VSQRTPS takes 2 or 3 operands")
    }
    // VSQRTPS m128/m32bcst, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b00, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x51)
            m.mrsd(lcode(v[1]), addr(v[0]), 16)
        })
    }
    // VSQRTPS m256/m32bcst, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b01, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x51)
            m.mrsd(lcode(v[1]), addr(v[0]), 32)
        })
    }
    // VSQRTPS m512/m32bcst, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b10, ehcode(v[1]), addr(v[0]), 0, kcode(v[1]), zcode(v[1]), bcode(v[0]))
            m.emit(0x51)
            m.mrsd(lcode(v[1]), addr(v[0]), 64)
        })
    }
    // VSQRTPS xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isXMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7c)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x08)
            m.emit(0x51)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VSQRTPS ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isYMMkz(v1) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7c)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x28)
            m.emit(0x51)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VSQRTPS xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[1]), v[0], 0)
            m.emit(0x51)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VSQRTPS m128, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x51)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VSQRTPS ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[1]), v[0], 0)
            m.emit(0x51)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VSQRTPS m256, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x51)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VSQRTPS {er}, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c)
            m.emit((zcode(v[2]) << 7) | (vcode(v[0]) << 5) | kcode(v[2]) | 0x18)
            m.emit(0x51)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VSQRTPS zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMMkz(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7c)
            m.emit((zcode(v[1]) << 7) | kcode(v[1]) | 0x48)
            m.emit(0x51)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VSQRTPS")
    }
    return p
}

// VSQRTSD performs "Compute Square Root of Scalar Double-Precision Floating-Point Value".
//
// Mnemonic        : VSQRTSD
// ISA extensions  : AVX, AVX512F
// Supported forms : (5 forms)
//
//    * VSQRTSD m64, xmm, xmm{k}{z}
//    * VSQRTSD xmm, xmm, xmm
//    * VSQRTSD m64, xmm, xmm
//    * VSQRTSD {er}, xmm, xmm, xmm{k}{z}
//    * VSQRTSD xmm, xmm, xmm{k}{z}
//
func (self *Program) VSQRTSD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VSQRTSD takes 3 or 4 operands")
    }
    // VSQRTSD m64, xmm, xmm{k}{z}
    if len(vv) == 0 && isM64(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x87, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x51)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    // VSQRTSD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x51)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VSQRTSD m64, xmm, xmm
    if len(vv) == 0 && isM64(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x51)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VSQRTSD {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xff ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x51)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VSQRTSD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xff ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x51)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VSQRTSD")
    }
    return p
}

// VSQRTSS performs "Compute Square Root of Scalar Single-Precision Floating-Point Value".
//
// Mnemonic        : VSQRTSS
// ISA extensions  : AVX, AVX512F
// Supported forms : (5 forms)
//
//    * VSQRTSS m32, xmm, xmm{k}{z}
//    * VSQRTSS xmm, xmm, xmm
//    * VSQRTSS m32, xmm, xmm
//    * VSQRTSS {er}, xmm, xmm, xmm{k}{z}
//    * VSQRTSS xmm, xmm, xmm{k}{z}
//
func (self *Program) VSQRTSS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VSQRTSS takes 3 or 4 operands")
    }
    // VSQRTSS m32, xmm, xmm{k}{z}
    if len(vv) == 0 && isM32(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x51)
            m.mrsd(lcode(v[2]), addr(v[0]), 4)
        })
    }
    // VSQRTSS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x51)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VSQRTSS m32, xmm, xmm
    if len(vv) == 0 && isM32(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x51)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VSQRTSS {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7e ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x51)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VSQRTSS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7e ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x51)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VSQRTSS")
    }
    return p
}

// VSTMXCSR performs "Store MXCSR Register State".
//
// Mnemonic        : VSTMXCSR
// ISA extensions  : AVX
// Supported forms : (1 form)
//
//    * VSTMXCSR m32
//
func (self *Program) VSTMXCSR(v0 interface{}) *Instruction {
    p := self.alloc(1, Operands{v0})
    // VSTMXCSR m32
    if isM32(v0) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, 0, addr(v[0]), 0)
            m.emit(0xae)
            m.mrsd(3, addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VSTMXCSR")
    }
    return p
}

// VSUBPD performs "Subtract Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VSUBPD
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (11 forms)
//
//    * VSUBPD m128/m64bcst, xmm, xmm{k}{z}
//    * VSUBPD xmm, xmm, xmm{k}{z}
//    * VSUBPD m256/m64bcst, ymm, ymm{k}{z}
//    * VSUBPD ymm, ymm, ymm{k}{z}
//    * VSUBPD m512/m64bcst, zmm, zmm{k}{z}
//    * VSUBPD xmm, xmm, xmm
//    * VSUBPD m128, xmm, xmm
//    * VSUBPD ymm, ymm, ymm
//    * VSUBPD m256, ymm, ymm
//    * VSUBPD {er}, zmm, zmm, zmm{k}{z}
//    * VSUBPD zmm, zmm, zmm{k}{z}
//
func (self *Program) VSUBPD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VSUBPD takes 3 or 4 operands")
    }
    // VSUBPD m128/m64bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x5c)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VSUBPD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x5c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VSUBPD m256/m64bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x5c)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VSUBPD ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x5c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VSUBPD m512/m64bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x5c)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VSUBPD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x5c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VSUBPD m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x5c)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VSUBPD ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x5c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VSUBPD m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x5c)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VSUBPD {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xfd ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x5c)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VSUBPD zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x5c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VSUBPD")
    }
    return p
}

// VSUBPS performs "Subtract Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VSUBPS
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (11 forms)
//
//    * VSUBPS m128/m32bcst, xmm, xmm{k}{z}
//    * VSUBPS xmm, xmm, xmm{k}{z}
//    * VSUBPS m256/m32bcst, ymm, ymm{k}{z}
//    * VSUBPS ymm, ymm, ymm{k}{z}
//    * VSUBPS m512/m32bcst, zmm, zmm{k}{z}
//    * VSUBPS xmm, xmm, xmm
//    * VSUBPS m128, xmm, xmm
//    * VSUBPS ymm, ymm, ymm
//    * VSUBPS m256, ymm, ymm
//    * VSUBPS {er}, zmm, zmm, zmm{k}{z}
//    * VSUBPS zmm, zmm, zmm{k}{z}
//
func (self *Program) VSUBPS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VSUBPS takes 3 or 4 operands")
    }
    // VSUBPS m128/m32bcst, xmm, xmm{k}{z}
    if len(vv) == 0 && isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x5c)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VSUBPS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x5c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VSUBPS m256/m32bcst, ymm, ymm{k}{z}
    if len(vv) == 0 && isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x5c)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VSUBPS ymm, ymm, ymm{k}{z}
    if len(vv) == 0 && isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x5c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VSUBPS m512/m32bcst, zmm, zmm{k}{z}
    if len(vv) == 0 && isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x5c)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VSUBPS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x5c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VSUBPS m128, xmm, xmm
    if len(vv) == 0 && isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x5c)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VSUBPS ymm, ymm, ymm
    if len(vv) == 0 && isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x5c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VSUBPS m256, ymm, ymm
    if len(vv) == 0 && isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x5c)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VSUBPS {er}, zmm, zmm, zmm{k}{z}
    if len(vv) == 1 && isER(v0) && isZMM(v1) && isZMM(v2) && isZMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7c ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x5c)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VSUBPS zmm, zmm, zmm{k}{z}
    if len(vv) == 0 && isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x5c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VSUBPS")
    }
    return p
}

// VSUBSD performs "Subtract Scalar Double-Precision Floating-Point Values".
//
// Mnemonic        : VSUBSD
// ISA extensions  : AVX, AVX512F
// Supported forms : (5 forms)
//
//    * VSUBSD m64, xmm, xmm{k}{z}
//    * VSUBSD xmm, xmm, xmm
//    * VSUBSD m64, xmm, xmm
//    * VSUBSD {er}, xmm, xmm, xmm{k}{z}
//    * VSUBSD xmm, xmm, xmm{k}{z}
//
func (self *Program) VSUBSD(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VSUBSD takes 3 or 4 operands")
    }
    // VSUBSD m64, xmm, xmm{k}{z}
    if len(vv) == 0 && isM64(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x87, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x5c)
            m.mrsd(lcode(v[2]), addr(v[0]), 8)
        })
    }
    // VSUBSD xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x5c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VSUBSD m64, xmm, xmm
    if len(vv) == 0 && isM64(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(3, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x5c)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VSUBSD {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0xff ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x5c)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VSUBSD xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xff ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x5c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VSUBSD")
    }
    return p
}

// VSUBSS performs "Subtract Scalar Single-Precision Floating-Point Values".
//
// Mnemonic        : VSUBSS
// ISA extensions  : AVX, AVX512F
// Supported forms : (5 forms)
//
//    * VSUBSS m32, xmm, xmm{k}{z}
//    * VSUBSS xmm, xmm, xmm
//    * VSUBSS m32, xmm, xmm
//    * VSUBSS {er}, xmm, xmm, xmm{k}{z}
//    * VSUBSS xmm, xmm, xmm{k}{z}
//
func (self *Program) VSUBSS(v0 interface{}, v1 interface{}, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(3, Operands{v0, v1, v2})
        case 1  : p = self.alloc(4, Operands{v0, v1, v2, vv[0]})
        default : panic("instruction VSUBSS takes 3 or 4 operands")
    }
    // VSUBSS m32, xmm, xmm{k}{z}
    if len(vv) == 0 && isM32(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x06, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), 0)
            m.emit(0x5c)
            m.mrsd(lcode(v[2]), addr(v[0]), 4)
        })
    }
    // VSUBSS xmm, xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x5c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VSUBSS m32, xmm, xmm
    if len(vv) == 0 && isM32(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(2, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x5c)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VSUBSS {er}, xmm, xmm, xmm{k}{z}
    if len(vv) == 1 && isER(v0) && isEVEXXMM(v1) && isEVEXXMM(v2) && isXMMkz(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[3]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[3]) << 4)))
            m.emit(0x7e ^ (hlcode(v[2]) << 3))
            m.emit((zcode(v[3]) << 7) | (vcode(v[0]) << 5) | (0x08 ^ (ecode(v[2]) << 3)) | kcode(v[3]) | 0x10)
            m.emit(0x5c)
            m.emit(0xc0 | lcode(v[3]) << 3 | lcode(v[1]))
        })
    }
    // VSUBSS xmm, xmm, xmm{k}{z}
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7e ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x5c)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VSUBSS")
    }
    return p
}

// VTESTPD performs "Packed Double-Precision Floating-Point Bit Test".
//
// Mnemonic        : VTESTPD
// ISA extensions  : AVX
// Supported forms : (4 forms)
//
//    * VTESTPD xmm, xmm
//    * VTESTPD m128, xmm
//    * VTESTPD ymm, ymm
//    * VTESTPD m256, ymm
//
func (self *Program) VTESTPD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VTESTPD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79)
            m.emit(0x0f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VTESTPD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x0f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VTESTPD ymm, ymm
    if isYMM(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d)
            m.emit(0x0f)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VTESTPD m256, ymm
    if isM256(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x0f)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VTESTPD")
    }
    return p
}

// VTESTPS performs "Packed Single-Precision Floating-Point Bit Test".
//
// Mnemonic        : VTESTPS
// ISA extensions  : AVX
// Supported forms : (4 forms)
//
//    * VTESTPS xmm, xmm
//    * VTESTPS m128, xmm
//    * VTESTPS ymm, ymm
//    * VTESTPS m256, ymm
//
func (self *Program) VTESTPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // VTESTPS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x79)
            m.emit(0x0e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VTESTPS m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x01, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x0e)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VTESTPS ymm, ymm
    if isYMM(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0xc4)
            m.emit(0xe2 ^ (hcode(v[1]) << 7) ^ (hcode(v[0]) << 5))
            m.emit(0x7d)
            m.emit(0x0e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VTESTPS m256, ymm
    if isM256(v0) && isYMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex3(0xc4, 0b10, 0x05, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x0e)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VTESTPS")
    }
    return p
}

// VUCOMISD performs "Unordered Compare Scalar Double-Precision Floating-Point Values and Set EFLAGS".
//
// Mnemonic        : VUCOMISD
// ISA extensions  : AVX, AVX512F
// Supported forms : (5 forms)
//
//    * VUCOMISD xmm, xmm
//    * VUCOMISD m64, xmm
//    * VUCOMISD m64, xmm
//    * VUCOMISD {sae}, xmm, xmm
//    * VUCOMISD xmm, xmm
//
func (self *Program) VUCOMISD(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VUCOMISD takes 2 or 3 operands")
    }
    // VUCOMISD xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[1]), v[0], 0)
            m.emit(0x2e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VUCOMISD m64, xmm
    if len(vv) == 0 && isM64(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x2e)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VUCOMISD m64, xmm
    if len(vv) == 0 && isM64(v0) && isEVEXXMM(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[1]), addr(v[0]), 0, 0, 0, 0)
            m.emit(0x2e)
            m.mrsd(lcode(v[1]), addr(v[0]), 8)
        })
    }
    // VUCOMISD {sae}, xmm, xmm
    if len(vv) == 1 && isSAE(v0) && isEVEXXMM(v1) && isEVEXXMM(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd)
            m.emit(0x18)
            m.emit(0x2e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VUCOMISD xmm, xmm
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0xfd)
            m.emit(0x48)
            m.emit(0x2e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VUCOMISD")
    }
    return p
}

// VUCOMISS performs "Unordered Compare Scalar Single-Precision Floating-Point Values and Set EFLAGS".
//
// Mnemonic        : VUCOMISS
// ISA extensions  : AVX, AVX512F
// Supported forms : (5 forms)
//
//    * VUCOMISS xmm, xmm
//    * VUCOMISS m32, xmm
//    * VUCOMISS m32, xmm
//    * VUCOMISS {sae}, xmm, xmm
//    * VUCOMISS xmm, xmm
//
func (self *Program) VUCOMISS(v0 interface{}, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc(2, Operands{v0, v1})
        case 1  : p = self.alloc(3, Operands{v0, v1, vv[0]})
        default : panic("instruction VUCOMISS takes 2 or 3 operands")
    }
    // VUCOMISS xmm, xmm
    if len(vv) == 0 && isXMM(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[1]), v[0], 0)
            m.emit(0x2e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // VUCOMISS m32, xmm
    if len(vv) == 0 && isM32(v0) && isXMM(v1) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[1]), addr(v[0]), 0)
            m.emit(0x2e)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // VUCOMISS m32, xmm
    if len(vv) == 0 && isM32(v0) && isEVEXXMM(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b00, ehcode(v[1]), addr(v[0]), 0, 0, 0, 0)
            m.emit(0x2e)
            m.mrsd(lcode(v[1]), addr(v[0]), 4)
        })
    }
    // VUCOMISS {sae}, xmm, xmm
    if len(vv) == 1 && isSAE(v0) && isEVEXXMM(v1) && isEVEXXMM(vv[0]) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[1]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c)
            m.emit(0x18)
            m.emit(0x2e)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[1]))
        })
    }
    // VUCOMISS xmm, xmm
    if len(vv) == 0 && isEVEXXMM(v0) && isEVEXXMM(v1) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[1]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[1]) << 4)))
            m.emit(0x7c)
            m.emit(0x48)
            m.emit(0x2e)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    if p.len == 0 {
        panic("invalid operands for VUCOMISS")
    }
    return p
}

// VUNPCKHPD performs "Unpack and Interleave High Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VUNPCKHPD
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VUNPCKHPD m128/m64bcst, xmm, xmm{k}{z}
//    * VUNPCKHPD xmm, xmm, xmm{k}{z}
//    * VUNPCKHPD m256/m64bcst, ymm, ymm{k}{z}
//    * VUNPCKHPD ymm, ymm, ymm{k}{z}
//    * VUNPCKHPD m512/m64bcst, zmm, zmm{k}{z}
//    * VUNPCKHPD zmm, zmm, zmm{k}{z}
//    * VUNPCKHPD xmm, xmm, xmm
//    * VUNPCKHPD m128, xmm, xmm
//    * VUNPCKHPD ymm, ymm, ymm
//    * VUNPCKHPD m256, ymm, ymm
//
func (self *Program) VUNPCKHPD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VUNPCKHPD m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x15)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VUNPCKHPD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x15)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VUNPCKHPD m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x15)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VUNPCKHPD ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x15)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VUNPCKHPD m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x15)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VUNPCKHPD zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x15)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VUNPCKHPD xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x15)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VUNPCKHPD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x15)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VUNPCKHPD ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x15)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VUNPCKHPD m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x15)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VUNPCKHPD")
    }
    return p
}

// VUNPCKHPS performs "Unpack and Interleave High Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VUNPCKHPS
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VUNPCKHPS m128/m32bcst, xmm, xmm{k}{z}
//    * VUNPCKHPS xmm, xmm, xmm{k}{z}
//    * VUNPCKHPS m256/m32bcst, ymm, ymm{k}{z}
//    * VUNPCKHPS ymm, ymm, ymm{k}{z}
//    * VUNPCKHPS m512/m32bcst, zmm, zmm{k}{z}
//    * VUNPCKHPS zmm, zmm, zmm{k}{z}
//    * VUNPCKHPS xmm, xmm, xmm
//    * VUNPCKHPS m128, xmm, xmm
//    * VUNPCKHPS ymm, ymm, ymm
//    * VUNPCKHPS m256, ymm, ymm
//
func (self *Program) VUNPCKHPS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VUNPCKHPS m128/m32bcst, xmm, xmm{k}{z}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x15)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VUNPCKHPS xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x15)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VUNPCKHPS m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x15)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VUNPCKHPS ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x15)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VUNPCKHPS m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x15)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VUNPCKHPS zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x15)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VUNPCKHPS xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x15)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VUNPCKHPS m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x15)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VUNPCKHPS ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x15)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VUNPCKHPS m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x15)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VUNPCKHPS")
    }
    return p
}

// VUNPCKLPD performs "Unpack and Interleave Low Packed Double-Precision Floating-Point Values".
//
// Mnemonic        : VUNPCKLPD
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VUNPCKLPD m128/m64bcst, xmm, xmm{k}{z}
//    * VUNPCKLPD xmm, xmm, xmm{k}{z}
//    * VUNPCKLPD m256/m64bcst, ymm, ymm{k}{z}
//    * VUNPCKLPD ymm, ymm, ymm{k}{z}
//    * VUNPCKLPD m512/m64bcst, zmm, zmm{k}{z}
//    * VUNPCKLPD zmm, zmm, zmm{k}{z}
//    * VUNPCKLPD xmm, xmm, xmm
//    * VUNPCKLPD m128, xmm, xmm
//    * VUNPCKLPD ymm, ymm, ymm
//    * VUNPCKLPD m256, ymm, ymm
//
func (self *Program) VUNPCKLPD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VUNPCKLPD m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x14)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VUNPCKLPD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x14)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VUNPCKLPD m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x14)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VUNPCKLPD ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x14)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VUNPCKLPD m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x14)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VUNPCKLPD zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x14)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VUNPCKLPD xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x14)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VUNPCKLPD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x14)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VUNPCKLPD ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x14)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VUNPCKLPD m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x14)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VUNPCKLPD")
    }
    return p
}

// VUNPCKLPS performs "Unpack and Interleave Low Packed Single-Precision Floating-Point Values".
//
// Mnemonic        : VUNPCKLPS
// ISA extensions  : AVX, AVX512F, AVX512VL
// Supported forms : (10 forms)
//
//    * VUNPCKLPS m128/m32bcst, xmm, xmm{k}{z}
//    * VUNPCKLPS xmm, xmm, xmm{k}{z}
//    * VUNPCKLPS m256/m32bcst, ymm, ymm{k}{z}
//    * VUNPCKLPS ymm, ymm, ymm{k}{z}
//    * VUNPCKLPS m512/m32bcst, zmm, zmm{k}{z}
//    * VUNPCKLPS zmm, zmm, zmm{k}{z}
//    * VUNPCKLPS xmm, xmm, xmm
//    * VUNPCKLPS m128, xmm, xmm
//    * VUNPCKLPS ymm, ymm, ymm
//    * VUNPCKLPS m256, ymm, ymm
//
func (self *Program) VUNPCKLPS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VUNPCKLPS m128/m32bcst, xmm, xmm{k}{z}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x14)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VUNPCKLPS xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x14)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VUNPCKLPS m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x14)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VUNPCKLPS ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x14)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VUNPCKLPS m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x14)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VUNPCKLPS zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512F)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x14)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VUNPCKLPS xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x14)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VUNPCKLPS m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x14)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VUNPCKLPS ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x14)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VUNPCKLPS m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x14)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VUNPCKLPS")
    }
    return p
}

// VXORPD performs "Bitwise Logical XOR for Double-Precision Floating-Point Values".
//
// Mnemonic        : VXORPD
// ISA extensions  : AVX, AVX512DQ, AVX512VL
// Supported forms : (10 forms)
//
//    * VXORPD m128/m64bcst, xmm, xmm{k}{z}
//    * VXORPD xmm, xmm, xmm{k}{z}
//    * VXORPD m256/m64bcst, ymm, ymm{k}{z}
//    * VXORPD ymm, ymm, ymm{k}{z}
//    * VXORPD m512/m64bcst, zmm, zmm{k}{z}
//    * VXORPD zmm, zmm, zmm{k}{z}
//    * VXORPD xmm, xmm, xmm
//    * VXORPD m128, xmm, xmm
//    * VXORPD ymm, ymm, ymm
//    * VXORPD m256, ymm, ymm
//
func (self *Program) VXORPD(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VXORPD m128/m64bcst, xmm, xmm{k}{z}
    if isM128M64bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x57)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VXORPD xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x57)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VXORPD m256/m64bcst, ymm, ymm{k}{z}
    if isM256M64bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x57)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VXORPD ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x57)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VXORPD m512/m64bcst, zmm, zmm{k}{z}
    if isM512M64bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x85, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x57)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VXORPD zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0xfd ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x57)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VXORPD xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x57)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VXORPD m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(1, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x57)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VXORPD ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x57)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VXORPD m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(5, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x57)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VXORPD")
    }
    return p
}

// VXORPS performs "Bitwise Logical XOR for Single-Precision Floating-Point Values".
//
// Mnemonic        : VXORPS
// ISA extensions  : AVX, AVX512DQ, AVX512VL
// Supported forms : (10 forms)
//
//    * VXORPS m128/m32bcst, xmm, xmm{k}{z}
//    * VXORPS xmm, xmm, xmm{k}{z}
//    * VXORPS m256/m32bcst, ymm, ymm{k}{z}
//    * VXORPS ymm, ymm, ymm{k}{z}
//    * VXORPS m512/m32bcst, zmm, zmm{k}{z}
//    * VXORPS zmm, zmm, zmm{k}{z}
//    * VXORPS xmm, xmm, xmm
//    * VXORPS m128, xmm, xmm
//    * VXORPS ymm, ymm, ymm
//    * VXORPS m256, ymm, ymm
//
func (self *Program) VXORPS(v0 interface{}, v1 interface{}, v2 interface{}) *Instruction {
    p := self.alloc(3, Operands{v0, v1, v2})
    // VXORPS m128/m32bcst, xmm, xmm{k}{z}
    if isM128M32bcst(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b00, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x57)
            m.mrsd(lcode(v[2]), addr(v[0]), 16)
        })
    }
    // VXORPS xmm, xmm, xmm{k}{z}
    if isEVEXXMM(v0) && isEVEXXMM(v1) && isXMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x00)
            m.emit(0x57)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VXORPS m256/m32bcst, ymm, ymm{k}{z}
    if isM256M32bcst(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b01, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x57)
            m.mrsd(lcode(v[2]), addr(v[0]), 32)
        })
    }
    // VXORPS ymm, ymm, ymm{k}{z}
    if isEVEXYMM(v0) && isEVEXYMM(v1) && isYMMkz(v2) {
        self.require(ISA_AVX512VL | ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x20)
            m.emit(0x57)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VXORPS m512/m32bcst, zmm, zmm{k}{z}
    if isM512M32bcst(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.evex(0b01, 0x04, 0b10, ehcode(v[2]), addr(v[0]), vcode(v[1]), kcode(v[2]), zcode(v[2]), bcode(v[0]))
            m.emit(0x57)
            m.mrsd(lcode(v[2]), addr(v[0]), 64)
        })
    }
    // VXORPS zmm, zmm, zmm{k}{z}
    if isZMM(v0) && isZMM(v1) && isZMMkz(v2) {
        self.require(ISA_AVX512DQ)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x62)
            m.emit(0xf1 ^ ((hcode(v[2]) << 7) | (ehcode(v[0]) << 5) | (ecode(v[2]) << 4)))
            m.emit(0x7c ^ (hlcode(v[1]) << 3))
            m.emit((zcode(v[2]) << 7) | (0x08 ^ (ecode(v[1]) << 3)) | kcode(v[2]) | 0x40)
            m.emit(0x57)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VXORPS xmm, xmm, xmm
    if isXMM(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x57)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VXORPS m128, xmm, xmm
    if isM128(v0) && isXMM(v1) && isXMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(0, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x57)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    // VXORPS ymm, ymm, ymm
    if isYMM(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[2]), v[0], hlcode(v[1]))
            m.emit(0x57)
            m.emit(0xc0 | lcode(v[2]) << 3 | lcode(v[0]))
        })
    }
    // VXORPS m256, ymm, ymm
    if isM256(v0) && isYMM(v1) && isYMM(v2) {
        self.require(ISA_AVX)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.vex2(4, hcode(v[2]), addr(v[0]), hlcode(v[1]))
            m.emit(0x57)
            m.mrsd(lcode(v[2]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for VXORPS")
    }
    return p
}

// VZEROALL performs "Zero All YMM Registers".
//
// Mnemonic        : VZEROALL
// ISA extensions  : AVX
// Supported forms : (1 form)
//
//    * VZEROALL
//
func (self *Program) VZEROALL() *Instruction {
    p := self.alloc(0, Operands{})
    // VZEROALL
    self.require(ISA_AVX)
    p.add(0, func(m *_Encoding, v []interface{}) {
        m.vex2(4, 0, nil, 0)
        m.emit(0x77)
    })
    return p
}

// VZEROUPPER performs "Zero Upper Bits of YMM Registers".
//
// Mnemonic        : VZEROUPPER
// ISA extensions  : AVX
// Supported forms : (1 form)
//
//    * VZEROUPPER
//
func (self *Program) VZEROUPPER() *Instruction {
    p := self.alloc(0, Operands{})
    // VZEROUPPER
    self.require(ISA_AVX)
    p.add(0, func(m *_Encoding, v []interface{}) {
        m.vex2(0, 0, nil, 0)
        m.emit(0x77)
    })
    return p
}

// XADDB performs "Exchange and Add".
//
// Mnemonic        : XADD
// Supported forms : (2 forms)
//
//    * XADDB r8, r8
//    * XADDB r8, m8
//
func (self *Program) XADDB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // XADDB r8, r8
    if isReg8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], isReg8REX(v[0]) || isReg8REX(v[1]))
            m.emit(0x0f)
            m.emit(0xc0)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // XADDB r8, m8
    if isReg8(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), isReg8REX(v[0]))
            m.emit(0x0f)
            m.emit(0xc0)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for XADDB")
    }
    return p
}

// XADDL performs "Exchange and Add".
//
// Mnemonic        : XADD
// Supported forms : (2 forms)
//
//    * XADDL r32, r32
//    * XADDL r32, m32
//
func (self *Program) XADDL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // XADDL r32, r32
    if isReg32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x0f)
            m.emit(0xc1)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // XADDL r32, m32
    if isReg32(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0xc1)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for XADDL")
    }
    return p
}

// XADDQ performs "Exchange and Add".
//
// Mnemonic        : XADD
// Supported forms : (2 forms)
//
//    * XADDQ r64, r64
//    * XADDQ r64, m64
//
func (self *Program) XADDQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // XADDQ r64, r64
    if isReg64(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[0]) << 2 | hcode(v[1]))
            m.emit(0x0f)
            m.emit(0xc1)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // XADDQ r64, m64
    if isReg64(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[0]), addr(v[1]))
            m.emit(0x0f)
            m.emit(0xc1)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for XADDQ")
    }
    return p
}

// XADDW performs "Exchange and Add".
//
// Mnemonic        : XADD
// Supported forms : (2 forms)
//
//    * XADDW r16, r16
//    * XADDW r16, m16
//
func (self *Program) XADDW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // XADDW r16, r16
    if isReg16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x0f)
            m.emit(0xc1)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
    }
    // XADDW r16, m16
    if isReg16(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x0f)
            m.emit(0xc1)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for XADDW")
    }
    return p
}

// XCHGB performs "Exchange Register/Memory with Register".
//
// Mnemonic        : XCHG
// Supported forms : (3 forms)
//
//    * XCHGB r8, r8
//    * XCHGB m8, r8
//    * XCHGB r8, m8
//
func (self *Program) XCHGB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // XCHGB r8, r8
    if isReg8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], isReg8REX(v[0]) || isReg8REX(v[1]))
            m.emit(0x86)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], isReg8REX(v[0]) || isReg8REX(v[1]))
            m.emit(0x86)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // XCHGB m8, r8
    if isM8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), isReg8REX(v[1]))
            m.emit(0x86)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // XCHGB r8, m8
    if isReg8(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), isReg8REX(v[0]))
            m.emit(0x86)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for XCHGB")
    }
    return p
}

// XCHGL performs "Exchange Register/Memory with Register".
//
// Mnemonic        : XCHG
// Supported forms : (5 forms)
//
//    * XCHGL r32, eax
//    * XCHGL eax, r32
//    * XCHGL r32, r32
//    * XCHGL m32, r32
//    * XCHGL r32, m32
//
func (self *Program) XCHGL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // XCHGL r32, eax
    if isReg32(v0) && v1 == EAX {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[0], false)
            m.emit(0x90 | lcode(v[0]))
        })
    }
    // XCHGL eax, r32
    if v0 == EAX && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0x90 | lcode(v[1]))
        })
    }
    // XCHGL r32, r32
    if isReg32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x87)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x87)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // XCHGL m32, r32
    if isM32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x87)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // XCHGL r32, m32
    if isReg32(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x87)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for XCHGL")
    }
    return p
}

// XCHGQ performs "Exchange Register/Memory with Register".
//
// Mnemonic        : XCHG
// Supported forms : (5 forms)
//
//    * XCHGQ r64, rax
//    * XCHGQ rax, r64
//    * XCHGQ r64, r64
//    * XCHGQ m64, r64
//    * XCHGQ r64, m64
//
func (self *Program) XCHGQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // XCHGQ r64, rax
    if isReg64(v0) && v1 == RAX {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[0]))
            m.emit(0x90 | lcode(v[0]))
        })
    }
    // XCHGQ rax, r64
    if v0 == RAX && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0x90 | lcode(v[1]))
        })
    }
    // XCHGQ r64, r64
    if isReg64(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[0]) << 2 | hcode(v[1]))
            m.emit(0x87)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x87)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // XCHGQ m64, r64
    if isM64(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x87)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // XCHGQ r64, m64
    if isReg64(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[0]), addr(v[1]))
            m.emit(0x87)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for XCHGQ")
    }
    return p
}

// XCHGW performs "Exchange Register/Memory with Register".
//
// Mnemonic        : XCHG
// Supported forms : (5 forms)
//
//    * XCHGW r16, ax
//    * XCHGW ax, r16
//    * XCHGW r16, r16
//    * XCHGW m16, r16
//    * XCHGW r16, m16
//
func (self *Program) XCHGW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // XCHGW r16, ax
    if isReg16(v0) && v1 == AX {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[0], false)
            m.emit(0x90 | lcode(v[0]))
        })
    }
    // XCHGW ax, r16
    if v0 == AX && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0x90 | lcode(v[1]))
        })
    }
    // XCHGW r16, r16
    if isReg16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x87)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x87)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // XCHGW m16, r16
    if isM16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x87)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // XCHGW r16, m16
    if isReg16(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x87)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for XCHGW")
    }
    return p
}

// XGETBV performs "Get Value of Extended Control Register".
//
// Mnemonic        : XGETBV
// Supported forms : (1 form)
//
//    * XGETBV
//
func (self *Program) XGETBV() *Instruction {
    p := self.alloc(0, Operands{})
    // XGETBV
    p.add(0, func(m *_Encoding, v []interface{}) {
        m.emit(0x0f)
        m.emit(0x01)
        m.emit(0xd0)
    })
    return p
}

// XLATB performs "Table Look-up Translation".
//
// Mnemonic        : XLATB
// Supported forms : (2 forms)
//
//    * XLATB
//    * XLATB
//
func (self *Program) XLATB() *Instruction {
    p := self.alloc(0, Operands{})
    // XLATB
    p.add(0, func(m *_Encoding, v []interface{}) {
        m.emit(0xd7)
    })
    // XLATB
    p.add(0, func(m *_Encoding, v []interface{}) {
        m.emit(0x48)
        m.emit(0xd7)
    })
    return p
}

// XORB performs "Logical Exclusive OR".
//
// Mnemonic        : XOR
// Supported forms : (6 forms)
//
//    * XORB imm8, al
//    * XORB imm8, r8
//    * XORB r8, r8
//    * XORB m8, r8
//    * XORB imm8, m8
//    * XORB r8, m8
//
func (self *Program) XORB(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // XORB imm8, al
    if isImm8(v0) && v1 == AL {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x34)
            m.imm1(toImmAny(v[0]))
        })
    }
    // XORB imm8, r8
    if isImm8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], isReg8REX(v[1]))
            m.emit(0x80)
            m.emit(0xf0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // XORB r8, r8
    if isReg8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], isReg8REX(v[0]) || isReg8REX(v[1]))
            m.emit(0x30)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], isReg8REX(v[0]) || isReg8REX(v[1]))
            m.emit(0x32)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // XORB m8, r8
    if isM8(v0) && isReg8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), isReg8REX(v[1]))
            m.emit(0x32)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // XORB imm8, m8
    if isImm8(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0x80)
            m.mrsd(6, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // XORB r8, m8
    if isReg8(v0) && isM8(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), isReg8REX(v[0]))
            m.emit(0x30)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for XORB")
    }
    return p
}

// XORL performs "Logical Exclusive OR".
//
// Mnemonic        : XOR
// Supported forms : (8 forms)
//
//    * XORL imm32, eax
//    * XORL imm8, r32
//    * XORL imm32, r32
//    * XORL r32, r32
//    * XORL m32, r32
//    * XORL imm8, m32
//    * XORL imm32, m32
//    * XORL r32, m32
//
func (self *Program) XORL(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // XORL imm32, eax
    if isImm32(v0) && v1 == EAX {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x35)
            m.imm4(toImmAny(v[0]))
        })
    }
    // XORL imm8, r32
    if isImm8Ext(v0, 4) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0x83)
            m.emit(0xf0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // XORL imm32, r32
    if isImm32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, v[1], false)
            m.emit(0x81)
            m.emit(0xf0 | lcode(v[1]))
            m.imm4(toImmAny(v[0]))
        })
    }
    // XORL r32, r32
    if isReg32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x31)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x33)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // XORL m32, r32
    if isM32(v0) && isReg32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x33)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // XORL imm8, m32
    if isImm8Ext(v0, 4) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0x83)
            m.mrsd(6, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // XORL imm32, m32
    if isImm32(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(0, addr(v[1]), false)
            m.emit(0x81)
            m.mrsd(6, addr(v[1]), 1)
            m.imm4(toImmAny(v[0]))
        })
    }
    // XORL r32, m32
    if isReg32(v0) && isM32(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x31)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for XORL")
    }
    return p
}

// XORPD performs "Bitwise Logical XOR for Double-Precision Floating-Point Values".
//
// Mnemonic        : XORPD
// ISA extensions  : SSE2
// Supported forms : (2 forms)
//
//    * XORPD xmm, xmm
//    * XORPD m128, xmm
//
func (self *Program) XORPD(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // XORPD xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x57)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // XORPD m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE2)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x57)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for XORPD")
    }
    return p
}

// XORPS performs "Bitwise Logical XOR for Single-Precision Floating-Point Values".
//
// Mnemonic        : XORPS
// ISA extensions  : SSE
// Supported forms : (2 forms)
//
//    * XORPS xmm, xmm
//    * XORPS m128, xmm
//
func (self *Program) XORPS(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // XORPS xmm, xmm
    if isXMM(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x0f)
            m.emit(0x57)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // XORPS m128, xmm
    if isM128(v0) && isXMM(v1) {
        self.require(ISA_SSE)
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x0f)
            m.emit(0x57)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for XORPS")
    }
    return p
}

// XORQ performs "Logical Exclusive OR".
//
// Mnemonic        : XOR
// Supported forms : (8 forms)
//
//    * XORQ imm32, rax
//    * XORQ imm8, r64
//    * XORQ imm32, r64
//    * XORQ r64, r64
//    * XORQ m64, r64
//    * XORQ imm8, m64
//    * XORQ imm32, m64
//    * XORQ r64, m64
//
func (self *Program) XORQ(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // XORQ imm32, rax
    if isImm32(v0) && v1 == RAX {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48)
            m.emit(0x35)
            m.imm4(toImmAny(v[0]))
        })
    }
    // XORQ imm8, r64
    if isImm8Ext(v0, 8) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0x83)
            m.emit(0xf0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // XORQ imm32, r64
    if isImm32Ext(v0, 8) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]))
            m.emit(0x81)
            m.emit(0xf0 | lcode(v[1]))
            m.imm4(toImmAny(v[0]))
        })
    }
    // XORQ r64, r64
    if isReg64(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[0]) << 2 | hcode(v[1]))
            m.emit(0x31)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x48 | hcode(v[1]) << 2 | hcode(v[0]))
            m.emit(0x33)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // XORQ m64, r64
    if isM64(v0) && isReg64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[1]), addr(v[0]))
            m.emit(0x33)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // XORQ imm8, m64
    if isImm8Ext(v0, 8) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0x83)
            m.mrsd(6, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // XORQ imm32, m64
    if isImm32Ext(v0, 8) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, 0, addr(v[1]))
            m.emit(0x81)
            m.mrsd(6, addr(v[1]), 1)
            m.imm4(toImmAny(v[0]))
        })
    }
    // XORQ r64, m64
    if isReg64(v0) && isM64(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.rexm(1, hcode(v[0]), addr(v[1]))
            m.emit(0x31)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for XORQ")
    }
    return p
}

// XORW performs "Logical Exclusive OR".
//
// Mnemonic        : XOR
// Supported forms : (8 forms)
//
//    * XORW imm16, ax
//    * XORW imm8, r16
//    * XORW imm16, r16
//    * XORW r16, r16
//    * XORW m16, r16
//    * XORW imm8, m16
//    * XORW imm16, m16
//    * XORW r16, m16
//
func (self *Program) XORW(v0 interface{}, v1 interface{}) *Instruction {
    p := self.alloc(2, Operands{v0, v1})
    // XORW imm16, ax
    if isImm16(v0) && v1 == AX {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.emit(0x35)
            m.imm2(toImmAny(v[0]))
        })
    }
    // XORW imm8, r16
    if isImm8Ext(v0, 2) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0x83)
            m.emit(0xf0 | lcode(v[1]))
            m.imm1(toImmAny(v[0]))
        })
    }
    // XORW imm16, r16
    if isImm16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, v[1], false)
            m.emit(0x81)
            m.emit(0xf0 | lcode(v[1]))
            m.imm2(toImmAny(v[0]))
        })
    }
    // XORW r16, r16
    if isReg16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), v[1], false)
            m.emit(0x31)
            m.emit(0xc0 | lcode(v[0]) << 3 | lcode(v[1]))
        })
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), v[0], false)
            m.emit(0x33)
            m.emit(0xc0 | lcode(v[1]) << 3 | lcode(v[0]))
        })
    }
    // XORW m16, r16
    if isM16(v0) && isReg16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[1]), addr(v[0]), false)
            m.emit(0x33)
            m.mrsd(lcode(v[1]), addr(v[0]), 1)
        })
    }
    // XORW imm8, m16
    if isImm8Ext(v0, 2) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0x83)
            m.mrsd(6, addr(v[1]), 1)
            m.imm1(toImmAny(v[0]))
        })
    }
    // XORW imm16, m16
    if isImm16(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(0, addr(v[1]), false)
            m.emit(0x81)
            m.mrsd(6, addr(v[1]), 1)
            m.imm2(toImmAny(v[0]))
        })
    }
    // XORW r16, m16
    if isReg16(v0) && isM16(v1) {
        p.add(0, func(m *_Encoding, v []interface{}) {
            m.emit(0x66)
            m.rexo(hcode(v[0]), addr(v[1]), false)
            m.emit(0x31)
            m.mrsd(lcode(v[0]), addr(v[1]), 1)
        })
    }
    if p.len == 0 {
        panic("invalid operands for XORW")
    }
    return p
}
